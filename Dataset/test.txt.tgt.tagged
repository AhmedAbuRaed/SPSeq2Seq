<t>   <cite> asked judges whether their paraphrases were  roughly interchangeable given the  genre. </t> 
<t> for example, <cite> evaluated their paraphrases  by asking judges whether paraphrases were  approximately conceptually equivalent. </t> 
<t> callison <cite> measure improvements in translation quality in terms of <cite> and in terms of subjective human evaluation when paraphrases are integrated into a statistical machine translation system. </t> 
<t>   bannard and <cite> replaced phrases with paraphrases in a number of  sentences and asked judges whether the substitutions  preserved meaning and remained grammatical. </t> 
<t> <cite> create a set of manual word alignments between pairs of english sentences. </t> 
<t> <cite> manually judge whether a paraphrase might be used to answer questions from the trec question answering track. </t> 
<t> <cite> suggested that multiple translations of the same foreign source  text were a perfect source for  naturally occurring paraphrases  because they are samples of text which convey the same meaning but are produced by different writers. </t> 
<t> cohn et al. (to appear) compares the use of  the multiple translation corpus with the msr corpus for this task. </t> 
<t> for example, in the <cite> content  units that are similar across human generated summaries are hand aligned. </t> 
<t> sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or  negative <cite>. </t> 
<t> sentiment classification at the sentence level has also been studied <cite>. </t> 
<t> sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or  negative <cite>. </t> 
<t> sentiment classification at the sentence level has also been studied <cite>. </t> 
<t> sentiment classification at the sentence level has also been studied <cite>. </t> 
<t> sentiment classification at the sentence level has also been studied <cite>. </t> 
<t> sentiment classification at the sentence level has also been studied <cite>. </t> 
<t> the works in <cite> perform opinion mining at the feature level. </t> <t> the task involves (#) extracting entity features (e.g.,  picture quality  and  battery life  in a camera review) and (#) finding orientations (positive, negative or neutral) of opinions expressed on the  features by reviewers. </t> 
<t> the works in <cite> perform opinion mining at the feature level. </t> <t> the task involves (#) extracting entity features (e.g.,  picture quality  and  battery life  in a camera review) and (#) finding orientations (positive, negative or neutral) of opinions expressed on the  features by reviewers. </t> 
<t> the works in <cite> perform opinion mining at the feature level. </t> <t> the task involves (#) extracting entity features (e.g.,  picture quality  and  battery life  in a camera review) and (#) finding orientations (positive, negative or neutral) of opinions expressed on the  features by reviewers. </t> 
<t> the works in <cite> perform opinion mining at the feature level. </t> <t> the task involves (#) extracting entity features (e.g.,  picture quality  and  battery life  in a camera review) and (#) finding orientations (positive, negative or neutral) of opinions expressed on the  features by reviewers. </t> 
<t> as discussed in the introduction, a closely related work to ours is <cite>. </t> 
<t> <cite> proposes a method to extract some useful items from  superlative sentences. </t> 
<t> discovering orientations of context dependent  opinion comparative words is related to identifying domain opinion words <cite>. </t> <t> both works use conjunction rules to find  such words from large domain corpora. </t> <t> one conjunction rule states that when two opinion words are linked by  and , their opinions are the same. </t> 
<t> discovering orientations of context dependent  opinion comparative words is related to identifying domain opinion words <cite>. </t> <t> both works use conjunction rules to find  such words from large domain corpora. </t> <t> one conjunction rule states that when two opinion words are linked by  and , their opinions are the same. </t> 
<t> <cite> studied the problem of identifying which entity has  more of certain features in comparative sentences. </t> 
<t> <cite> remove constraints imposed by the size of main  memory by using an external data structure. </t> 
<t> to our knowledge, the strand of research initiated by callison <cite> and <cite> and extended here is the  rst to do so. </t> 
<t> <cite> address this bottleneck  with a promising approach based on parallel processing, showing reductions in real time that are linear in the number of cpus. </t> 
<t> <cite> substantially reduce model size with a  ltering method. </t> 
<t> to our knowledge, the strand of research initiated by callison <cite> and <cite> and extended here is the  rst to do so. </t> <t> decoding speed might be partially addressed using a mixture of online and of ine computation as in <cite>, but faster algorithms are  still needed. </t> 
<t> for example, <cite> and  <cite> improve translation accuracy using discriminatively trained models with  contextual features of source phrases. </t> <t> their features are easy to obtain at runtime using our approach, which  nds source phrases in context. </t> <t> however, to make their experiments tractable, they trained their discriminative models of ine only for  the speci c phrases of the test set. </t> 
<t> for example, <cite> and  <cite> improve translation accuracy using discriminatively trained models with  contextual features of source phrases. </t> <t> their features are easy to obtain at runtime using our approach, which  nds source phrases in context. </t> <t> however, to make their experiments tractable, they trained their discriminative models of ine only for  the speci c phrases of the test set. </t> 
<t> it may be possible to address this problem with a  novel data structure known as a compressed selfindex <cite>, which supports fast pattern matching on a representation that  is close in size to the information theoretic minimum required by the data. </t> 
<t> indeed, nli can enable more immediate applications, such as semantic search and question answering <cite>. </t> 
<t> up to now, the most successful approaches have used fairly shallow semantic representations, relying on measures of  lexical or semantic overlap <cite>, pattern based relation extraction <cite>, or approximate matching of predicate argument structure <cite>. </t> 
<t> in recent years a spectrum of approaches to robust, opendomain nli have been explored within the context of the recognizing textual entailment challenge <cite>. </t> 
<t> up to now, the most successful approaches have used fairly shallow semantic representations, relying on measures of  lexical or semantic overlap <cite>, pattern based relation extraction <cite>, or approximate matching of predicate argument structure <cite>. </t> 
<t> up to now, the most successful approaches have used fairly shallow semantic representations, relying on measures of  lexical or semantic overlap <cite>, pattern based relation extraction <cite>, or approximate matching of predicate argument structure <cite>. </t> 
<t> at the other extreme, some researchers have approached nli as logical deduction, building on work in theoretical semantics to translate sentences into  rst order logic (fol), and then applying a theorem prover or model builder <cite>. </t> 
<t> at the other extreme, some researchers have approached nli as logical deduction, building on work in theoretical semantics to translate sentences into  rst order logic (fol), and then applying a theorem prover or model builder <cite>. </t> 
<t> ) fol based systems that have attained high precision <cite> have done so at the cost of very poor recall. </t> 
<t> both previous work (cf. </t> <t> <cite> discussed below) and our present results   nd these distinctions extremely helpful. </t> <t> secondly, their system relies on a separate preprocessing stage to classify non anaphoric pronouns, and mark the gender of certain nps (mr., mrs.  and some  rst names). </t> <t> this allows the incorporation of external data and learning systems, but conversely, it requires these decisions to be made sequentially. </t> <t> our system classi es non anaphoric pronouns jointly, and learns gender without an  external database. </t> <t> next, they only handle thirdperson pronouns, while we handle  rst and second as well. </t> <t> finally, as a demonstration of em s capabilities, its evidence is equivocal. </t> <t> their em  requires careful initialization   suf ciently careful that the em version only performs #.#  better than the initialized program alone. </t> <t> <cite> is one of the few papers that use the <cite> corpus used here. </t> <t> they achieve a very high ##  correct, but this is  given hand annotated number, gender and syntactic binding features to  lter candidate antecedents and also ignores non anaphoric pronouns. </t> 
<t> another key paper is <cite>. </t> <t> the data  annotated for the ge research is used here for testing and development data. </t> <t> also, there are many overlaps between their formulation of the problem  and ours. </t> <t> for one thing, their model is generative, although they do not note this fact, and (with the partial exception we are about to mention) they obtain their probabilities from hand annotated data  rather than using em. </t> <t> lastly, they learn their gender information (the probability of that a pronoun will have a particular gender given its antecedent) using a truncated em procedure. </t> <t> once they have  derived all of the other parameters from the training data, they go through a larger corpus of unlabeled data collecting estimated counts of how often each word generates a pronoun of a particular gender. </t> <t> they then normalize these probabilities and the result is used in the  nal program. </t> <t> this is, in fact, a single iteration of em. </t> <t> <cite> is one of the few papers that use the <cite> corpus used here. </t> <t> they achieve a very high ##  correct, but this is  given hand annotated number, gender and syntactic binding features to  lter candidate antecedents and also ignores non anaphoric pronouns. </t> 
<t> there are also several papers which treat coference as an unsupervised clustering problem <cite>. </t> 
<t> probably the closest approach to our own is <cite>, which also presents  an em approach to pronoun resolution, and obtains quite successful results. </t> 
<t> only perhaps one in four or  ve nps are markable <cite>. </t> 
<t> there are also several papers which treat coference as an unsupervised clustering problem <cite>. </t> 
<t> a quite different unsupervised approach is <cite>, which uses self training of a  discriminative system, initialized with some conservative number and gender heuristics. </t> <t> the system uses the conventional ranking approach, applying a maximum entropy classi er to pairs of pronoun and potential antecedent and selecting the best antecedent. </t> <t> in each iteration of self training,  the system labels the training corpus and its decisions are treated as input for the next training phase. </t> <t> the system improves substantially over a  hobbs baseline. </t> <t> in comparison to ours, their feature set is quite similar, while their learning approach is rather different. </t> <t> in addition, their system does not classify non anaphoric pronouns, </t> <t> a third paper that has signi cantly in uenced our work is that of <cite>. </t> 
<t> a quite different unsupervised approach is <cite>, which uses self training of a  discriminative system, initialized with some conservative number and gender heuristics. </t> <t> the system uses the conventional ranking approach, applying a maximum entropy classi er to pairs of pronoun and potential antecedent and selecting the best antecedent. </t> <t> in each iteration of self training,  the system labels the training corpus and its decisions are treated as input for the next training phase. </t> <t> the system improves substantially over a  hobbs baseline. </t> <t> in comparison to ours, their feature set is quite similar, while their learning approach is rather different. </t> <t> in addition, their system does not classify non anaphoric pronouns, </t> <t> a third paper that has signi cantly in uenced our work is that of <cite>. </t> 
<t> <cite>, the word forms in a given protolanguage are reconstructed using a viterbi multialignment between a small number of its descendant  languages. </t> <t> the alignment is computed using handset parameters. </t> <t> deterministic rules characterizing  changes between pairs of observed languages are extracted from the alignment when their frequency is  higher than a threshold, and a proto phoneme inventory is built using linguistically motivated rules and parsimony. </t> <t> a reconstruction of each observed word is  rst proposed independently for each language. </t> <t> if at least two reconstructions agree, a majority vote is taken, otherwise no reconstruction is proposed. </t> 
<t> in bouchard <cite>, a stochastic model  of sound change is used and reconstructions are inferred by performing probabilistic inference over an  evolutionary tree expressing the relationships between languages. </t> 
<t> supervised named entity recognition now performs almost as well as human annotation in <cite> and has excellent performance on  other languages <cite>. </t> 
<t> supervised approaches such as <cite> have used clustering to group together different nominals referring  to the same entity in ways similar to the  consistency  approach outlined below in section #.#. </t> 
<t> co training <cite> begins with a small set of labeling heuristics and gradually adds examples to the training data. </t> 
<t> supervised named entity recognition now performs almost as well as human annotation in <cite> and has excellent performance on  other languages <cite>. </t> 
<t> for a survey of the state of the art, see <cite>. </t> 
<t> co training <cite> begins with a small set of labeling heuristics and gradually adds examples to the training data. </t> <t> various co training approaches presented in <cite> all score about ##  on a dataset of named entities  the inital labels were assigned using # hand written seed rules. </t> <t> however, <cite>  show that a mixture of naive bayes generative clustering model (which they call an em model), initialized with the same seed rules, performs much more poorly at ## . </t> 
<t> much later work <cite> relies on the use of extremely large corpora which allow very precise, but sparse features. </t> <t> for instance <cite> and <cite> use web queries to  count occurrences of  cities such as x  and similar phrases. </t> 
<t> much later work <cite> relies on the use of extremely large corpora which allow very precise, but sparse features. </t> <t> for instance <cite> and <cite> use web queries to  count occurrences of  cities such as x  and similar phrases. </t> 
<t> much later work <cite> relies on the use of extremely large corpora which allow very precise, but sparse features. </t> 
<t> much later work <cite> relies on the use of extremely large corpora which allow very precise, but sparse features. </t> 
<t> models of the internal structure of names have been used for cross document coreference <cite> and a goal in their own right <cite>. </t> <t> <cite> take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. </t> <t> their generative model designates a particular mention of a name as a  representative  and generates all other mentions from it according to an editing process. </t> 
<t> models of the internal structure of names have been used for cross document coreference <cite> and a goal in their own right <cite>. </t> <t> <cite>  uses a markov chain to generate # different components of people s names, again assuming that the  class of personal names can be pre distinguished using a name list. </t> <t> he infers coreference relationships  between similar names appearing in the same document, using the same notion of consistency between names as our model. </t> 
<t> models of the internal structure of names have been used for cross document coreference <cite> and a goal in their own right <cite>. </t> <t> <cite> operates only on authors of scienti c papers. </t> <t> their model accounts  for a wider variety of name variants than ours, including misspellings and initials. </t> <t> in addition, they  con rm our intuition that gibbs sampling for inference has insuf cient mobility  rather than using a heuristic algorithm as we do (see section #.#), they use a data driven block sampler. </t> 
<t> <cite> and <cite> propose automatically learning annotations  that add information to categories to improve monolingual parsing quality. </t> <t> since the parsing task requires selecting the most non annotated tree, the annotations add an additional level of structure that  must be marginalized during search. </t> <t> they demonstrate improvements in parse quality only when a variational approximation is used to select the most likely unannotated tree rather than simply stripping annotations from the map annotated tree. </t> 
<t> <cite> and <cite> propose automatically learning annotations  that add information to categories to improve monolingual parsing quality. </t> <t> since the parsing task requires selecting the most non annotated tree, the annotations add an additional level of structure that  must be marginalized during search. </t> <t> they demonstrate improvements in parse quality only when a variational approximation is used to select the most likely unannotated tree rather than simply stripping annotations from the map annotated tree. </t> 
<t> <cite> extend this work to lattice structures. </t> 
<t> more recently, work by  <cite> propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. </t> 
<t> <cite> extract nbest lists containing unique translations rather than unique derivations, while <cite>  use the minimum bayes risk decision rule to select the lowest risk (highest bleu score) translation rather than derivation from an n best list. </t> 
<t> <cite> extract nbest lists containing unique translations rather than unique derivations, while <cite>  use the minimum bayes risk decision rule to select the lowest risk (highest bleu score) translation rather than derivation from an n best list. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> <t> training the transliteration model is typically done under supervised settings <cite>, or  weakly supervised settings with additional temporal information <cite>. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> <t> training the transliteration model is typically done under supervised settings <cite>, or  weakly supervised settings with additional temporal information <cite>. </t> <t> in the transliteration community there are several works <cite> that show how the feature representation of a word pair  can be restricted to facilitate learning a string similarity model. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> <t> training the transliteration model is typically done under supervised settings <cite>, or  weakly supervised settings with additional temporal information <cite>. </t> 
<t> transliteration methods typically fall into two categories  generative approaches <cite> that try to  produce the target transliteration given a source language ne, and discriminative approaches <cite>, that try to identify the correct transliteration for a word in the source language given several  candidates in the target language. </t> <t> training the transliteration model is typically done under supervised settings <cite>, or  weakly supervised settings with additional temporal information <cite>. </t> <t> in the transliteration community there are several works <cite> that show how the feature representation of a word pair  can be restricted to facilitate learning a string similarity model. </t> <t> we follow the approach discussed in <cite>, which considers the feature representation as a structured prediction problem and  nds the set of optimal assignments (or  feature activations), under a set of legitimacy constraints. </t> <t> this approach stresses the importance of interaction between learning and inference, as the  model iteratively uses inference to improve the sample representation for the learning problem and uses  the learned model to improve the accuracy of the inference process. </t> 
<t> this has been shown both in supervised settings <cite> and unsupervised settings <cite> in which constraints are used to bootstrap the  model. </t> 
<t> this has been shown both in supervised settings <cite> and unsupervised settings <cite> in which constraints are used to bootstrap the  model. </t> <t> <cite> describes an unsupervised training of a constrained conditional model  (ccm), a general framework for combining statistical models with declarative constraints. </t> 
<t> this has been shown both in supervised settings <cite> and unsupervised settings <cite> in which constraints are used to bootstrap the  model. </t> 
<t> this has been shown both in supervised settings <cite> and unsupervised settings <cite> in which constraints are used to bootstrap the  model. </t> 
<t> in the transliteration community there are several works <cite> that show how the feature representation of a word pair  can be restricted to facilitate learning a string similarity model. </t> 
<t> more recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words  they use these representations to impose stronger constraints on topic assignments <cite>. </t> 
<t> more recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words  they use these representations to impose stronger constraints on topic assignments <cite>. </t> 
<t> in earlier topic modeling work such as latent dirichlet allocation (<cite>, documents are treated as bags of words, where each word receives a separate topic  assignment  the topic assignments are auxiliary variables to the main task of language modeling. </t> 
<t> more recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words  they use these representations to impose stronger constraints on topic assignments <cite>. </t> 
<t> for instance, content models <cite>  are implemented as hmms, where the states correspond to topics of domain speci c information,  and transitions re ect pairwise ordering preferences. </t> 
<t> even approaches that break text into contiguous chunks <cite> assign topics based on local context. </t> 
<t> more recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words  they use these representations to impose stronger constraints on topic assignments <cite>. </t> 
<t> for instance, content models <cite>  are implemented as hmms, where the states correspond to topics of domain speci c information,  and transitions re ect pairwise ordering preferences. </t> 
<t> modeling ordering constraints sentence ordering has been extensively studied in the context of probabilistic text modeling for summarization and generation <cite>. </t> 
<t> modeling ordering constraints sentence ordering has been extensively studied in the context of probabilistic text modeling for summarization and generation <cite>. </t> 
<t> modeling ordering constraints sentence ordering has been extensively studied in the context of probabilistic text modeling for summarization and generation <cite>. </t> 
<t> in earlier topic modeling work such as latent dirichlet allocation (<cite>, documents are treated as bags of words, where each word receives a separate topic  assignment  the topic assignments are auxiliary variables to the main task of language modeling. </t> 
<t> <cite> use the parameters of the maximum entropy model learned from the source domain as the means of a gaussian prior when training a new model on the target data. </t> 
<t> <cite>  rst train a ne tagger on the source domain, and then use the tagger s predictions as features for training and testing on the target domain. </t> 
<t> for generative syntactic parsing, <cite> have used the source domain data to construct a dirichlet prior for map estimation of the pcfg for the target domain. </t> 
<t> the only work we are aware of that directly models the different distributions in the source and the target domains is by daume <cite>. </t> <t> they assume a  truly source domain  distribution, a  truly target domain  distribution, and a  general domain  distribution. </t> <t> the source (target) domain data is generated from a mixture of the  truly source  (target) domain  distribution and the  general domain  distribution. </t> 
<t> <cite> propose a domain adaptation method that uses the unlabeled target instances to  infer a good feature representation, which can be regarded as weighting the features. </t> 
<t> wikipedia has been the subject of a considerable amount of research in recent years including <cite>, <cite>, <cite>, <cite>, and <cite>. </t> 
<t> wikipedia has been the subject of a considerable amount of research in recent years including <cite>, <cite>, <cite>, <cite>, and <cite>. </t> 
<t> wikipedia has been the subject of a considerable amount of research in recent years including <cite>, <cite>, <cite>, <cite>, and <cite>. </t> 
<t> wikipedia has been the subject of a considerable amount of research in recent years including <cite>, <cite>, <cite>, <cite>, and <cite>. </t> 
<t> wikipedia has been the subject of a considerable amount of research in recent years including <cite>, <cite>, <cite>, <cite>, and <cite>. </t> 
<t> the most relevant to our work are <cite>, toral and <cite>, and <cite>. </t> <t> more details follow, but it is worth noting that all known prior results are fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resource. </t> <t> toral and <cite> used wikipedia to create lists of named entities. </t> <t> they used the first sentence of wikipedia articles as likely definitions of the article titles, and used them to attempt to  classify the titles as people, locations, organizations, or none. </t> <t> unlike the method presented in this paper, their algorithm relied on wordnet (or an  equivalent resource in another language). </t> <t> the authors noted that their results would need to pass a manual supervision step before being useful for the ner task, and thus did not evaluate their results in the context of a full ner system. </t> 
<t> the most relevant to our work are <cite>, toral and <cite>, and <cite>. </t> <t> more details follow, but it is worth noting that all known prior results are fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resource. </t> <t> <cite>, by contrast to the above,  used wikipedia primarily for named entity disambiguation, following the path of bunescu and <cite>. </t> 
<t> the most relevant to our work are <cite>, toral and <cite>, and <cite>. </t> <t> more details follow, but it is worth noting that all known prior results are fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resource. </t> <t> similarly, <cite> used wikipedia, particularly the first sentence of each article, to create lists of entities. </t> <t> rather than building entity dictionaries associating words and  phrases to the classical ner tags (person, location, etc.) they used a noun phrase following  forms of the verb  to be  to derive a label. </t> <t> for example, they used the sentence  franz fischler ... </t> <t> is  an austrian politician  to associate the label  politician  to the surface form  franz fischler. </t> <t>   they proceeded to show that the dictionaries generated by their method are useful when integrated into an ner system. </t> <t> we note that their technique relies  upon a part of speech tagger, and thus was not appropriate for inclusion as part of our non english system. </t> 
<t> <cite>, by contrast to the above,  used wikipedia primarily for named entity disambiguation, following the path of bunescu and <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> quirk and <cite> investigated the  impact of parsing accuracy on statistical mt. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> though the evaluation of syntactic parsers has been a major concern in the parsing community, and a  couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation <cite>. </t> 
<t> many relation classi cation algorithms utilize wordnet. </t> <t> among the ## systems presented by the ## semeval teams, some utilized the manually provided wordnet tags for the dataset pairs (e.g., <cite>). </t> <t> in all cases, usage of wn tags improves the results signi cantly. </t> 
<t> some other systems that avoided using the labels used wn as a supporting resource for their algorithms <cite>. </t> 
<t> only three avoided wn altogether <cite>. </t> <t> many other works manually develop a set of heuristic features devised with some speci c relationship in mind, like a wordnet based meronymy feature <cite> or size of feature <cite>. </t> 
<t> some other systems that avoided using the labels used wn as a supporting resource for their algorithms <cite>. </t> 
<t> some other systems that avoided using the labels used wn as a supporting resource for their algorithms <cite>. </t> 
<t> other resources used for relationship discovery  include <cite>, thesauri or synonym sets <cite> and domainspeci c semantic hierarchies like <cite>. </t> <t> this hierarchy and a corresponding dataset were used in <cite> and <cite> for evaluation of their algorithms. </t> 
<t> other resources used for relationship discovery  include <cite>, thesauri or synonym sets <cite> and domainspeci c semantic hierarchies like <cite>. </t> 
<t> only three avoided wn altogether <cite>. </t> <t> many other works manually develop a set of heuristic features devised with some speci c relationship in mind, like a wordnet based meronymy feature <cite> or size of feature <cite>. </t> 
<t> only three avoided wn altogether <cite>. </t> <t> freely available tools like <cite> allow easy experimentation with common learning algorithms <cite>. </t> <t> in this paper we did not focus on a single ml algorithm, letting algorithm selection be automatically based on cross validation results on the training set, as in <cite> but using more algorithms and allowing a more  exible parameter choice. </t> 
<t> a wide variety of features are used by different  algorithms, ranging from simple bag of words frequencies to wordnet based features <cite>. </t> <t> <cite> proposed a different scheme with ## classes. </t> 
<t> other resources used for relationship discovery  include <cite>, thesauri or synonym sets <cite> and domainspeci c semantic hierarchies like <cite>. </t> 
<t> many relationship classi cation methods utilize some language dependent preprocessing, like deep or shallow parsing, part of speech tagging and named entity annotation <cite>. </t> <t> while  the obtained features were shown to improve classi cation performance, they tend to be language dependent and error prone when working on unusual  text domains and are also highly computationally intensive when processing large corpora. </t> 
<t> <cite>, numerous works have used patterns for discovery and identi cation of instances of semantic relationships (e.g., <cite>). </t> 
<t> <cite>, numerous works have used patterns for discovery and identi cation of instances of semantic relationships (e.g., <cite>). </t> 
<t> <cite>, numerous works have used patterns for discovery and identi cation of instances of semantic relationships (e.g., <cite>). </t> 
<t> strategies were developed for discovery of multiple patterns for some speci ed lexical relationship  <cite> and for unsupervised pattern ranking <cite>. </t> <t> this hierarchy and a corresponding dataset were used in <cite> and <cite> for evaluation of their algorithms. </t> 
<t> strategies were developed for discovery of multiple patterns for some speci ed lexical relationship  <cite> and for unsupervised pattern ranking <cite>. </t> 
<t> <cite>, numerous works have used patterns for discovery and identi cation of instances of semantic relationships (e.g., <cite>). </t> 
<t> <cite> discover relationship instances by clustering entities appearing in similar contexts. </t> 
<t> various learning algorithms have been used for relation classi cation. </t> <t> common choices include variations of <cite>, decision trees and memory based learners. </t> 
<t> various learning algorithms have been used for relation classi cation. </t> <t> common choices include variations of <cite>, decision trees and memory based learners. </t> <t> this hierarchy and a corresponding dataset were used in <cite> and <cite> for evaluation of their algorithms. </t> 
<t> <cite> use pattern clusters to de ne general relationships, but these are speci c to a given concept. </t> 
<t> <cite> we present  an approach to extract pattern clusters from an untagged corpus. </t> 
<t> <cite> proposed a two level hierarchy with # <cite> classes at the top (bottom) levels # </t> 
<t> freely available tools like <cite> allow easy experimentation with common learning algorithms <cite>. </t> 
<t> the most recent dataset has been developed for semeval ## <cite>. </t> 
<t> <cite> improve on this model by treating image regions and keywords as a bi text and using the em algorithm to construct an image region word dictionary. </t> 
<t> more sophisticated graphical models <cite> have also been employed including gaussian mixture models (gmm) and latent dirichlet allocation (lda). </t> 
<t> the earliest approaches are closely related to image classi cation <cite>, where pictures are assigned  a set of simple descriptions such as indoor, outdoor, landscape, people, animal. </t> <t> a binary classi er is trained for each concept, sometimes in a  one vs  all  setting. </t> 
<t> the co occurrence model <cite> collects co occurrence counts between words  and image features and uses them to predict annotations for new images. </t> 
<t> finally, relevance models originally developed for information retrieval, have been successfully applied to image annotation <cite>. </t> 
<t> finally, relevance models originally developed for information retrieval, have been successfully applied to image annotation <cite>. </t> 
<t> the earliest approaches are closely related to image classi cation <cite>, where pictures are assigned  a set of simple descriptions such as indoor, outdoor, landscape, people, animal. </t> <t> a binary classi er is trained for each concept, sometimes in a  one vs  all  setting. </t> 
<t> another important group of related work is on using syntactic dependency features in a vector space model for measuring word similarity, e.g., <cite>, <cite>, <cite>, and <cite>. </t> 
<t> the same approach is applied to classifying noun modi er pairs   using the diverse dataset of <cite>, turney and littman achieve f measures of ##.#  with ##  ne grained relations, and ##.#  with # course grained relations. </t> 
<t> <cite> characterize the relationship between two words as a vector with coordinates corresponding to the web frequencies of ###  xed phrases like  x for y   and  y for x  instantiated from a  xed set of ## joining terms like for, such as, not the, is  , etc. </t> <t> these vectors are used in a  nearest neighbor classi er to solve sat verbal analogy problems, yielding ##  accuracy. </t> 
<t> <cite> presents an unsupervised algorithm for mining the web for patterns expressing implicit semantic relations. </t> <t> for example, cause (e.g., cold virus) is best characterized by  y   causes x , and  y in   early x  is the best pattern for temporal (e.g., morning frost). </t> <t> with # classes, he achieves f measure ##.# . </t> 
<t> <cite> reduces the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set  of, for, in,  at, on, from, with or about. </t> <t> he achieved ##  accuracy using corpus frequencies. </t> 
<t> <cite> extends the above approach by introducing the latent relational analysis (lra), which  uses automatically generated synonyms, learns suitable patterns, and performs singular value decomposition in order to smooth the frequencies. </t> 
<t> the full algorithm consists of ## steps described in detail in <cite>. </t> <t> when applied to sat questions, it achieves the state of the art accuracy of ## . </t> <t> on the diverse dataset, it yields an f measure of ##.#  with ## classes, and ##  with # classes. </t> 
<t> <cite> used a discriminative classi er to assign ## relations for noun compounds from biomedical text, achieving ##  accuracy. </t> 
<t> <cite> reported ##  accuracy with  a  descent of hierarchy  approach which characterizes the relationship between the nouns in a bioscience noun noun compound based on the mesh categories the nouns belong to. </t> 
<t> this result was improved to ##.#  by <cite> who used web derived n gram frequencies. </t> 
<t> <cite> use syntactic clues and the identity of the nouns in a nearest neighbor classi er, achieving ## ##  accuracy. </t> 
<t> <cite> characterized the semantic relationship in a noun noun compound using the verbs connecting the two nouns by comparing them to prede ned seed verbs. </t> <t> their approach is highly resource intensive (uses wordnet, corelex and moby s thesaurus), and is quite sensitive to the seed set of verbs  on a collection of ### examples and ## relations, they achieved ##.#  accuracy with ## seed verbs, but only ##.#  with ## seed verbs. </t> 
<t> <cite> extract paraphrases from dependency tree paths whose ends contain semantically similar sets of words by generalizing over these ends. </t> 
<t> <cite> apply both classic (svm and  decision trees) and novel supervised models (semantic scattering and iterative semantic specialization), using wordnet, word sense disambiguation, and a set of linguistic features. </t> <t> they test their system against both lauer s # prepositional paraphrases and another set of ## semantic relations, achieving up to ##  accuracy on the latter. </t> 
<t> in a previous work <cite>, we have shown that the relationship between the nouns  in a noun noun compound can be characterized using verbs extracted from the web, but we provided no formal evaluation. </t> 
<t> another important group of related work is on using syntactic dependency features in a vector space model for measuring word similarity, e.g., <cite>, <cite>, <cite>, and <cite>. </t> 
<t> another important group of related work is on using syntactic dependency features in a vector space model for measuring word similarity, e.g., <cite>, <cite>, <cite>, and <cite>. </t> 
<t> the approach is extended by <cite>, who use named  entity recognizers and look for anchors belonging to matching semantic classes, e.g., location, organization. </t> 
<t> the idea is further extended by <cite>, who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. </t> 
<t> the work in <cite> clusters do cuments based on the entity (person, organization, and location) names and can be applied to the disambiguation  of semi structured documents, such as web pages. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> <t> the approach of <cite> is based on exploiting the link structure  of pages on the web, with the hyp otheses that web pages belonging to the same real person are more likely to be linked  together. </t> <t> three algorithms are presented for disambiguation, the  rst is jus t exploiting the link structure of web pages and forming clusters based on link analysis, the second algorithm is based on word similarities between documents  and does clustering using agglomerative conglomerative double clustering (a dc), the third approach combines link analysis with a dc clustering. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> 
<t> recently, researchers have started to use external databases,  such as ontology and web search engine in order to improve the classi cation and clustering qualities in different domains <cite>. </t> <t> for example, querying the web and  utilizing the search results are used for word sense disambiguation (wsd) <cite> and record linkage in publications domain <cite>. </t> <t> in <cite> the authors prop osed to use the co occurrence counts  for disambiguation of different meanings of words. </t> <t> the authors used web based similarity measures like webjaccard, webdice, and so on. </t> <t> these measures are also utilized as features for the svm based trainer along with a set of token based features, where the trainer learns the probability of two terms being the same. </t> 
<t> recently, researchers have started to use external databases,  such as ontology and web search engine in order to improve the classi cation and clustering qualities in different domains <cite>. </t> <t> for example, querying the web and  utilizing the search results are used for word sense disambiguation (wsd) <cite> and record linkage in publications domain <cite>. </t> <t> similarly the approach in <cite> uses the web as a knowledge source for data cleaning. </t> <t> the study proposed a way to for mulate queries and used some standard measures like tf idf similarity to compute the similarity of two different references to an entity. </t> 
<t> the ne based single link clustering was the one of the top three systems <cite>, because the named entity network alone enables to identify most of the individuals. </t> <t> the work in <cite> is a complementary work to the one in <cite>. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> <t> in sem eval ####, a workshop on weps task was held <cite>. </t> <t> sixteen different teams from different universities have participated to the task. </t> <t> the participated systems utilized  named entities, tokens, urls, etc that exist in the documents. </t> <t> it has been shown that as the extracted information  increases the quality of the clustering increases. </t> <t> use of different ne recognition tools affects the results of clustering as well. </t> 
<t> recently, researchers have started to use external databases,  such as ontology and web search engine in order to improve the classi cation and clustering qualities in different domains <cite>. </t> 
<t> recently, researchers have started to use external databases,  such as ontology and web search engine in order to improve the classi cation and clustering qualities in different domains <cite>. </t> <t> for example, querying the web and  utilizing the search results are used for word sense disambiguation (wsd) <cite> and record linkage in publications domain <cite>. </t> <t> hence, the study in kanani and mccallum <cite> tries to solve the problem in the case of limited resources. </t> <t> the suggested algorithm increased the accuracy of data cleaning while keeping the number of queries to a search engine minimal. </t> <t> the work in <cite> is a complementary work to the one in <cite>. </t> 
<t> in our previous work we also have developed interrelated techniques to solve various entity resolution challenges, e.g. <cite>. </t> 
<t> in our previous work we also have developed interrelated techniques to solve various entity resolution challenges, e.g. <cite>. </t> 
<t> in our previous work we also have developed interrelated techniques to solve various entity resolution challenges, e.g. <cite>. </t> 
<t> in our previous work we also have developed interrelated techniques to solve various entity resolution challenges, e.g. <cite>. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> 
<t> in our previous work we also have developed interrelated techniques to solve various entity resolution challenges, e.g. <cite>. </t> 
<t> in our previous work we also have developed interrelated techniques to solve various entity resolution challenges, e.g. <cite>. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> 
<t> there are several research efforts that address speci cally web person search and related challenges <cite>. </t> 
<t> some recent works have considered using user prior knowledge to bootstrap learning but they typically assume that prior knowledge is given at the outset <cite>. </t> <t> the proposed techniques typically involve   soft labeling  instances containing the user labeled features by assigning them to categories associated with those of the features. </t> 
<t> some recent works have considered using user prior knowledge to bootstrap learning but they typically assume that prior knowledge is given at the outset <cite>. </t> <t> the proposed techniques typically involve   soft labeling  instances containing the user labeled features by assigning them to categories associated with those of the features. </t> 
<t> our proposed method is an instance of query based learning <cite> and an extension of standard ( pool based ) active learning which  focuses on selective sampling of instances from a pool of unlabeled data <cite>. </t> 
<t> more recently the works of magennis and rijsbergen <cite> and ruthven <cite> try to design experiments for term feedback in the ad hoc retrieval task in  a manner similar to ours. </t> <t> magennis and rijsbergen had some de ciencies in their approach, a fact that they acknowledge. </t> <t> ruthven improved on their experimental setup and found that term feedback using an oracle could give performance improvements even over automatic query expansion. </t> <t> however, he found that users cannot  mark the terms required by the optimal query with reasonable precision. </t> 
<t> more recently the works of magennis and rijsbergen <cite> and ruthven <cite> try to design experiments for term feedback in the ad hoc retrieval task in  a manner similar to ours. </t> <t> magennis and rijsbergen had some de ciencies in their approach, a fact that they acknowledge. </t> <t> ruthven improved on their experimental setup and found that term feedback using an oracle could give performance improvements even over automatic query expansion. </t> <t> however, he found that users cannot  mark the terms required by the optimal query with reasonable precision. </t> 
<t> the work of huang and mitchell <cite> is similar in that a user is queried on features and documents at the same time. </t> 
<t> some recent work has proposed extending the query model to include feature as well as document level feedback <cite>. </t> <t> that work largely demonstrated the need to consider such a dual mode of feedback showing the bene ts of such an approach. </t> 
<t> our proposed method is an instance of query based learning <cite> and an extension of standard ( pool based ) active learning which  focuses on selective sampling of instances from a pool of unlabeled data <cite>. </t> 
<t> sebastiani s survey paper <cite> provides an overview of techniques in text categorization, a research problem that sits in the joint space of machine learning and information retrieval. </t> 
<t> automated text classi cation is  a supervised learning task, de ned as automatically assigning pre de ned category labels to documents <cite>. </t> <t> yang and liu <cite> conducted a controlled study of # well known text classi cation methods  support vector machine (svm), k nearest neighbor (knn), a neural network (nnet), linear least square fit (llsf) mapping, and naive bayes (nb). </t> <t> their results show that svm, knn, and llsf signi cantly  outperform nnet and nb when the number of positive training examples per category are small <cite>. </t> 
<t> yang and pedersen studied   ve feature selection methods for aggressive dimensionality reduction  term selection based on document frequency (df), information gain (ig), mutual information, a   # </t> <t> test (ciii), and term strength <cite>. </t> <t> using the knn and linear least squares fit mapping (llsf) techniques, they found  ig and ciii most effective in aggressive term removal without losing categorization accuracy. </t> <t> they also found that df thresholding, the simplest method with the lowest cost in computation could reliably replace ig or ciii when the computations of those measure were expensive. </t> 
<t> in ####, topic detection and tracking (tdt) research was extended from english to chinese <cite>. </t> 
<t> in topic tracking, a system is given several (e.g., # #) initial seed documents and asked to monitor the incoming news stream for further  documents on the same topic <cite>, the effectiveness of crosslanguage classi ers (trained on chinese data and tested on english) was worse than monolingual classi ers. </t> 
<t> popular techniques for text classi cation include probabilistic classi ers (e.g, naive bayes classi ers), decision tree classi ers, regression methods (e.g., linear least square fit), on line ( ltering) methods (e.g., perceptron), the rocchio method, neural networks, example based classi ers (e.g., knn),  support vector machines, bayesian inference networks, genetic algorithms, and maximum entropy modelling <cite>. </t> 
<t> gliozzo and strapparava <cite> investigated english and italian cross language text classi cation by using comparable corpora and bilingual dictionaries (multiwordnet and the  collins english italian bilingual dictionary). </t> <t> the comparable corpus was used for latent semantic analysis which  exploits the presence of common words among different languages in the term by document matrix to create a space in which documents in both languages were represented. </t> <t> their cross language classi er, either trained on english and tested on italian, or trained on italian and tested on english, achieved an f# of #.##, worse than their monolingual classi er <cite>. </t> 
<t> olsson et al. <cite> classi ed czech documents using english training data. </t> <t> they translated czech document vectors into english document vectors using a probabilistic dictionary which contained conditional word translation probabilities for ##,### word translation pairs. </t> <t> their  concept label  knn classi er <cite> achieved precision of #.##, which is ##  of the precision of a corresponding monolingual classi er. </t> 
<t> bel et al. <cite> studied an english spanish bilingual classi cation task for the international labor organization (ilo) corpus, which had ## categories. </t> <t> they tried two approaches   a poly lingual approach in which both english and spanish training and test data were available, and cross lingual approach in which training examples were available in one  language. </t> <t> using the poly lingual approach,inwhichasingle classi er was built from a set of training documents in both languages, their winnow classi er, which, like svm, computes an optimal linear separator in the term space between positive and negative training examples, achieved f # of #.###, worse than their monolingual english classi er <cite> but better than their monolingual spanish classi er <cite>. </t> <t> for the cross lingual approach, they used two translation methods terminology translation and pro le translation. </t> 
<t> rigutini et al. <cite> studied english and italian cross language text classi cation in which training data were available in  english and the documents to be classi ed were in italian. </t> <t> they used a naive bayes classi er to classify english  ware, auto and sports. </t> <t> english training data <cite> were translated into italian using o ce translator idiomax. </t> <t> their cross language classi er  was created using expectation maximation (em), with english training data (translated into italian) used to initialize the em iteration on the unlabeled italian documents. </t> <t> once the italian documents were labeled, these documents were  used to train an italian classi er. </t> <t> the cross language classi er performed slightly worse than monolingual classi er, probably due to the quality of their translated italian data. </t> 
<t> some previous work <cite> has used heuristic methods like manually  compiled rules to cluster evidence from similar answer candidates. </t> <t> graph based clustering was also used to consider non transitiveness in similarity <cite>. </t> 
<t> some previous work <cite> has used heuristic methods like manually  compiled rules to cluster evidence from similar answer candidates. </t> 
<t> in many systems, cutoff threshold has been used to select the most probably top n answers <cite> or exhaustive search to  nd all possible candidates has been applied <cite>. </t> 
<t> in many systems, cutoff threshold has been used to select the most probably top n answers <cite> or exhaustive search to  nd all possible candidates has been applied <cite>. </t> 
<t> one of the most common approaches relies on wordnet, cyc and gazetteers for answer validation or answer reranking. </t> <t> in this approach, answer candidates are either removed or discounted if they are not found within the resource s hierarchy corresponding to the expected answer type of the question <cite>. </t> 
<t> one of the most common approaches relies on wordnet, cyc and gazetteers for answer validation or answer reranking. </t> <t> in this approach, answer candidates are either removed or discounted if they are not found within the resource s hierarchy corresponding to the expected answer type of the question <cite>. </t> 
<t> one of the most common approaches relies on wordnet, cyc and gazetteers for answer validation or answer reranking. </t> <t> in this approach, answer candidates are either removed or discounted if they are not found within the resource s hierarchy corresponding to the expected answer type of the question <cite>. </t> 
<t> wikipedia s  structured information has been used for answer type checking <cite>. </t> 
<t> there was an attempt to combine geographical databases with wordnet for type checking of lo cation questions <cite>. </t> <t> however, the experimental results  show that the combination did not improve performance because of the increased semantic ambiguity which accompanies broader coverage of location names. </t> <t> this is evidence  that the method of combining potential answers may matter as much as the choice of resources. </t> 
<t> one of the most common approaches relies on wordnet, cyc and gazetteers for answer validation or answer reranking. </t> <t> in this approach, answer candidates are either removed or discounted if they are not found within the resource s hierarchy corresponding to the expected answer type of the question <cite>. </t> 
<t> the  web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords <cite>. </t> 

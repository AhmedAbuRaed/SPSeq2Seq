extracting structural paraphrases from aligned monolingual corpora. we present an approach for automatically   learning paraphrases from aligned monolingual corpora. our algorithm works   by generalizing the syntactic paths between corresponding anchors in aligned  sentence pairs. compared to previous  work, structural paraphrases generated by  our algorithm tend to be much longer   on average, and are capable of capturing long distance dependencies. in addition to a standalone evaluation of our  paraphrases, we also describe a question   answering application currently under development that could immensely bene t from automatically learned structural  paraphrases. we present an unsupervised method for acquiring structural paraphrases, or fragments of syntactic trees that are roughly semantically equivalent, from  aligned monolingual corpora. the structural paraphrases produced by our algorithm are similar to the s rules advocated by katz and levin for question answering (####), except that our paraphrases are  automatically generated. because there is disagreement regarding the exact de nition of paraphrases (dras, ####), we employ that operating de nition  that structural paraphrases are roughly interchangeable within the speci c con guration of syntactic structures that they specify. our approach is a synthesis of techniques developed by barzilay and mckeown (####) and lin  and pantel (####), designed to overcome the limitations of both. in addition to the evaluation of paraphrases generated by our method, we also describe  a novel information retrieval system under development that is designed to take advantage of structural paraphrases.
extracting paraphrases from a parallel corpus. we present an unsupervised learning algorithm for identi cation of paraphrases from a corpus of multiple english translations of  the same source text. our approach  yields phrasal and single word lexical   paraphrases as well as syntactic paraphrases. this paper presents a corpus based method for automatic extraction of paraphrases. we use a  large collection of multiple parallel english translations of novels # this corpus provides many instances of paraphrasing, because translations preserve the meaning of the original source, but  may use different words to convey the meaning. however, our parallel corpus is far from the clean parallel corpora used in mt. we will return to this point later in section #. based on the speci cs of our corpus, we developed an unsupervised learning algorithm for paraphrase extraction. we base our method for paraphrasing extraction  on the assumption that phrases in aligned sentences which appear in similar contexts are paraphrases. a novel feature of our approach is the ability to extract multiple kinds of paraphrases   identi cation of lexical paraphrases. in contrast to earlier work on similarity, our approach allows identi cation of multi word paraphrases, in addition to single words, a challenging issue for corpus based techniques. our approach yields a set of paraphrasing patterns by extrapolating the syntactic and morphological structure of extracted paraphrases. in the following sections, we provide an overview of existing work on paraphrasing, then we describe data used in this work, and detail our  paraphrase extraction technique. we present results of our evaluation, and conclude with a discussion of our results. in this paper, we presented a method for corpusbased identi cation of paraphrases from multiple english translations of the same source text. we showed that a co training algorithm based on  contextual and lexico syntactic features of paraphrases achieves high performance on our data. the wide range of paraphrases extracted by our  algorithm sheds light on the paraphrasing phenomena, which has not been studied from an empirical perspective. we will also investigate a more powerful representation of contextual features. this will allow us to extract macro syntactic paraphrases in addition to local paraphrases which are currently produced by the algorithm.
improved statistical machine translation using paraphrases. we show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. our results show that augmenting a stateof the art smt system with paraphrases  leads to signi cantly improved coverage  and translation quality. for a training   corpus with ##,### sentence pairs we increase the coverage of unique test set unigrams from ##  to ## , with more than  half of the newly covered items accurately  translated, as opposed to none in current  approaches. here we addressthe problem of unknown phrases. speci cally we show that upon encountering an unknown source phrase, we can substitute a paraphrase for it and then proceed using the translation of that  paraphrase. we derive these paraphrases from resources that are external to the parallel corpus that the translation model is trained from, and we are able to exploit (potentially more abundant) parallel corpora from other language pairs to do so. in this paper we    de ne a method for incorporating paraphrases of unseen source phrases into the statistical machine translation process. show that by translating paraphrases we achieve a marked improvement in coverage and translation quality, especially in the case of unknown words which to date have been left untranslated. argue that while we observe an improvement in bleu score, this metric is particularly poorly suited to measuring the sort of improvements that we achieve.
paraphrasing with bilingual parallel corpora. we show that this task can be  done using bilingual parallel corpora, a  much more commonly available resource. using alignment techniques from phrasebased statistical machine translation, we  show how paraphrases in one language  can be identi ed using a phrase in another   language as a pivot. we de ne a paraphrase probability that allows paraphrases  extracted from a bilingual parallel corpus   to be ranked using translation probabilities, and show how it can be re ned to  take contextual information into account. we evaluate our paraphrase extraction and  ranking methods using a set of manual   word alignments, and contrast the quality with paraphrases extracted from automatic alignments. in this paper we introduce a novel method for extracting paraphrases that uses bilingual parallel corpora. instead of relying on scarce monolingual parallel data, our method utilizes the abundance of bilingual  parallel data that is available. this allows us to create a much larger inventory of phrases that is applicable to a wider range of texts. our method for identifying paraphrases is an  extension of recent work in phrase based statistical machine translation (koehn et al., ####). the  essence of our method is to align phrases in a bilingual parallel corpus, and equate different english phrases that are aligned with the same phrase in the  other language. ing when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of diab and resnik (####) which posits different word  senses when a single english word maps onto different words in the foreign language (we return to this point in section #.#). the remainder of this paper is as follows  section # contrasts our method for extracting paraphrases with the monolingual case, and describes how we rank the extracted paraphrases with a probability assignment. section # describes our experimental setup and includes information about how phrases were selected, how we manually aligned parts of the  bilingual corpus, and how we evaluated the paraphrases. section # gives the results of our evaluation and gives a number of example paraphrases extracted with our technique.
unsupervised construction of large paraphrase corpora  exploiting massively parallel news sources. we investigate unsupervised techniques for  acquiring monolingual sentence level  paraphrases from a corpus of temporally and  topically clustered news articles collected from  thousands of web based news sources. we  evaluate both datasets using a word alignment  algorithm and a metric borrowed from machine  translation. our goal here is rather different  our interest lies in constructing a monolingual broad domain corpus of pairwise aligned sentences. # in what follows we compare two strategies for unsupervised construction of such a corpus, one employing string similarity and the other associating sentences that may overlap very little at the string level. we measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text. we show that although the edit distance corpus is well suited as training data for the alignment algorithms currently used in smt, it is an incomplete source of information about paraphrase relations, which exhibit many of the characteristics of comparable bilingual corpora or free translations. we conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques. however, there is a disparity between the kinds of paraphrase alternations that we need to be able to align and those that we can already align well using current smt techniques. hand evaluation, though, indicates that many of the phenomena that we are interested in learning may be absent from this l## data. techniques like our f# extraction strategies appear to extract a more diverse variety of data, but yield more noise. we believe that an approach with the strengths of both methods would lead to significant improvement in paraphrase identification and generation. we believe that this work has potential impact on the fields of summarization, information retrieval, and question answering. our ultimate goal is to apply current smt techniques to the problems of paraphrase recognition and generation. we feel that this is a natural extension of the body of recent developments in smt  perhaps explorations in monolingual data may have a reciprocal impact. in this paper we have described just one example of a class of data extraction techniques that we hope will scale to this task.
dirt   discovery of inference rules from text. in this paper, we propose an unsupervised method for discovering  inference rules from text, such as  x is author of y    x wrote y ,   x solved y    x found a solution to y , and  x caused y    y is  triggered by x . our algorithm is based on an  extended version of harris  distributional hypothesis, which  states that words that occurred in the same contexts tend to be  similar. instead of using this hypothesis on words, we apply it to  paths in the dependency trees of a parsed corpus. many algorithms have been proposed to mine textual data. we propose an unsupervised method for discovering inference rules, such as  x is author of y   x wrote y ,  x solved y   x found a solution to y , and  x caused y y is triggered by x . we call  x wrote y   x is the author of y  an inference rule. in this paper, we use the term inference rule because we also want to include relationships that are not exactly paraphrases, but are nonetheless related and are potentially useful to information retrieval systems. our goal is to automatically discover such rules. in this paper, we present an unsupervised algorithm, dirt, for discovery of inference rules from text. our algorithm is a generalization of previous algorithms for finding similar words [##][##][##]. instead of applying the distributional hypothesis to words, we apply it to paths in dependency trees. essentially, if two paths tend to link the same sets of words, we hypothesize that their meanings are similar. since a path represents a binary relationship, we generate an inference rule for each pair of similar paths. the remainder of this paper is organized as follows. in the next section, we review previous work. in section #, we define paths in dependency trees and describe their extraction from a parsed corpus. section # presents the dirt system and a comparison of our system s output with manually generated paraphrase expressions is shown in section #. finally, we conclude with a discussion of future work. to the best of our knowledge, this is the first attempt to discover such knowledge automatically from a large corpus of text. we introduced the extended distributional hypothesis, which states that paths in dependency trees have similar meanings if they tend to connect similar sets of words. treating paths as binary relations, our algorithm is able to generate inference rules by searching for similar paths. our experimental results show that the extended distributional hypothesis can indeed be used to discover very useful inference rules, many of which, though easily recognizable, are difficult for humans to recall. for example, instead of generating a rule  x manufactures y   x s y factory , we may want to generate a rule with an additional clause   x manufactures y   x s y factory, where y is an artifact .
ordering circumstantials for multi document summarization. in this article, we introduce sentence fusion, a novel text to text generation technique for synthesizing common information across documents. in this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization. instead, we want a fine grained approach that can identify only those pieces of sentences that are common.
improved statistical alignment models. in this article we present a new parallel corpus with paraphrase annotations. we adopt a definition of paraphrase based on word alignments and show that it yields high inter annotator agreement. as kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. we discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and f#) and also in developing linguistically rich paraphrase models based on syntactic structure. in this article we present a resource that could potentially be used to address these problems.
the pyramid method  incorporating human content selection variation in summarization evaluation. in this article, we address these very questions by proposing a method for analysis of multiple human abstracts into semantic content units. such analysis allows us not only to quantify human variation in content selection, but also to assign empirical importance weight to different content units. we discuss the reliability of content unit annotation, the properties of pyramid scores, and their correlation with other evaluation methods. as the field turns to the development of more advanced non extractive summarizers, we will clearly need to move to a more sophisticated evaluation method which can handle semantic equivalence at varying levels of granularity. the pyramid method, the description and analysis of which are the focus of this paper, provides a unified framework for addressing the issues outlined above. our analysis of the scoring method shows that despite the inherent difficulty of the task, it can be performed reliably. in the remainder of this article, we first define the analysis method, showing how it is used to create pyramids and score system output (section #). we then present our analysis confirming the need for multiple models in section #, turn to a discussion of the reliability of manual content analysis of automated summaries (section #), and before closing, discuss other evaluation approaches and compare them with the pyramid method approach (section #). conclusions in this article, we presented the pyramid evaluation method, which is based on the semantic analysis of multiple human models. we demonstrated that the semantic analysis into content units can be performed reliably and that pyramid scores lead to stable evaluation results. we hope that in the future data from pyramid annotations will be also used to further research in abstractive summarization through the study of different verbalizations of the same content and the packaging of information in sentences.
thumbs up? we consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review   is positive or negative. using movie reviews as data, we  nd that standard machine learning techniques de nitively out  perform human produced baselines. however, the three machine learning methods   we employed (naive bayes, maximum entropy classi cation, and support vector machines) do not perform as well on sentiment  classi cation as on traditional topic based  categorization. we conclude by examining   factors that make the sentiment classi cation problem more challenging. in this paper, we examine the effectiveness of applying machine learning techniques to the sentiment classi cation problem. so, apart from presenting our results obtained via machine learning techniques,  we also analyze the problem to gain a better understanding of how di cult it is.
learning extraction patterns for subjective expressions. this paper presents a bootstrapping process   that learns linguistically rich extraction patterns for subjective (opinionated) expressions. consequently, we believe  that subjectivity learning systems must be trained on extremely large text collections before they will acquire a  subjective vocabulary that is truly broad and comprehensive in scope. to address this issue, we have been exploring the use of bootstrapping methods to allow subjectivity classi ers  to learn from a collection of unannotated texts. our research uses high precision subjectivity classi ers to automatically identify subjective and objective sentences in unannotated texts. this process allows us to generate a  large set of labeled sentences automatically. the second emphasis of our research is using extraction patterns to represent subjective expressions. using the (automatically) labeled sentences as training data, we apply an extraction pattern learning  algorithm to automatically generate patterns representing subjective expressions. our experimental results show that  this bootstrappingprocess increases the recall of the highprecision subjective sentence classi er with little loss in  precision. we also  nd that the learned extraction patterns capture subtle connotationsthat are more expressive than the individual words by themselves. this paper is organized as follows. section # overviews our general approach, describes the high precision subjectivity classi ers, and  explains the algorithmfor learning extraction patterns associated with subjectivity. section # describes the data that we use, presents our experimental results, and shows examples of patterns that are learned. finally, section # summarizes our  ndings and conclusions. first, we demonstrated that high precision subjectivity classi cation can beusedto generate a largeamount of labeledtrainingdata forsubsequentlearningalgorithms to exploit. second, we showed that an extraction pattern learning technique can learn subjective expressions that are linguistically richer than individual words or  xed phrases. we found that similar expressions may behave very differently, so that one expression may be strongly indicative of subjectivity  but the other may not. third, we augmented our original high precision subjective classi er with these newly learned extraction patterns. in future work, we plan to experiment with  different con gurations of these classi ers, add new subjective language learners in the bootstrapping process,  and address the problem of how to identify new objective sentences during bootstrapping.
thumbs up or thumbs down? this paper presents a simple unsupervised  learning algorithm for classifying reviews   as recommended (thumbs up) or not recommended (thumbs down). in  this paper, the semantic orientation of a   phrase is calculated as the mutual information between the given phrase and the  word  excellent  minus the mutual  information between the given phrase and  the word  poor . in this paper, i present a simple unsupervised  learning algorithm for classifying a review as recommended or not recommended. this paper introduces a simple unsupervised learning algorithm for rating a review as thumbs up or down.
determining the sentiment of opinions. we  present a system that, given a topic,  automatically finds the people who hold  opinions about that topic and the sentiment  of each opinion. we experiment with  various models of classifying and  combining sentiment at word and sentence  levels, with promising results. we wish to study opinion in general  our work most closely resembles that of (yu and hatzivassiloglou ####). since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. for our purposes, we describe an opinion as a quadruple [topic, holder, claim, sentiment] in which the holder believes a claim about the topic, and in many cases associates a sentiment, such as good or bad, with the belief. for example, the following opinions contain claims but no sentiments   i believe the world is flat   the gap is likely to go bankrupt   bin laden is hiding in pakistan   water always flushes anti clockwise in the southern hemisphere  like yu and hatzivassiloglou (####), we want to automatically identify sentiments, which in this work we define as an explicit or implicit expression in text of the holder s positive, negative, or neutral regard toward the claim about the topic. (other sentiments we plan to study later. ) sentiments always involve the holder s emotions or desires, and may be present explicitly or only implicitly   i think that attacking iraq would put the us in a difficult position  (implicit)  the us attack on iraq is wrong  (explicit)  i like ike  (explicit)  we should decrease our dependence on oil  (implicit)  reps. tom petri and william f. goodling asserted that counting illegal aliens violates citizens  basic right to equal representation  (implicit) in this paper we address the following challenge problem. to avoid the problem of differentiating between shades of sentiments, we simplify the problem to  identify just expressions of positive, negative, or neutral sentiments, together with their holders. we approach the problem in stages, starting with words and moving on to sentences. we take as unit sentiment carrier a single word, and first classify each adjective, verb, and noun by its sentiment. we experimented with several classifier models. when combining word level sentiments, we therefore first determine for each holder a relevant region within the sentence and then experiment with various models for combining word sentiments. we plan to extend our work to more difficult cases such as sentences with weak opinion bearing words or sentences with multiple opinions about a topic. to improve identification of the holder, we plan to use a parser to associate regions more reliably with holders. we plan to explore other learning techniques, such as decision lists or svms.
converting dependency structures to phrase structures. in this paper, we present the first experimental results classifying the strength of opinions and other types of subjectivity and classifying the subjectivity of deeply nested clauses. we use a wide range of features, including new syntactic features developed for opinion recognition. in ## fold cross validation experiments using support vector regression, we achieve improvements in mean squared error over baseline ranging from ##  to ## .
pulse  mining customer opinions from free text. we present a prototype system, code named pulse,formining topics and sentiment orientation jointly from free text customer feedback. we describe the application of the prototype system to a database   of car reviews. we describe a simple but effective technique for clustering sentences, the application of a bootstrapping  approach to sentiment classi cation, and a novel user interface. paying people to mine this free form information can be extremely expensive, and given the high volume of such free text is only feasible by careful sampling. # the project  that we describe in this paper, code named pulse, combines the two dimensions of topic and sentiment and presents the results in an intuitive visualization. document level assessment, which is the focus of most sentiment classi cation studies, is too coarse for our purposes. in a review document, for example, we often  nd mixed positive and negative assessments such as   overall the car is a good car. # as we will discuss in further detail below, sentence level granularity of analysis allows the discovery of new information even in those scenarios where an overall product rating is already provided at the document level. we  rst describe the data to which pulse has been applied (section #). we then describe the prototype system, consisting of a visualization component (section #.#), a simple but effective clustering algorithm (section #.#), and a machine learned classi er that can be rapidly trained for a new domain (section #.#) by bootstrapping from a relatively small set of labeled data. we applied pulse to a sample of the car reviews database[#]. for this reason we ignore the document level scores and annotated a randomly selected sample of #,### sentences for sentiment. we measured pair wise inter annotator agreement on a separate randomly selected sample of ### sentences using cohen s kappa score.
toward opinion summarization  linking the sources. we target the problem of linking source  mentions that belong to the same entity  (source coreference resolution), which is  needed for creating opinion summaries. in   this paper we describe how source coreference resolution can be transformed into   standard noun phrase coreference resolution, apply a state of the art coreference  resolution approach to the transformed  data, and evaluate on an available corpus  of manually annotated opinions. in this work we address the dearth of approaches for summarizing opinion information. in particular, we focus on the problem of source coreference resolution, i.e. given the associated opinion expressions and their polarity, this source coreference information is the critical  knowledge needed to produce the summary of figure # (although the two target mentions, bulgaria and our country, would also need to be identi ed as coreferent). our work is concerned with  ne grained expressions of opinions and assumes that a system can rely on the results of effective opinion and source extractors such as those described in riloff and wiebe (####), bethard et al. presented  with sources of opinions, we approach the problem of source coreference resolution as the closely target delaying of bulgaria s accession to the eu] would be a serious mistake  [ source bulgarian prime minister sergey stanishev] said in an interview for the german daily suddeutsche zeitung. [ target our country] serves as a model and encourages countries from the region to follow despite the dif culties , [ source he] added. nevertheless, as a  rst attempt at source  coreference resolution, we employ a state of theart machine learning approach to np coreference resolution developed by ng and cardie (####). using a corpus of manually annotated opinions, we perform an extensive evaluation and obtain  strong initial results for the task of source coreference resolution. as a  rst step toward opinion summarization we  targeted the problem of source coreference resolution. we showed that the problem can be tackled effectively as noun coreference resolution. one aspect of source coreference resolution that  we do not address is the use of unsupervised information. as a follow up to the work described in this paper we developed a method that utilizes the unlabeled nps in the corpus using a structured rule learner (stoyanov and cardie, ####).
mining and summarizing customer reviews. in this  research, we aim to mine and to summarize all the customer  reviews of a product. this summarization task is different from  traditional text summarization because we only mine the features  of the product on which the customers have expressed their  opinions and whether the opinions are positive or negative. we do  not summarize the reviews by selecting a subset or rewrite some  of the original sentences from the reviews to capture the main  points as in the classic text summarization. our task is performed  in three steps  (#) mining product features that have been  commented on by customers  (#) identifying opinion sentences in  each review and deciding whether each opinion sentence is  positive or negative  (#) summarizing the results. this paper  proposes several novel techniques to perform these tasks. our  experimental results using reviews of a number of products sold  online demonstrate the effectiveness of the techniques. in this research, we study the problem of generating feature based summaries of customer reviews of products sold online. let us use an example to illustrate a feature based summary. assume that we summarize the reviews of a particular digital camera, digital camera #. our task is different from traditional text summarization [##, ##, ##] in a number of ways. first of all, a summary in our case is structured rather than another (but shorter) free text document as produced by most text summarization systems. second, we are only interested in features of the product that customers have opinions on and also whether the opinions are positive or negative. we do not summarize the reviews by selecting or rewriting a subset of the original sentences from the reviews to capture their main points as in traditional text summarization. as indicated above, our task is performed in three main steps  (#) mining product features that have been commented on by customers. we make use of both data mining and natural language processing techniques to perform this task. however, for completeness, we will summarize its techniques in this paper and also present a comparative evaluation. to decide the opinion orientation of each sentence (whether the opinion expressed in the sentence is positive or negative), we perform three subtasks. these words are also called opinion words in this paper. second, for each opinion word, we determine its semantic orientation, e.g., positive or negative. finally, we decide the opinion orientation of each sentence. our experimental results with a large number of customer reviews of # products sold online show that fbs and its techniques are highly effectiveness. in this paper, we proposed a set of techniques for mining and summarizing product reviews based on data mining and natural language processing methods. our experimental results indicate that the proposed techniques are very promising in performing their tasks. we believe that this problem will become increasingly important as more people are buying and expressing their opinions on the web. in our future work, we plan to further improve and refine our techniques, and to deal with the outstanding problems identified above, i.e., pronoun resolution, determining the strength of opinions, and investigating opinions expressed with adverbs, verbs and nouns. finally, we will also look into monitoring of customer reviews. we believe that monitoring will be particularly useful to product manufacturers because they want to know any new positive or negative comments on their products whenever they are available.
opinion observer  analyzing and comparing opinions on the web. this paper focuses on online customer reviews of  products. this paper focuses on online customer reviews of products. in this paper, we propose an analysis system with a visual component to compare consumer opinions of different products. we use figure # to illustrate the idea. our system comes to help naturally in this case. for format (#), we need to identify both product features and opinion orientations. in [##], we proposed several techniques to perform these tasks for format (#), which are also useful for format (#). in this paper, we propose a new technique to identify product features from pros and cons in format (#). we show that the techniques in [##] are not suitable for format (#) because of short phrases or incomplete sentences (we call them sentence segments) in pros and cons rather than full sentences. we do not analyze detailed reviews of format (#) as they are elaborations of pros and cons. note that our visualization system is applicable to all three formats. our work is related but quite different from sentiment classification [e.g., #, #, ##, ##, ##, ##, ##, ##]. we will discuss this and other related work in section #. this paper makes the following contributions  #. to the best of our knowledge, opinion observer is the first system that allows comparison of consumer opinions of multiple (competing) products (it can be one). our experimental results show that the proposed technique is highly effective. in this paper, we focused on one type of opinion sources, customer reviews of products. we proposed a novel visual analysis system to compare consumer opinions of multiple products. to support visual analysis, we designed a supervised pattern discovery method to automatically identify product features from pros and cons in reviews of format (#). in our future work, we will improve the automatic techniques, study the strength of opinions, and investigate how to extract useful information from other types of opinion sources.
extracting product features and opinions from reviews. this paper introduces   opine, an unsupervised informationextraction system which mines reviewsin order to build a model of important product features, their evaluation by reviewers, and their relative  quality across products. this paper focuses on product reviews, though our methods apply to a broader range of opinions. we decompose the problem of review mining into the following main subtasks  i. this paper introduces opine, an unsupervised information extraction system that embodies a solution to each  of the above subtasks. this paper focuses on the  rst # review mining subtasks and our contributions are as follows  #. we compare opine with the most relevant previous review mining system (hu and liu, ####) and  nd that opine s precision on the feature extraction task is ##  better though its recall is #  lower on hu s data sets. we show that # # of this increase in precision comes from using opine s feature assessment mechanism on review data while the rest is due to web pmi statistics. input  product class c, reviews r. output  set of [feature, ranked opinion list] tuples r    parsereviews(r)  e    ndexplicitfeatures(r , c)  o    ndopinions(r , e)  co   clusteropinions(o)  i    ndimplicitfeatures(co, e)  ro   rankopinions(co)   (f, o i , ... o j )...   outputtuples(ro, i   e)  the remainder of this paper is organized as follows  section # introduces the basic terminology, section # gives an overview of opine, describes and evaluates its main components, section # describes related work and section # presents our conclusion.
topic sentiment mixture  modeling facets and opinions in weblogs. in this paper, we de ne the problem of topic sentiment analysis on weblogs and propose a novel probabilistic model to  capture the mixture of topics and sentiments simultaneously. to improve the accuracy and utility of opinion mining from blog data, we prop ose to conduct an in depth analysis  of blog articles to reveal the major topics in an article, associate each topic with sentiment polarities, and model the dynamics of each topic and its corresponding sentiments. free battery from dell..           site and they show dell coupon code as early as possible dell  apos s price is cheaper, we still don  apos t want it. to the best of our knowledge, no existing work could simultaneously extract multiple topics and different sentiments from weblog articles. in this paper, we study the novel problem of modeling subtopics and sentiments simultaneously in weblogs. we  formally de ne the topic sentiment analysis (tsa) p roblem and propose a probabilistic mixture model called topicsentiment mixture (tsm) to model and extract the multiple subtopics and sentiments in a collection of blog articles. with this model, we can extract the topic subtopics from  blog articles, reveal the correlation of these topics and different sentiments, and further model the dynamics of each topic and its associated sentiments. we evaluate our approach on different weblog data sets. the results show that our method is effective for all the tasks of the topic sentiment analysis. our method req uires no prior k nowledge about a domain, and can extract general sentiment mo dels app licable to any ad hoc queries. although we only tested the tsm on weblog articles, it is applicable to any text data with mixed topics and sentiments, such as customer reviews and emails. in section #, we formally de ne the problem of topic sentiment analysis. in section #, we present the topic sentiment mixture model and discuss the estimation of its parameters. we show how  to extract the dynamics of topics and sentiments in section #, and present our experiment results in section #. in sections # and #, we discuss th e related work and conclude. in this paper, we formally de ne the problem of topicsentiment analysis and propose a new probabilistic topicsentiment mixture model (tsm) to solve this problem. with this model, we could effectively (#) learn general sentiment models  (#) extract topic models orthogonal to sentiments, which can represent the neutral content of a subtopic  and (#) extract topic life cycles and the associated sentiment  dynamics. we evaluate our model on different weblog collections  the results show that the tsm model is effective  for topic sentiment analysis, generating more useful topicsentiment result summaries for blog search than a state ofthe art blog opinion search engine (opinmind). there are several interesting extensions to our work. in this work, we assume that the content of sentiment models is the same for all topics in a collection.
determining the sentiment of opinions. this paper studies a text mining problem, comparative sentence mining. in this paper, we propose two novel techniques based on two new types of sequential rules to perform the tasks. results show that our techniques are very promising. we can see that comparative sentences use different language constructs from typical opinion sentences (although the first comparative sentence above is also an opinion). in this paper, we study the problem of comparative sentence mining.
an empirical approach to the interpretation of superlatives. in this paper we introduce an empirical  approach to the semantic interpretation of   superlative adjectives. we present a corpus annotated for superlatives and propose an interpretation algorithm that uses   a wide coverage parser and produces semantic representations. we achieve fscores between #.## and #.## for detecting  attributive superlatives and an accuracy in  the range of #.## #.## for determining the  correct comparison set. as far as we are  aware, this is the  rst automated approach  to superlatives for open domain texts and  questions. in this paper we describe our work on the analysis of superlative adjectives, which is empirically grounded and is implemented into an existing wide coverage text understanding system. to get an overview of the behaviour of superlatives in text, we annotated newswire data, as well as queries obtained from search engines logs. on  the basis of this corpus study, we propose, implement and evaluate a syntactic and semantic analysis for superlatives. to the best of our knowledge,  this is the  rst automated approach to the interpretation of superlatives for open domain texts that  is grounded on actual corpus evidence and thoroughly evaluated. we have presented the  rst empirically grounded study of superlatives, and shown the feasibility of  their semantic interpretation in an automatic fashion. using combinatory categorial grammar and  discourse representation theory we have implemented a system that is able to recognise a superlative expression and its comparison set with high accuracy. for developing and testing our system, we have  created a collection of over #,### instances of superlatives, both in newswire text and in natural  language questions. this very  rst corpus of superlatives allows us to get a comprehensive picture of the behaviour and distribution of superlatives in real occurring data. thanks to such broad view of the phenomenon, we were able discover issues  previously unnoted in the formal semantics literature, such as the interaction of prenominal possessives and superlatives, which cause problems  at the syntax semantics interface in the determination of the comparison set. with respect to  superlatives, our experiments show that the quality of the raw output is not entirely satisfactory. however, we have also shown that some simple post processing rules can increase the performance considerably.
a s t o c hastic parts program and noun phrase parser for unrestricted text. we identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. # if we k n o w t h a t t wo w ords relate to the same property (for example, members of the same scalar group such a s hot and cold) b u t have diierent orientations, we can usually infer that they are antonyms. in this paper, we present and evaluate a method that automatically retrieves semantic orientation information using indirect information collected from a large corpus. our method achieves high precision (more than ## ), and, while our focus to date has been on adjectives, it can be directly applied to other word classes. ultimately, our goal is to use this method in a larger system to automatically identify antonyms and distinguish near synonyms. overview of our approach our approach relies on an analysis of textual corpora that correlates linguistic features, or indicators, with # exceptions include a small number of terms that are both negative from a pragmatic viewpoint and yet stand in an antonymic relationshipp such terms frequently lexicalize two u n wanted extremes, e.g., verbose  terse.
fully automatic lexicon expansion for domain oriented sentiment analysis. this paper proposes an unsupervised   lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a speci c   domain. as a clue to obtain candidate  polar atoms, we use context coherency,   the tendency for same polarities to appear successively in contexts. the experimental results show that the  precision of polarity assignment with  the automatically acquired lexicon was   ##  on average, and our method is robust for corpora in diverse domains and  for the size of the initial lexicon. extensive syntactic patterns enable us to detect sentiment  expressions and to convert them into semantic structures with high precision, as reported  by kanayama et al. this paper addresses the japanese version of domain oriented sentiment analysis,  which identi es polar clauses conveying goodness and badness in a speci c domain, including rather objective expressions. to solve this problem, we have devised  an unsupervised method to acquire domaindependent lexical knowledge where a user has only to collect unannotated domain corpora. for lexical learning from unannotated corpora, our method uses context coherency in terms of polarity, an assumption that polar  clauses with the same polarity appear successively unless the context is changed with  adversative expressions. exploiting this tendency, we can collect candidate polar atoms  with their tentative polarities as those adjacent to the polar clauses which have been identi ed by their domain independent polar  atoms in the initial lexicon. we use both intrasentential and inter sentential contexts to obtain more candidate polar atoms. our assumption is intuitively reasonable, but there are many non polar (neutral) clauses adjacent to polar clauses. our algorithm is fully automatic in the  sense that the criteria for the adoption of polar atoms are set automatically by statistical  estimation based on the distributions of coherency  coherent precision and coherent density. thus our learning method can be used not only by  the developers of the system, but also by endusers. in the next section, we review related work,  and section # describes our runtime sa system. in section #, our assumption for unsupervised learning, context coherency and its key metrics, coherent precision and coherent density are discussed. section # describes our unsupervised learning method. experimental results are shown in section #, and we conclude in section #. we proposed an unsupervised method to acquire polar atoms for domain oriented sa, and  demonstrated its high performance.
interpreting comparative constructions in biomedical text. we propose a methodology using  underspecified semantic interpretation to  process comparative constructions in  medline citations, concentrating on two  structures that are prevalent in the research  literature reporting on clinical trials for  drug therapies. we discuss the generalization of the  methodology to other entities such as  therapeutic and diagnostic procedures. in this paper, we discuss the extension of an existing semantic interpretation system to address comparative structures. we expanded a symbolic semantic interpreter to identify comparative constructions in biomedical text. we identify two compared terms and scalar comparative structures in medline citations. although we restricted the method to comparisons of drug therapies, the method can be easily generalized to other entities such as diagnostic and therapeutic procedures. acknowledgement this study was supported in part by the intramural research programs of the national institutes of health, national library of medicine.
ef cient phrase table representation for machine translation with applications to online mt and speech translation. we will present an ef cient representation with two key properties  on demand  loading and a pre x tree structure for the  source phrases. we will show that this representation scales  well to large data tasks and that we are able  to store hundreds of millions of phrase pairs  in the phrase table. this  ltering is a time consuming task, as we have to go over the whole phrase table. furthermore, we have to repeat this  ltering step whenever we want to translate a new source text. to address these problems, we will use an ef cient representation of the phrase table with two key properties  on demand loading and a pre x tree structure for the source phrases. using on demand loading, we will load only a small fraction of the overall phrase table into memory. we load only the phrase pairs that  are required for one sentence into memory. another advantage of the on demand loading is that we are able to translate new source sentences without  ltering. to overcome this, we use a binary format which is a memory map of the internal representation used during decoding. additionally, we load coherent chunks of the tree structure instead  of individual phrases, i.e. we have only few disk access operations. in our experiments, the on demand loading is not slower than the traditional approach. as pointed out in (mathias and byrne, ####), one problem in speech translation is that we have to match the phrases of our phrase table against a  word graph representing the alternative asr transcriptions. we will present a phrase matching algorithm that effectively solves this combinatorial problem exploiting the pre x tree data structure of the  phrase table. the remaining part is structured as follows  we will  rst discuss related work in sec. #, we will describe the phrase table representation. afterwards, we will present applications in speech translation and online mt in sec. we proposed an ef cient phrase table data structure which has two key properties  #. we are able to store hundreds of millions of phrase pairs and require only a very small amount of memory during decoding, e.g. this enables us to run the mt system on devices with limited hardware resources or alternatively to utilize the freed memory for other models. utilizing the pre x tree structure enables us to ef ciently match source phrases against the phrasetable. using the novel algorithm, we are able to handle large cns, which was prohibitively expensive beforehand. we have shown that this data structure scales very well to large data tasks like the chinese english nist task.
scaling phrase based statistical machine translation to larger corpora and longer phrases. in this paper we describe a novel data   structure for phrase based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is   required by current decoder implementations. we detail the computational complexity and average retrieval times for   looking up phrase translations in our suf x array based data structure. we show  how sampling can be used to reduce the  retrieval time by orders of magnitude with  no loss in translation quality. we present a data structure that is easily capable of handling the largest data sets currently available, and show that it can be scaled to much larger data sets. in this paper we    motivate the problem with storing enumerated phrases in a table by examining the memory requirements of the method for the nist data set   detail the advantages of using long phrases in smt, and examine their potential coverage   describe a suf x array based data structure which allows for the retrieval of translations of arbitrarily long phrases, and show that it requires far less memory than a table   calculate the computational complexity and average time for retrieving phrases and show how this can be sped up by orders of magnitude with no loss in translation accuracy
fast, easy, and cheap  construction of statistical machine translation models with mapreduce. we describe mapreduce implementations of   two algorithms used to estimate the parameters for two word alignment models and one  phrase based translation model, all of which   rely on maximum likelihood probability estimates. on a ## machine cluster, experimental  results show that our solutions exhibit good   scaling characteristics compared to a hypothetical, optimally parallelized version of current state of the art single core tools. in this paper we present mapreduce implementations of training algorithms for two kinds of models  commonly used in statistical mt today  a phrasebased translation model (koehn et al., ####) and  word alignment models based on pairwise lexical translation trained using expectation maximization (dempster et al., ####). currently, such models  take days to construct using standard tools with publicly available training corpora  our mapreduce implementation cuts this time to hours. as an bene t to the community, it is our intention to release this code under an open source license. it is worthwhile to emphasize that we present these results as a  sweet spot  in the complex design space of engineering decisions. in light of possible  tradeoffs, we argue that our solution can be considered fast (in terms of running time), easy (in terms  of implementation), and cheap (in terms of hardware costs). in our opinion, these are not worthwhile tradeoffs. in contrast, our algorithms were developed within a matter of weeks, as part of a  cloud computing  course project (lin, ####). in the next section we provide an overview of  mapreduce. in section # we describe several general solutions to computing maximum likelihood estimates for  nite, discrete probability distributions. we have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the mapreduce framework, yielding a distributed solution that is cost effective, scalable, robust, and exact  (i.e., doesn t resort to approximations). we have further shown that on a ## machine cluster of commodity  hardware, the mapreduce implementations have excellent performance and scaling characteristics. it is our hope that by reducing the cost of this these pieces of the translation pipeline, we will see a  greater diversity of experimental manipulations. towards that end, we intend to release this code under an open source license. for our part, we plan to continue pushing the limits of current word alignment models by moving towards a distributed representation of the model parameters used in the expectation step of em and  abandoning the compiled model representation. this will enable us to scale to larger corpora as well as to explore different uses of translation models, such as techniques for processing comparable corpora,  where a strict sentence alignment is not possible under the limitations of current tools. finally, we note that the algorithms and techniques we have described here can be readily extended to problems in other areas of nlp and beyond. in these areas, model estimation can be a costly process, and therefore we believe this work will be of interest for these applications as well. it is our expectation that mapreduce will also provide solutions that are fast, easy, and cheap.
improving translation quality by discarding most of the phrasetable. in this paper, we show that it is possible to prune phrasetables using a straightforward approach based on signi cance testing, that this approach does not  adversely affect the quality of translation as measured by bleu score, and that savings in terms of  number of discarded phrase pairs can be quite substantial. we will now return to the interaction of the selection in our beam search of the top ## candidates based on forward conditional probabilities. this will affect our results but most likely in the following manner  #. nonetheless, this is a subject for further study, especially as we consider alternatives to our   lter ##  approach for managing beam width. the negative log p value promises to be a useful feature and we are currently evaluating its merits.
an efficient phrase to phrase alignment model for arbitrarily long phrase and large corpora. in this paper, we describe a novel phraseto phrase alignment model which allows for arbitrarily long phrases and works for very  large bilingual corpora. we believe that longer phrases encapsulate more contexts of the words and the translation qualities are expected to be higher than that  of short phrases. in this paper, we introduce a new strategy to cope with this problem. instead of aligning the phrases offline, we extract the phrase translations on the fly for  each testing sentences. we use suffix array (manber ####) to index the training corpus and a novel fast algorithm to search all the substrings (phrases) of the testing sentences in the training data. thus, we do not need to store any phrase translations and we can use arbitrarily long phrases. in the following sections, we first show the empirical evidence that long phrases do improve the translation qualities. then we will introduce our phrase alignment model asp which finds the alignment for a source phrase of any length. in the end, we will introduce a mixture online offline alignment strategy which allows for arbitrarily long phrases and works with arbitrarily large bilingual corpora efficiently. we presented a successful statistical machine translation system using the mixture online offline alignment model. we will experiment with other word co occurrence statistics, such as the  mutual information, chi square, or dice coefficient.
word sense disambiguation improves statistical machine translation. in this paper, we successfully integrate a state of the art wsd  system into a state of the art hierarchical  phrase based mt system, hiero. we show   for the  rst time that integrating a wsd system improves the performance of a state ofthe art statistical mt system on an actual   translation task. in this paper, we successfully integrate a stateof the art wsd system into the state of the art hierarchical phrase based mt system, hiero (chiang,  ####). the contribution of our work lies in showing for the  rst  time that integrating a wsd system signi cantly improves the performance of a state of the art statistical mt system on an actual translation task. in the next section, we describe our wsd system. then, in section #, we describe the hiero mt system and introduce the two new features used to integrate the wsd system into hiero. in section #, we describe the training data used by the wsd system. in section #, we describe how the wsd translations provided are used by the decoder of the mt system. in section # and #, we present and analyze our experimental results, before concluding in section #. we have shown that wsd improves the translation performance of a state of the art hierarchical  phrase based statistical mt system and this improvement is statistically signi cant. we have also demonstrated one way to integrate a wsd system into an mt system without introducing any rules that compete against existing rules, and where the feature weight tuning and decoding place the wsd system on an equal footing with the other model components. finally, besides our proposed approach of  integrating wsd into statistical mt via the introduction of two new features, we could explore other alternative ways of integration.
improving statistical machine translation using word sense disambiguation. we show for the  rst time that incorporating   the predictions of a word sense disambiguation system within a typical phrase based  statistical machine translation (smt) model  consistently improves translation quality   across all three different iwslt chineseenglish test sets, as well as producing statistically signi cant improvements on the  larger nist chinese english mt task   and moreover never hurts performance on  any test set, according not only to bleu   but to all eight most commonly used automatic evaluation metrics. in this paper, we address   this problem by investigating a new strategy for integrating wsd into an smt system, that performs fully phrasal multi word   disambiguation. instead of directly incorporating a senseval style wsd system, we   rede ne the wsd task to match the exact same phrasal translation disambiguation  task faced by phrase based smt systems. our results provide the  rst known empirical evidence that lexical semantics are indeed useful for smt, despite claims to the  contrary. hr#### ## c ####, and by the  hong kong research grants council (rgc) research grants we have shown for the  rst time that integrating multi word phrasal wsd models into phrase based smt consistently helps on all commonly available automated translation quality evaluation metrics on all three different test sets from the chinese english  iwslt## text translation task, and yields statistically signi cant gains on the larger nist chineseenglish task. we have proposed to consistently integrate wsd models both during training, where sense de nitions and sense annotated data are automatically extracted from the word aligned parallel corpora from smt training, and during testing, where the phrasal wsd probabilities are used by the smt system just like all the other lexical choice features. our model makes use of typical wsd features that are almost never used in smt systems, and requires a dynamically created translation lexicon on a per sentence basis. to our knowledge this constitues the  rst attempt  at fully integrating state of the art wsd with conventional phrase based smt. the single word approach, as we reported in carpuat et al. (####), improved bleu and nist scores  for phrase based smt, but subsequent detailed empirical studies we have performed since then suggest that single word wsd approaches are less successful when evaluated under all other mt metrics (carpuat and wu, ####). thus, fully phrasal wsd  predictions for longer phrases, as reported in this paper, are particularly important to improve translation quality. the results reported in this paper cast new light on the wsd vs. smt debate, suggesting that a close integration of wsd and smt decisions should be incorporated in a smt model that successfully uses  wsd predictions. our objective here is to demonstrate that this technique works for the widest possible class of models, so we have chosen as the baseline the most widely used phrase based smt  model. our positive results suggest that our experiments could be tried on other current statistical  mt models, especially the growing family of treestructured smt models employing stochastic transduction grammars of various sorts (wu and chiang, ####).
compressed full text indexes. in this paper we present the main concepts underlying (compressed) self indexes. we explain the relationship between text entropy regularities that show up in index structures permit compressing them. then we cover the most relevant self indexes focusing on how they exploit text compressibility to achieve compact structures that can e ciently solve various search problems. our aim is to give the background to understand follow the developments in this area. for space  complexities, we present only the main term related to entropies and seek to m inimize space.
using scenario knowledge in automatic question answering. this paper describes a novel framework   for using scenario knowledge in opendomain question answering (q a) applications that uses a state of the art textual  entailment system (hickl et al., ####b) in   order to discover textual information relevant to the set of topics associated with a  scenario description. in this paper, we propose that scenario relevant passages in natural language texts can be identi ed  by recognizing a semantic relation, known as contextual entailment (ce), that exists between a text passage and one of a set of subquestions that are conventionally implied by a scenario. under this  model, we expect that a scenario s can be considered to contextually entail a passage t, when there exists at least one subquestion q derived from s that textually entails the passage t. we show that  by using a state of the art textual entailment system (hickl et al., ####b), we can provide q a systems with another mechanism for approximating  the inference between questions and relevant answers. we show how each of these cases of contextual entailment can be computed and how it can be used in the intrinsic and extrinsic evaluation of a q a system. section # introduces our notion of contextual entailment and provides a framework  for recognizing instances of ce between scenarios and both questions and answers. section # describes the textual entailment system used at the core of our ce system. section # presents results from our evaluations, and section # summarizes our conclusions this paper introduced a new form of textual entailment, known as contextual entailment, which can be used to recognize scenario relevant information in both the questions users ask and in the answers that automatic q a systems return. in addition  to outlining a framework for recognizing contextual entailment in texts, we showed that contextual entailment information can signi cantly enhance the quality of answers returned by a q a system in response to users  questions about a particular scenario. in our evaluations, we found that using  contextual entailment allowed q a systems to improve their accuracy by more than ##  overall.
recognizing textual entailment using lexical similarity. we describe our participation in the   pascal #### recognizing textual entailment challenge. our method is based   on calculating  directed  sentence similarity  checking the directed  semantic    word overlap between the text and the hypothesis. we use frequency based term   weighting in combination with two different lexical similarity measures. our best  run shows #.## accuracy on the test data,  although the difference between our two   runs is not signi cant. we found remarkably different optimal threshold values for  the development and test data. we argue  that, in addition to accuracy, precision and  recall are valuable measures to consider  for textual entailment. in this paper we describe a simple system based  on lexical similarity, with two different word similarity measures. we also present our of cial results and a deeper analysis of the system s performance. we described our participation in the pascal #### recognizing textual entailment challenge, with a simple sentence similarity based system that uses two different word similarity measures. although both our runs show signi cant improvement over random guessing, the improvement is based only on one subtask (cd). we found that the system cannot be further tuned without over tting, which suggests that other, deeper text features need to be explored.
the pascal recognising textual entailment challenge. this paper describes the pascal network of excellence recognising textual  entailment (rte) challenge benchmark  # . we say that t entails h if the meaning of h can be inferred  from the meaning of t, as would typically be interpreted by people. as in other evaluation tasks our definition of textual entailment is operational, and corresponds to the judgment criteria given to the annotators  who decide whether this relationship holds between a given pair of texts or not. yet, any attempt to make significant reference to this rich body of literature, and to deeply understand the  relationship between the operational textual entailment definition and relevant formal notions, would be beyond the scope of the current challenge  and this paper. consequently, we hypothesize that textual entailment recognition is a suitable generic task for evaluating and comparing applied semantic inference models. #.# the challenge scope as a first step towards the above goal we created a dataset of text hypothesis (t h) pairs of small text snippets, corresponding to the general news domain (see table #). overall, we were aiming at an explorative rather than a competitive setting, hoping that  meaningful baselines and analyses for the capabilities of current systems will be obtained. we expect them to change over time and hope that participants  apos  contributions, observations and comments will help shaping this evolving research direction. overall, we hope that future similar benchmarks will be carried out and will  help shaping clearer frameworks, and corresponding research communities, for applied research on semantic inference.
recognizing textual entailment with lcc s groundhog system. we introduce a new system for recognizing textual entailment (known  as groundhog) which utilizes a  classi cation based approach to combine  lexico semantic information derived  from text processing applications with a  large collection of paraphrases acquired  automatically from the www. trained on  ###,### examples of textual entailment  extracted from newswire corpora, our  system managed to classify more than  ##  of the pairs in the #### pascal  rte test set correctly. we can try to (#) derive linguistic information from the hypothesis text pair, and cast the inference recognition as a classi cation problem  or (#) evaluate the probability that an entailment can exist between the hypothesis text pair  (#) represent the knowledge from the hypothesis text pair in some representation language that can be associated with an inferential mechanism  or (#) use the classical ai de nition of entailment and build models of the  world in which the hypothesis and the text are respectively true, and then check whether the models associated with the hypothesis are included in the models associated with the text. it is not clear which methodology is superior, but we argue that  the  rst two methods rely more heavily on the accuracy and robustness of processing information from  text, whereas the other two methods make use of reasoning technologies or model checking methods that  apply to any kind of knowledge, not only to linguistic knowledge derived from text. although we believe that each of these methods should be investigated fully, we decided to focus only on the  rst method, which allows us to make use of some of the natural language processing tools  developed at lcc. for this purpose, we have developed a system called groundhog, which relies on our ability to derive a variety of lexico semantic information from text, including information about  named entities, coreference, and syntactic and semantic dependencies. in addition, since textual paraphrases are a special case of entailment, we expect  techniques used successfully in paraphrase recognition should also be useful for textual entailment. in our system, textual alignment is used to capture the candidate portions from the text and the hypothesis that could be paraphrases. we claim that the quality of the classi er that we have trained is also due to the new sources of data that we have exploited. section # describes the architecture of groundhog, while section # details the linguistic  processing that we have applied to each hypothesistext pair. we have described our methodology for recognizing  textual entailment that was utilized in the #### pascal rte challenge. we are satis ed with our results from this evaluation, which we feel illustrates the potential of one of the four textual entailment frameworks that we considered in section #.
investigating a generic paraphrase based approach for relation extraction. we   propose a generic paraphrase based approach for relation extraction (re), aiming at a dual goal  obtaining an applicative   evaluation scheme for paraphrase acquisition and obtaining a generic and largely   unsupervised con guration for re. we analyze the potential of our approach and  evaluate an implemented prototype of it  using an re dataset. our  ndings reveal a  high potential for unsupervised paraphrase  acquisition. we also identify the need for   novel robust models for matching paraphrases in texts, which should address syntactic complexity and variability. this paper investigates the applicability of a generic  paraphrase based  approach to  the relation extraction (re) task, using an available re dataset of protein interactions. our con guration assumes a set of entailing templates (non symmetric  paraphrases ) for the  target relation. for example, for the target relation  x interact with y  we would assume a set of  entailing templates as in tables # and #. in addition, we require a syntactic matching module that identi es template instances in text. first, we manually analyzed the proteininteraction dataset and identi ed all cases in which protein interaction is expressed by an entailing template. next, we implemented a prototype that utilizes a state of the art method for learning entailment relations from the web (szpektor et al., ####), the minipar dependency parser (lin, ####) and a syntactic matching module. the contributions of our investigation follow  the dual goal set above. to the best of our knowledge, this is the  rst comprehensive evaluation  that measures directly the performance of unsupervised paraphrase acquisition relative to a standard application dataset. our  ndings are encouraging for  both goals, particularly relative to their early maturity level, and reveal constructive evidence for the remaining room for improvement. we have presented a paraphrase based approach for relation extraction (re), and an implemented  system, that rely solely on unsupervised paraphrase acquisition and generic syntactic template matching. our approach differs from previous unsupervised ie methods in that we identify instances of a speci c relation while prior methods identi ed template relevance only at the general scenario level. we manually analyzed the potential of our approach on a dataset annotated with protein interactions. additionally, we manually assessed the coverage of the tease acquisition algorithm and found that  ##  of the distinct pairs can be potentially recognized with the learned templates, assuming an  ideal matcher, indicating a signi cant potential recall for completely unsupervised paraphrase acquisition. finally, we evaluated our current system performance and found it weaker than supervised  re methods, being far from ful lling the potential indicated in our manual analyses due to insuf cient syntactic matching. but, even our current performance may be considered useful given the very small amount of domain speci c information used by the system. most importantly, we believe that our analysis and evaluation methodologies for an re dataset provide an excellent benchmark for unsupervised learning of paraphrases and entailment rules. in the long run, we plan to develop and improve our acquisition and matching algorithms, in order to  realize the observed potential of the paraphrasebased approach. notably, our  ndings point to the need to learn generic morphological and syntactic variations in template matching, an area which has rarely been addressed till now.
textual entailment resolution via atomic propositions. this paper presents an approach to solving  the problem of textual entailment  recognition and describes the computer  application built to demonstrate the  performance of the proposed approach.
applying cogex to recognize textual entailment. the pascal rte challenge has helped   lcc to explore the applicability of enhancements that have been made to our  logic form representation and wordnet  lexical chains generator. our system  transforms each t h pair into logic form  representation with semantic relations. our   cogex logic prover is then used to attempt to prove entailment. our approach attempts to recognize textual entailment by determining if the hypothesis sentence can be logically derived from the text passage using a logic prover. our logic prover operates by  reductio ad absurdum  or  proof by contradiction  (wos, ####). a description of our system s implementation is  provided in section #, our results and some performance analysis are included in section #, and some  nal concluding remarks are made in section #. we participated in the rte challenge mainly as a learning experience and a test of our existing logic prover system implemented in a new way.
when logical inference helps determining textual entailment (and when it doesn t). we compare and combine two methods  to approach the second textual entailment  challenge (rte #)  a shallow method  based mainly on word overlap and a   method based on logical inference, using  rst order theorem proving and model  building techniques. we use a machine  learning technique to combine features of  both methods. we submitted two runs,   one using only the shallow features, yielding an accuracy of ##.# , and one using  features of both methods, performing with  an accuracy score of ##.# . we try to explain the reason for these results. in this paper we summarise the results of our efforts in the #### #### recognising textual entailment (rte #) challenge (the task of deciding, given two text fragments, whether the meaning of one text  is entailed inferred from another text). we experimented with shallow and deep semantic analysis  methods. we will introduce the shallow semantic analysis (section #) and the deep semantic analysis  (section #), present the results of our two runs (section #), and discuss them (section #). our work for rte # is essentially a further development of our approach to rte # (bos and markert, ####). in addition, we also re ned the method for ranking the output, which is used for calculating average precision.
proceedings of the ##th annual meeting of the association for computational linguistics. in this paper we compare pronoun resolution algorithms and introduce a centering algorithm (left right centering) that adheres to the constraints and rules of centering theory and is an alternative to brennan, friedman, and pollard s (####) algorithm. we then use the left right centering algorithm to see if two psycholinguistic claims on cf list ranking will actually improve pronoun resolution accuracy. our results from this investigation lead to the development of a new syntax based ranking of the cf list and corpus based evidence that contradicts the psycholinguistic claims. introduction the aims of this paper are to compare implementations of pronoun resolution algorithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution. in this study, this ability to alter an algorithm slightly and test its performance is central. next we use the conclusions from two psycholinguistic experiments on ranking the cf list, the salience of discourse entities in prepended phrases ( gordon, grosz, and gilliom #### ) and the ordering of possessor and possessed in complex nps ( gordon et al. finally, we show that the results from two psycholinguistic experiments, thought to provide a better ordering of the cf list, do not improve lrc s performance when they are incorporated (section #). unlike the other three algorithms analyzed in this project, the hobbs algorithm does not appeal to any discourse models for resolution  rather, the parse tree and grammatical rules are the only information used in pronoun resolution.
predicting the semantic orientation of adjectives. this paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm. we incorporate multiple anaphora resolution factors into a statistical framework specifically the distance between the pronoun and the proposed antecedent, gender number animaticity of the proposed antecedent, governing head information and noun phrase repetition. we combine them into a single probability that enables us to identify the referent. our first experiment shows the relative contribution of each source of information and demonstrates a success rate of ##.#  for all sources combined. we present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves ##.#  accuracy. introduction we present a statistical method for determining pronoun anaphora. we present some typical results as well as the more rigorous results of a blind evaluation of its output. ) we first discuss the training features we use and then derive the probability equations from them. the first piece of useful information we consider is the distance between the pronoun and the candidate antecedent. secondly, we look at the syntactic situation in which the pronoun finds itself.
proceedings of the ##rd annual meeting of the acl. this paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. this paper presents a new corpus based approach to noun phrase coreference. we believe that it is the first such unsupervised technique developed for the general noun phrase coreference task. in short, we view the task of noun phrase coreference resolution as a clustering task. in an evaluation on the muc # coreference resolution corpus, our clustering approach achieves an f measure of ##.# , placing it firmly between the worst (## ) and best (## ) systems in the muc! as a result, we believe that viewing noun phrase coreference as clustering provides a promising framework for corpus based coreference resolution.
an expectation maximization approach to pronoun resolution. we propose an unsupervised expectation   maximization approach to pronoun resolution. we show that unsupervised learning is possible in this context, as the performance of our system is comparable to  supervised methods. our results indicate  that a probabilistic gender number model,  determined automatically from unlabeled  text, is a powerful feature for this task. our approach is a synthesis of linguistic and statistical methods. this allows us to train on and resolve all third person pronouns in a large question answering corpus. we learn lexicalized  gender number, language, and antecedent probability models. we gain further performance improvement by initializing em with a gender number model derived from special cases in the training data. we also  demonstrate how the models learned through our unsupervised method can be used as features in a supervised pronoun resolution system. we have demonstrated that unsupervised learning is  possible for pronoun resolution. we achieve accuracy of ##  on an all pronoun task, or ##  when a true antecedent is available to em. furthermore, the lexicalized models learned in our system,  especially the pronoun model, are potentially powerful features for any supervised pronoun resolution system.
a corpus based investigation of de nite description use. we present the results of a study of de nite descriptions use in written   texts aimed at asse ssing the feasibility of annotating corpora with information about de nite description interpretation. we ran two experiments,  in which subjects were asked to classify the uses of de nite descriptions  in a corpus of ## ne wspaper a rticles, c ontaining a total of #### de nite  descriptions. we measured the agreement among annotators a bout the  classes assigned to de nite descriptions, as well as the agreement about  the antecedent as signed to those de nites that the annotators classi ed as  being related to an antecedent in the text. the most in teresting result of this  study from a corpus a nnotation perspective was the rather low agreement   (k #.##) that we obtained using versions of hawkins  and prin ce s classi  cation schemes  better results (k #.##) were obtained using the simpli ed  scheme proposed by fraurud that includes only two classes,  rst mention  and subsequen t mention. from a linguistic point of view, the   most interesting observations were the great number of discourse new definites in our corpus (in one of our experiments, about ##  of the de nites  in the collection were cl assi ed as discourse  new, ##  as anaphoric, an d  ##  as associative bridging) and the presence of de nites which did n ot  seem to require a complete disambiguation. this paper will appear in computational linguistics. (a) back into the soup  ## the work presented in this paper was inspired by the growing realization in  the  eld of computational linguistics of the need for an experimental evaluation of linguistic theories semantic theories, in o ur case. the evaluation we  are considering typically takes the form of experiments in which humans subjects are asked to annotate texts from a corpus (or recordings of spoken conversations) according to a certain classi cation scheme, and the agreement among their annotations is measured (see, e.g., (passonneau and litman, ####) or the papers in (moore and walker, ####)). our own concern are semant ic judgments concerning the interpretation  of noun phrases with the de nite article the, that we will call de nite descriptions, following (russell, ####). we w ill not be concerned wi th other cases of de nite noun phrases such as pronouns, or possessive descriptions  hence the term de nite description rather t han the more general term de nite np. we c oncentrated on written texts in this study. # ####  prince, ####  fraurud, ## ##)  yet, we are aware of no attempt at verifying whether non linguistically trained subjects are capable o f recognizing the proposed distinctions, which is a precondition for using these schemes for the kind of large scale text annotation exercises which are necessa ry to evaluate a system s performance as done in muc. our intention was to do the same for  de nite descriptions. we ran two experiments to tes t how good are naive subjects at doing the form of linguistic analysis presupposed by current schemes for classifying de nite descriptions. (where by  how go od  here we mean   how much do they agree among themselves , as commonly assumed in work of this kind. ) our s ubjects were asked to classify the de nite descriptions found in a  corpus of natural language texts according to classi cation schemes that we developed starting from the taxonomies proposed by hawkins (####) and prince  (####  ####) , but which t ook into account our intention of letting  naive  speakers perform the classi cation. our experiments we re also designed to assess the feasibility of a system to process de nite descriptions on unrestricted text and to collect data that could be used for this implementation. for both of these reasons, the classi cation schemes that we tried differ in several respects from those adopted in prior corpus based studies such as (prince, ####   fraurud, ####). our study is also differe nt from these previous ones in that measuring the agreement among annotators became an issue (carletta, ####). we used for the experiments a set of randomly selected articles from the wall street journal contained in t he acl dci cd rom, rather than a corpus of transcripts of spoken language corpora such as the hcrc maptask corpus (anderson et al., ####) or the trains corpus (heeman and allen, ####). a  second reason was that we intended to use computer simulations of the classi cation tas k to supplement the results of our experiments, and we needed a parsed corpus for t his purpose  the articles we chose were all part of the penn treebank (marcus et al., ####). we review two existing classi cation schemes in section  #  we then discuss our two classi cation experiments in sections # and #, respectively. # #.# some consequences of this research this study raises the issue of how feasible it is to annotate corpora for anaphoric information. we observed two problems about the task of classifying de nite descriptions   rst, neither of the more complex classi cation schemes we tested resulted in a very good agreement among annotat ors  and second, even the task of identifying the antecedent of  discourse related  de nite descriptions  (i.e., co referential and bridging) is problematic we only obtained an acceptable agreement in the case of co refere ntial de nite descriptions, and it was  dif cult for our annota tors to choose a single antecedent for a de nite description when both bridging and co reference are allowed. on the positive side, we have t wo positive observations  subjects do reasonably well at distinguishing  rst me ntion from subsequent mention antecedents, and at identifying the antecedent of a subsequent mention de nite description. we brie y discuss below our progress in t his direction so far. our study con rms the  ndings of previous work (e.g., (fraurud, ####)) that a  great number of t he de nite descriptions in texts are discourse new  in our s e cond experiment we found an equal number of discourse new and  discourserelated  de nite descriptions, although many of the de nite descriptions classi ed as discourse new could be seen as as sociative in a loose sense. ## of the existing theories of de nite descriptions, t he one that comes closest to accounting for all of t he uses of de nite descriptions that we observed  is l obner s (####). given that  rst mention de nite descriptions are so numerous, and that recognizing them does not depend on commons e nse knowledge alone, we conclude that any general theory o f de nite description interpretation should include methods for recognizing such de nites. the architecture of our own classi er (see below) is also consistent with fraurud s hypothes is that these methods  are not jus t used when no suitable antecedent can be found, but more extensive investigations will be needed before we can conclude that this architecture signi cantly outperforms other ones. a signi cant percentage of the larger situation de nite descriptions e ncountered in our corpus cannot be said to be in the  global focus  in any signi cant sense  as we observed above, in many of these cases the writer seems to rely on the reader s capability to add a new object such as the illinois commerce commission to her his model of the world, rather t han expecting that object to be already present. ## #.# a (semi) automatic classi er as already m e ntioned, we are in the course of implementing a system capable of performing the classi cation task se mi automatically (vieira, ####). our system implements t he  dual processing  strategy discussed above. we  trained  a version of the system on the corpus used for the  rst experiment, and then compared its classi cation of the corpus used for the second experiment with that of our three subjects. we developed two versions of the system  one which only atte mpts to classify subsequent mention and discourse new de nite descriptions (vieira and poesio, ####), and one which also attempts to classify bridging references (poesio et al., ####). the  rst version of the system  nds a classi cation for ### de nite descriptions out of the ### in our test data (the articles used in the second experiment). the agreement between t he system and the three annotators o n the two classes  rst mention and subsequent mention is k #.## overall (k #.## for the three annotators on the converted annotation), if all de nite descriptions to which  the system can t assign a class i cation are treated as  rst mention  the coef cient of agreement is k #.## if we do not count the de nite descriptions that  the system cannot classify (k #.## for the annotators on just those de nite descriptions). the version of the system that also attempts to recognize bridging references has a worse performance, which is not surprising given the problems  our subjects had in classifying bridging descriptions. this version of the system  nds a classi cation for ### descriptions out of ###, and its agreement with the three annotators is k #.## if the cases that the system cannot classify are not counted (k #.## for the three annotators on # categories with just these de nites)  k #.## if we count the cases that the system does not class ify as discourse new (for ### descriptions)  and k #.## again if we count the cases that t he system does not classify as bridging (again, ### descriptions). ## #.# future work we collected plenty of data about de nite descriptions that we are still in the process of analyzing. one issue we are studying at the moment is what to do with bridging references  how to classify them if at all, a nd how to process them. we also intend to study loebner s hypothesis about the role played by the distinction between  sortal  and  relational  head nouns in determining the type of process involved in the resolution of a de nite description, possibly by  nding a way to ask our subje cts to re cognize these distinctions. and we plan to study the issue of generic de nites. an obvious direction in which to extend this study is by looking at other kinds of anaphoric expressions such as pronouns and demonstratives. we are performing preliminary studies in this direction. finally, we would like to em phasize that although this study is the mo st extensive investigation of de nite description use in a corpus that we know of ( we looked at a total of more than #### de nite descriptions in ## te xts, i.e., almost three times as many as in fraurud s study), in practice we still got very little data on many of the use s of de nite descriptions, so some caution is necessary in inte rpreting these results. the problem is that the kind of analysis we performed is extremely time consuming  it will be crucial in t he future to  nd ways of performing this task that will allow us to analyze more data, possibly with the help of computer simulations.
clustering algorithms for noun phrase coreference resolution. in this paper, we present four clustering algorithms for noun phrase coreference resolution. we developed  two novel algorithms for this task, a fuzzy algorithm and its hard variant and evaluated their performance  on two different sets of texts in comparison with an existing fuzzy and a hard clustering algorithm that  are desc ribed in the literature. our algorithms perform slightly better and do not rely on a prede ned  threshold distance value for cluster membership. in addition, our fuzzy clustering algorithm seems to  perform better than a hard clustering on a pronoun resolution task. our coreference resolution focuses on detecting  identity  relationships (i.e. for the  rst subtask we use the same set of features as in [#]. we implemented two novel algorithms for the second step  a progressive fuzzy clustering algorithm and its hard variant. we also implemented the hard clustering algorithm presented in [#] and f function f words ##.# ( of mismatching words) (  of words in longer np) head noun #.# # if the head noun differs  else # pronoun #.# # if np i is pronoun and np j is not  else # j is inde nite and not appositive  else # i subsumes n p j appositive    # if n p j is appositive and np i a fuzzy clustering algorithm as described in [#]. our goal is to test the quality of the coreference resolution that is achieved by these four algorithms. we then describe our experiments and their results. in this paper we compared four clustering methods for coreference resolution  one progressive fuzzy, its hard variant and another two algorithms, taken from the literature. we evaluated them on two kinds of corpora, a standard one used in the coreference resolution task and another one containing more pronominal entities. our algorithms are on top  when all the entities are considered. for the pronoun resolution, our fuzzy algorithm obbl# #.## #.## #.## #.## bl# #.## #.## #.## #.## fc p #.## #.## #.## #.## hc v #.## #.## #.## #.## hc c thrs. these results are obtained despite the fact that our algorithms do not rely on a threshold distance value for cluster membership, w hich makes them corpus independent. in the future we plan to perform more experiments with different types of texts and to enlarge the feature set based on current linguistic theories. we also plan to integrate the noun phrase coreference tool in our text summarization system.
competitive self trained pronoun interpretation. we describe a system for pronoun interpretation that is self trained from raw data,  that is, using no annotated training data. in this short paper we describe a system for (third person) pronoun interpretation that is self trained from raw  data, that is, using no annotated training data whatsoever. we  rst brie y describe the supervised system (described in more detail in kehler et al. (####)) to which we will compare the self trained system. we then describe our hobbsian baseline  algorithm, and present the results of all three systems. as a next step, we will take a closer look at the training data acquired to try to ascertain the underlying reasons for this success. for instance, whereas our algorithm uses the current model s probabilities  in a winner take all strategy for positive example selection, these probabilities could instead be used to dictate the likelihood that examples are assigned a positive outcome, or they could be thresholded in  various ways to create a more discerning positive outcome assignment mechanism. the relative generality of our feature set was appropriate given the size of the data sets used.
unsupervised coreference resolution in a nonparametric bayesian model. we present an unsupervised, nonparametric bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. while most existing coreference work  is driven by pairwise decisions, our model  is fully generative, producing each mention   from a combination of global entity properties and local attentional state. despite being unsupervised, our system achieves a ##.#  muc f  # measure on the muc # test set,   broadly in the range of some recent supervised results. in this paper, we present a nonparametric generative  model of a document corpus which naturally connects these two processes. first, rich features require plentiful labeled data, which we do not have for coreference tasks in most domains and languages. # these models, as ours, are generative ones, since the focus is on cluster discovery and the data is generally unlabeled. in this paper, we present a novel, fully generative, nonparametric bayesian model of mentions in a  document corpus. our model captures both withinand cross document coreference. as a joint model of several kinds of discourse variables, it can be used to make predictions about  either kind of coreference, though we focus experimentally on within document measures. to the best of our ability to compare, our model achieves the best unsupervised coreference performance. we have presented a novel, unsupervised approach to coreference resolution  global entities are shared  across documents, the number of entities is determined by the model, and mentions are generated by  a sequential salience model and a model of pronounentity association. although our system does not perform quite as well as state of the art supervised systems, its performance is in the same general range, despite the system being unsupervised.
communications of the acm. this paper describes a program which attempts to replicate the methodology described by crowley almost in its entirety. for example, if we are comparing the english  four  with the german  vier , we need to know that the  f  corresponds to the  v  and not to any other character. in this paper, a process called dynamic programming is used for character level alignment, but guy initially considers all possible phoneme matches within a word pair, later using a variant of the chi square test to work out which sound matches are significant.
a probabilistic approach to language change. we present a probabilistic approach to language change in which word forms  are represented by phoneme sequences that undergo stochastic edits along the  branches of a phylogenetic tree. we use this framework to explore the consequences of two different schemes for de ning probabilistic models of phonological change, evaluating  these schemes by reconstructing ancient word forms of romance languages. most of what we know about language change comes from the comparative method, in which words from different languages are compared in order to identify their relationships. we take a quantitative approach to diachronic linguistics that alleviates this problem by operating at the phoneme level. our approach combines the advantages of the classical, phoneme based, comparative method with the robustness of corpus based probabilistic models. we focus on the case where the words are etymological cognates across languages, e.g. following [#], we use this information to estimate a contextualized model of phonological change expressed as a probability distribution over rules applied to individual phonemes. for example, we can reconstruct ancestral word forms or inspect the rules learned along each branch of a phylogeny to identify sound laws. alternatively, we can observe a word in one or more modern languages, say french and spanish, and query the corresponding word form in another language, say italian. in this paper, we use this general approach to evaluate the performance of two different schemes for de ning probability distributions over rules. inspired by the prevalence of multi scale rules in diachronic phonology and modern phonological theory, we develop a new  scheme in which rules possess a set of features, and a distribution over rules is de ned using a loglinear model. we evaluate both schemes in reconstructing ancient word forms, showing that the new linguistically motivated change can improve performance signi cantly. in this paper, we presented a novel probabilistic model of phonological change, in which the rules governing changes in the sound of words are parametrized using the features of the phonemes involved. we believe that this probabilistic approach has the potential to support quantitative analysis of the history of languages in a way that can scale to large datasets while remaining sensitive to the concerns that have traditionally motivated diachronic linguistics. acknowledgments we would like to thank bonnie chantarotwong for her help with the ipa converter and our reviewers for their comments.
muc # named entity task definition.
facile  description of the ne system used for muc #. introduction in this paper, we describe the system used by the umist team as members of the facile consortium, to undertake the ne task in muc #. we looked forward to slightly higher scores than we obtained in the formal run, because at the dry run stage, we had obtained almost identical scores to our best results with training data. in the rest of this paper, we first give some background on the context in which the system used in the muc # ne task was developed. we then outline its internal structure, concentrating on the rule notation which is its most salient feature. introduction in this paper, we describe the system used by the umist team as members of the facile consortium, to undertake the ne task in muc #. we looked forward to slightly higher scores than we obtained in the formal run, because at the dry run stage, we had obtained almost identical scores to our best results with training data. in the rest of this paper, we first give some background on the context in which the system used in the muc # ne task was developed. we then outline its internal structure, concentrating on the rule notation which is its most salient feature. we required name finding to be done in all four languages, using a standard interface with a morphological analyser and tagger for those languages.
proceedings of the second conference on empirical methods in natural language processing. we present a m ulti level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. as input, our technique requires only unan notated training texts and a handful of seed words for a category. we use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. to m a k e this approach more robust, we add a second level of bootstrapping (meta bootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. we evaluated this multi level bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. we explore the idea of learning both a dictionary of extraction patterns and a domain speciic semantic lexicon simultaneously. furthermore, our technique requires no special training resources. the input to our algorithm is a set of unannotated training texts and a handful of  seed  words for the semantic category of interest. the heart of our approach is a mutual bootstrapping technique that learns extraction patterns from the seed words and then exploits the learned extraction patterns to identify more words that belong to the semantic category. in this paper, we rst describe the mutual bootstrapping algorithm that generates both a semantic lexicon and extraction patterns simultaneously. in the second section, we describe how the mutual bootstrapping process is itself bootstrapped to produce more accurate dictionaries at each iteration. in the third section, we present the results from experiments with two text collections  a set of corporate web pages, and a corpus of terrorism newswire articles.
introduction to the conll #### shared task  language independent named entity recognition. we describe the conll #### shared task    language indep e ndent named entity recognition. we give background information on  the data sets (english and german) and  the evaluation method, present a general  overview of the systems that have taken   part in the task and discuss their performance. we will conce ntrate on four types of named entities  persons, locations, organizations and names of  miscellaneous entities that do not belong to the previous three groups.
a survey of named entity recognition and classification. we present here a survey of fifteen years of research in the nerc field from #### to ####. we survey these techniques as well as other critical aspects of nerc such as features evaluation methods. to the best of our knowledge nerc features techniques evaluation methods have not been surveyed extensively yet. finally we present our conclusions. from #### (# publication) to #### (we found # publications in english) the publication rate remained relatively low.
unsupervised models for named entity classification. this paper discusses the use of unlabeled examples  for the problem of named entity classification. however, we show that the use of unlabeled  data can reduce the requirements for supervision  to just # simple   quot seed  quot  rules. we present two algorithms. unfortunately, yarowsky  apos s method is not well understood from a theoretical viewpoint   we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective  function. our first algorithm is similar to yarowsky  apos s, but with some important modifications motivated by (blum and mitchell ##). roughly speaking, the new  algorithm presented in this paper performs a similar search, but instead minimizes a bound on the  number of (unlabeled) examples on which two classifiers disagree. the method shares some characteristics of the decision list algorithm presented in this paper. (riloff and jones ##) was brought to our attention as we were preparing the final version of this paper.
acquisition of categorized named entities for web search. we present a lightlysupervised method for acquiring named entities in arbitrary  categories. we illustrate applications of the method in  web search, and describe experiments on ### million web  documents and news articles. as illustrated in figure #, our haystack of documents can be then considered from a different perspective  that of a goldmine that  hides  valuable information nuggets about various names, including nuggets that encode their categories. #.# approach in this paper, we present a lightweight, lightly supervised method for acquiring named entities in arbitrary categories  from web documents. in the trade off between method complexity and output coverage, we opted from the start for low complexity. we focus on textual content rather than structural  clues. after further discussion in section #, we glance at future work in section #. this paper presented a lightly supervised method for accessing, decoding and exploiting a very small part of the information that web texts wear on their sleeves. we showed that lightweight text processing techniques make  it possible to acquire a very broad range of categorized instance names from unstructured text. in our experiments, word capitalization was the only clue used to detect possible names in text. to increase precision and recall, we will explore  other clues for  nding candidate names, as well as a full edged iterative learning algorithm for detecting contextual extraction patterns.
unsupervised named entity extraction from the web  an experimental study. in its  rst major run, knowitall extracted over ##,### class instances, but suggested a challenge  how can we improve knowitall s recall and extraction rate without sacri cing precision? this paper presents three distinct ways to address this challenge and evaluates their performance. in concert, our methods gave knowitall a # fold to # fold  increase in recall at precision of #.##, and discovered over ##,### cities missing from the  tipster gazetteer. this paper describes knowitall, an unsupervised, domain independent system that extracts information from the web. in our initial experiments with knowitall, we have focused on a sub problem of information extraction, building lists of named entities found on the web, such as instances of the class city or the class film. knowitall is able to extract instances of relations, such as capitalof(city,country) or starsin(actor,film), but the focus of this paper is on extracting comprehensive lists of named entities. by instantiating the pattern for the class city, knowitall extracts three candidate cities from the sentence   we provide tours to cities such as paris, london, and berlin. since we cannot compute  true recall  on the web, the paper uses the term  recall  to refer to the size of the set of facts extracted. in this paper we focus on one key challenge  how can we improve knowitall s recall and extraction rate so that it extracts substantially more members of large classes such as cities and  lms while maintaining high precision? we describe and compare three distinct methods added to knowitall in order to improve its recall    pattern learning (pl)  learns domain speci c patterns that serve both as extraction rules and as validation patterns to assess the accuracy of instances extracted by the rules. we evaluate each method experimentally, demonstrate their synergy, and compare with the baseline knowitall system described in [##]. our main contributions are  #. we demonstrate that it is feasible to carry out unsupervised, domain independent information extraction from the web with high precision. we present the  rst comprehensive overview of knowitall, our novel information extraction system. we describe knowitall s key design decisions and the experimental justi cation for them. we show that web based mutual information statistics can be effective in validating the output of an information extraction system. we describe and evaluate three methods for improving the recall and extraction rate of a web information extraction system. while our implementation is embedded in knowitall, the lessons learned are quite general. for example, we show that le typically  nds  ve to ten times more extractions than other methods, and that its extraction rate is forty times faster. we demonstrate that our methods, when used in concert, can increase knowitall s recall by # fold to # fold over the baseline knowitall system. the remainder of this paper is organized as follows. sections # to # describe our three methods for enhancing knowitall s recall, and section # reports on our experimental comparison between the methods. we discuss related work in section #, directions for future work in section #, and conclude in section #. our pattern learning (pl), subclass extraction (se), and list extraction (le) methods greatly improve on the recall of the baseline knowitall system described in [##], while maintaining precision and improving extraction rate. remarkably, we found that le s extraction rate was over forty times greater than that of the other methods.
large scale named entity disambiguation based on wikipedia data. this paper presents a large scale system for the  recognition and semantic disambiguation of  named entities based on information extracted  from a large encyclopedic collection and web   search results. our aim has been to build a named entity recognition and disambiguation system that employs a comprehensive list of entities and a vast amount of world knowledge. thus, we turned our attention to the wikipedia collection, the largest organized knowledge repository on the web (remy, ####). they employed several of the disambiguation resources discussed in this paper (wikipedia entity pages, redirection pages,  categories, and hyperlinks) and built a contextarticle cosine similarity model and an svm based on a taxonomy kernel. the system discussed in this paper performs both named entity identification and disambiguation. however, while nominator made heavy use of heuristics and lexical clues  to solve the structural ambiguity of entity mentions, we employ statistics extracted from wikipedia and web search results. we augment the wikipedia category information with information automatically extracted from wikipedia list pages and use it in conjunction with the context information in a vectorial model that employs a novel disambiguation method. we presented a large scale named entity disambiguation system that employs a huge amount of  information automatically extracted from wikipedia over a space of more than #.# million entities. the system described in this paper has been fully implemented as a web browser (figure #), which can analyze any web page or client text document. the application on a large scale of such an entity extraction and disambiguation system could result in a move from the current space of words to a space of concepts, which enables several paradigm shifts and opens new research directions, which we  are currently investigating, from entity based indexing and searching of document collections to  personalized views of the web through entitybased user bookmarks.
a framework for named entity recognition in the open domain. in this paper, a system for named entity recognition in the open domain (nero) is described. in this paper, the goal is to automatically identify the entities that are likely to be of interest in any scenario context, with no knowledge a priori. this paper has presented a framework for ner  in the open domain, and has described an implemented system, nero, that embodies this framework (section #). the evaluation reported in this paper has been insu cient. the overall value of the framework proposed in this paper remains an open question.
the snow learning architecture. we present two machine learning approaches to this problem, which we call the   robust reading   problem. our first approach is a discriminative approach, trained in a supervised way. our second approach is a generative model, at the heart of which is a view on how documents are generated and how names (of different entity types) are   sprinkled   into them. in its most general form, our model assumes  (#) a joint distribution over entities (e.g., a document that mentions   president kennedy   is more likely to mention   oswald   or   white house   than   roger clemens  ), (#) an   author   model, that assumes that at least one mention of an entity in a document is easily identifiable, and then generates other mentions via (#) an appearance model, governing how mentions are transformed from the   representative   mention. we show that both approaches perform very accurately, in the range of ##    ##  f# measure for different entity types, much better than previous approaches to (some aspects of) this problem. our extensive experiments exhibit the contribution of relational and structural features and, somewhat surprisingly , that the assumptions made within our generative model are strong enough to yield a very powerful approach, that performs better than a supervised approach with limited supervised information. to search a large collection of articles in order to pinpoint the concise answer    on may ##, ####. was born on november ##, ####   , but this fact refers to our target entity s son. ad hoc solutions to this problem, as we show, fail to provide a reliable and accurate solution. this paper presents two learning approaches to the problem of robust reading, compares them to existing approaches and shows that an unsupervised learning approach can be applied very successfully to this problem, provided that it is used along with strong but realistic assumptions on the nature of the use of names in documents.
unsupervised learning of name structure from coreference data. we present two methods for learning the structure of personal names from unlabeled data. we found that coreference constraints on names improve the performance of   the model from ##.#  to ##.# . we are interested in this problem in its own right, but  also as a possible way to improve named entity   recognition (by recognizing the structure of different kinds of names) and as a way to improve  noun phrase coreference determination. we present two methods for the unsupervised learning of the structure of personal names as  found in wall street journal text. more specifically, we consider a  name  to be a sequence of  proper nouns from a single noun phrase (as indicated by penn treebank style parse trees). for example,  defense secretary george w. smith  would be a name and we would analyze it into  the components  defense secretary  (a descriptor),  george  (a  rst name),  w.  (a middle name, we do not distinguish between initials and  true  names), and  smith  (a last name). we consider two unsupervised models for learning this information. we henceforth call this the  name  model. typically the same individual is mentioned several times  in the same article (e.g., we might later encounter  mr. smith ), and the pattern of such references, and the mutual constraints among them, could very well help our unsupervised methods determine the correct structure. we  call this the  coreference  model. we were attracted to this second model as it might offer a small example of how semantic information like  coreference could help in learning structural information. to the best of our knowledge there has not been any previous work on learning personal structure. we are aware of one previous case of unsupervised learning of lexical information from possible coreference, namely that of ge et. we have presented two methods for the unsupervised discovery of personal name structure. as we have also noted, we should also be able to  use this research in the quest for better unsupervised learning of named entity recognition,  and the model that attends to coreference information can potentially be useful for programs aimed directly at this latter problem. finally, many of us believe that the power of unsupervised learning methods for linguistic vice president president reagan mr. president president reagan dean p. guerin guerin   amp  turner ronald reagan white house speaker house james wright rev.
a latent dirichlet model for unsupervised entity resolution. we show  how to extend the latent dirichlet allocation model for this task  and propose a probabilistic model for collective entity resolution  for relational domains where references are connected to each other. our approach differs from other recently proposed entity resolution  approaches in that it is a) generative, b) does not make pair wise   decisions and c) captures relations between entities through a hidden group variable. we propose a novel sampling algorithm for  collective entity resolution which is unsupervised and also takes  entity relations into account. additionally, we do not assume the  domain of entities to be known and show how to infer the number  of entities from the data. we demonstrate the utility and practicality  of our relational entity resolution approach for author resolution in   two real world bibliographic datasets. in addition, we present preliminary results on characterizing conditions under which relational  information is useful. given a collection of entity references, or references for short, we would like to a) determine the collection of  true  underlying entities and b) correctly map the references in the collection to these entities. examples include computer vision, where we need to  gure out when regions in two different images refer to the same underlying object (the correspondence problem)  natural language processing where we would like to determine which noun phrases refer to the same underlying entity (co reference resolution)  and databases, where, when merging two databases or cleaning a database, we need to determine when two records are referring to the same underlying individual (deduplication). we are interested in resolving references when they are  connected to each other via relational links, as in the bibliographic domain where author names in papers are connected by co author links. we show that collective entity resolution improves performance over independent pair wise resolution. our model differs from most of the above in that it is unsupervised, does not assume the underlying entities to be known, does not make pairwise decisions and explicitly models relations between entities using group membership. we introduce a generative probabilistic model for entity  resolution that builds on the recently proposed latent dirichlet allocation model (lda) [#]. unlike most existing models, we do not introduce a decision variable for each potential duplicate pair of references, but instead have an entity label for each reference. to model collaborative relations between entities, we introduce a group label for each reference, so that entities coming from the same collaborative group are  more likely to be observed in a relation. for author resolution, this means that we model collaborative groups to explain co authorship relations. the generative process in our  model may be viewed as an extension of the dirichlet process mixture model  the group labels in our model in uence the choice of entities for each author reference in a paper. another contribution of this paper is an unsupervised gibbs sampling algorithm for collective entity resolution. it is unsupervised because we do not make use of a labeled  training set and it is collective because the resolution decisions depend on each other through the group labels. further, the number of entities is not  xed in our model, and we propose a novel sampling strategy to estimate the most likely number of entities given the references. we present a motivating example in section # and related research in section #. in section #, we  rst adapt the lda model for document authors and extend it for entity resolution in section #. in section # and section #, we describe how entity attributes  are modeled. section # describes our novel algorithm for determining the number of entities and in section ## and section ## we explore parameter choices and algorithmic improvements . finally, we present experimental results on real and synthetic data in section ## and conclude in section ##. in this paper, we have developed a probabilistic generative model for collectively resolving entities in relational data. our model may be viewed as extending the dirichlet process mixture model to capture relations between entities or components. we propose an unsupervised approach for collective inference in our model that does not require any  labeled training data. in addition, we present a novel sampling strategy to estimate the number of entities automatically from the references. we have demonstrated the utility of the proposed model on two real world citation datasets. additionally, we have identi ed some of the conditions under which these models are expected to provide greater bene t. areas for future work include extending the models to resolve multiple entity classes and better characterization of collaborative graphs amenable to these models.
probabilistic cfg with latent annotations. this paper de nes a generative probabilistic model of parse trees, which we call  pcfg la. in experiments using the penn wsj corpus, our   automatically trained model gave a performance of ##.#  (f  , sentences ##  words), which is comparable to that of an  unlexicalized pcfg parser created using  extensive manual feature selection. this paper de nes a generative model of parse trees that we call pcfg with latent annotations (pcfg la). the main focus of this paper is to examine the effectiveness of the automatically trained models in parsing. because exact inference with a pcfg la, i.e., selection of the most probable parse, is np hard, we are forced to use some approximation of it. we empirically compared three different approximation  methods. henderson s parsing model (henderson, ####) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced  hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours.
learning accurate, compact, and interpretable tree annotation. we present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood   of a training treebank. starting with a simple xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. in contrast with previous work, we are able   to split various terminals to different degrees, as appropriate to the actual complexity in the data. our  grammars automatically learn the kinds of linguistic  distinctions exhibited in previous work on manual  tree annotation. on the other hand, our grammars   are much more compact and substantially more accurate than previous work on automatic annotation. despite its simplicity, our best grammar achieves  an f  # of ##.#  on the penn treebank, higher than  fully lexicalized systems. in this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as np, vp, etc.) we present a method that combines the strengths of  both manual and automatic approaches while addressing some of their common shortcomings. (####) and prescher (####), we induce splits in a fully automatic fashion. however, we use a  more sophisticated split and merge approach that allocates subsymbols adaptivelywhere they are most effective, like a linguist would. another contribution is that, unlike previous work, we investigate smoothed models, allowing us to split grammars more heavily before running into the oversplitting effect discussed in klein and manning (####), where data fragmentation outweighs increased expressivity. our method is capable of learning grammars of substantially smaller size and higher accuracy than previous grammar re nement work, starting from a simpler initial grammar. for example, even beginning with an x bar grammar (see section #.#) with ## symbols, our best grammar, using #### symbols, achieves a test set f # of ##.# . our grammar s accuracy was higher than fully lexicalized systems, including the maximum entropy inspired parser of charniak and johnson (####). #.# experimental setup we ran our experiments on the wall street journal  (wsj) portion of the penn treebank using the standard setup  we trained on sections # to ##, and we  used section # as a validation set for tuning model hyperparameters. we used the evalb parseval reference implementation, available from sekine and collins (####), for scoring. for the  nal test  we selected the grammar that performed best on the development set. our experiments are based on a completely unannotated x bar style grammar, obtained directly from the penn treebank by the binarization procedure shown in by using a split and mergestrategy and beginningwith  the barest possible initial structure, our method reliably learns a pcfg that is remarkably good at parsing. hierarchical split merge training enables us to  learn compact butaccurate grammars, rangingfrom extremely compact (an f # of ##  with only ### symbols) to extremely accurate (an f # of ##.#  for our largest grammar with only #### symbols). in order to overcome data fragmentation and over tting, we smooth our parameters. smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without over tting our training set. not only is our parser more accurate, but the learned grammar is also signi cantly smaller than that of previous work.
lattice minimum bayes risk decoding for statistical machine translation. we present minimum bayes risk (mbr) decoding over translation lattices that compactly   encode a huge number of translation hypotheses. we describe conditions on the loss function that will enable ef cient implementation  of mbr decoders on lattices. we introduce   an approximation to the bleu score (papineni et al., ####) that satis es these conditions. our experiments show   that the lattice mbr decoder yields moderate, consistent gains in translation performance over n best mbr decoding on arabicto english, chinese to english and englishto chinese translation tasks. we conduct a   range of experiments to understand why lattice mbr improves upon n best mbr and  study the impact of various parameters on  mbr performance. in this paper we explore a different strategy  to perform mbr decoding over translation lattices (uef ng et al., ####) that compactly encode a huge number of translation alternatives relative to an n best list. we will introduce conditions on the loss functions that can be incorporated in lattice mbr decoding. we  describe an approximation to the bleu score (papineni et al., ####) that will satisfy these conditions. our lattice mbr decoding is realized using weighted finite state automata. we expect lattice mbr decoding to improve  upon n best mbr primarily because lattices contain many more candidate translations than the nbest list. we conduct a range of translation experiments to analyze lattice  mbr and compare it with n best mbr. an important aspect of our lattice mbr is the linear approximation to the bleu score. we will show that mbr decoding under this score achieves a performance that is at least as good as the performance obtained under sentence level bleu score. we  review mbr decoding in section # and give the formulation in terms of a gain function. in section #, we describe the conditions on the gain function for  ef cient decoding over a lattice. in section #, we introduce the corpus bleu approximation that makes  it possible to perform ef cient lattice mbr decoding. we present lattice mbr  experiments in section #.
a discriminative latent variable model for statistical machine translation. we  argue that a principle reason for this failure is   not dealing with multiple, equivalent translations. we present a translation model which  models derivations as a latent variable, in both   training and decoding, and is fully discriminative and globally optimised. additionally, we   show that regularisation is essential for maximum conditional likelihood models in order  to avoid degenerate solutions. we believe this is because these frequency count based # models cannot easily incorporate non independent  and overlapping features, which are extremely useful in describing the translation process. we argue that this is due  to a number of inherent problems that discriminative models for smt must address, in particular the  problems of spurious ambiguity and degenerate solutions. for this reason, to our knowledge, all discriminative models proposed to date either side step the problem by choosing simple  model and feature structures, such that spurious ambiguity is lessened or removed entirely (ittycheriah  and roukos, ####  watanabe et al., ####), or else ignore the problem and treat derivations as translations (liang et al., ####  tillmann and zhang, ####). in this paper we directly address the problem of spurious ambiguity in discriminative models. we use a synchronous context free grammar (scfg) translation system (chiang, ####), a model which  has yielded state of the art results on many translation tasks. we present two main contributions. first, we develop a log linear model of translation which  is globally trained on a signi cant number of parallel sentences. our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates     (koehn et al., ####). second, within this framework, we model the derivation, d, as a latent variable, p(e, d f ), which is marginalised out in training and decoding. we show empirically that this treatment results in signi cant improvements over a maximum derivation model. in section # we list the challenges that discriminative smt must  face above and beyond the current systems. we situate our work, and previous work, on discriminative systems in this context. we present our model  in section #, including our means of training and decoding. section # reports our experimental setup and results, and  nally we conclude in section #.
a better  best list  practical determinization of weighted finite tree automata. we introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of  the weight of all multiply derived trees. we also demonstrate our algorithm s effectiveness on two large scale tasks. in this paper we extend that work to deal with  grammars that produce trees. we will present an algorithm for determinizing weighted  nite tree recognizers, and use a variant of the procedure found in (huang and chiang, ####) to obtain  best lists of trees that are weighted correctly and contain no repetition. in section #, we  introduce the formalisms of tree automata, speci cally the tree to weight transducer. in section #, we present the algorithm. finally, in section # we show the results of applying weighted determinization to recognizers obtained from the packed forest output of two natural language tasks. we have shown that weighted determinization is useful for recovering  best unique trees from a weighted forest. we have improved evaluation scores by incorporating the presented algorithm into our mt work and we believe that other nlp researchers working with trees can similarly bene t from this algorithm. we plan for our weighted determinization algorithm to be one component in a generally available tree automata package for intersection, composition, training, recognition, and generation of weighted and unweighted tree automata for research tasks such as the ones described above.
minimum bayes risk decoding for statistical machine translation. we present minimum bayes risk (mbr) decoding for statistical machine translation. we describe a hierarchy of loss functions that incorporate different levels of linguistic information  from word strings, word to word alignments  from an mt system, and syntactic structure  from parse trees of source and target language  sentences. we report the performance of the   mbr decoders on a chinese to english translation task. our results show that mbr decoding can be used to tune statistical mt performance for speci c loss functions. however, given the many factors that in uence translation quality, it is unlikely that we will  nd a single  translation metric that will be able to judge all these factors. we  apply the minimum bayes risk (mbr) techniques developed for automatic speech recognition (goel and byrne,  ####) and bitext word alignment for statistical mt (kumar and byrne, ####), to the problem of building automatic mt systems tuned for speci c metrics. we will show that mbr decoding can be applied to machine translation in two scenarios. given an automatic  mt metric, we design a loss function based on the metric and use mbr decoding to tune mt performance under the metric. we also show how mbr decoding can be used to incorporate syntactic structure into a statistical mt system by building specialized loss functions. in particular we describe  the design of a bilingual tree loss function that can explicitly use syntactic structure for measuring translation quality. mbr decoding under this loss function allows us to integrate syntactic knowledge into a statistical mt  system without buildingdetailed models of linguisticfeatures, and retraining the system from scratch. we  rst present a hierarchy of loss functions for translation based on different levels of lexical and syntactic information from source and target language sentences. this hierarchy includes the loss functions useful in both situations where we intend to apply mbr decoding. we # translation under the various translation loss functions. we  nally report the performance of mbr decoders optimized for each loss function.
estimation of probabilities from , pame data for the language model component of a .weech recognizer. in this paper we present a model for statistical english to korean transliteration that generates transliteration candidates with probability. in this paper, we present a statistical method to transliterate english words in korean alphabet to generate various candidates. net)  phonetic mapping table construction first of all, we generate a mapping between english and korean phonetic unit pairs ( table # ).
named entity transliteration with comparable corpora. in this paper we investigate chineseenglish name transliteration using comparable corpora, corpora where texts in the  two languages deal in some of the same  topics   and therefore share references   to named entities   but are not translations of each other. we present two distinct methods for transliteration, one approach using phonetic transliteration, and   the second using the temporal distribution of candidate pairs. we then propose a   novel score propagation method that utilizes the co occurrence of transliteration   pairs within document pairs. as part of a more general project on multilingual named entity identi cation, we are interested  in the problem of name transliteration across languages that use different scripts. one particular issue is thediscovery of named entities in  comparable  texts in multiple languages, where by comparable we mean texts that are about the same topic, but are not in general translations of each other. # we wish to use this expectation to leverage transliteration, and thus the identi cation of named entities across languages. our idea is that the occurrence of a cluster of names in, say, an english text, should be useful if we  nd a cluster of what looks like the same names in a chinese or arabic text. an example of what we are referring to can be found in figure #. we will demonstrate that this intuition is correct. in this paper we have discussed the problem of  chinese english name transliteration as one component ofa system to  nd matching names in comparable corpora. we have proposed two methods for transliteration, one that is more traditional and based on phonetic correspondences, and one that  is based on word distributions and adopts methods from information retrieval. we have shown that both methods yield good results, and that even better results can be achieved by combining the methods. we have further showed that one can  improve upon the combined model by using reinforcement via score propagation when transliteration pairs cluster together in document pairs. the work we report is ongoing. we are investigating transliterations among several language  pairs, and are extending these methods to korean, arabic, russian and hindi   see (tao et al., ####).
machine transliteration. we describe evaluate a method for performing backwards transliterations by machine. usc inforrnation sciences institute marina del rey ca ##### usc computer science department los angeles ca ##### t usc computer science department los angeles ca ##### ( ) #### association for computational linguistics computational linguistics volume ## number # t (a)   (ka)   (sa)   (ta)  (na)    quot  (ha)  (ma)   (ra) (i)   (k )   (shi) y (ch )   (ni) a (hi)   (mi)   (ri) (u)   (ku) x (su) # (tsu)   (nu) # (hu)   (mu) #  (ru)  n(e)  (ke)   (se)   (te)   (he)   (he) fl (me)   (re)    (ba) #  quot (ga)    lt  (pa)  y(za)  (da) t (a)  v (ya)   (ya)    lt (be)  (ge)   (pe)    apos (ze)   (n)   (e)   (v) (bo)  (go)   (po)   (zo)    apos (chi)   (o) v (wa)    angela johnson omaha beach pro soccer tonya harding ramp lamp casual fashion team leader notice how the transliteration is more phonetic than orthographic  the letter h indoes not produce any katakana. transliteration is not trivial to automate but we will be concerned with an even more challenging problem  going from katakana back to english i.e.
a joint source channel model for machine transliteration. with the n gram tm model, we automate  the orthographic alignment process to  derive the aligned transliteration units from  a bilingual dictionary. in this paper, we focus on automatic chinese transliteration of foreign alphabet names. because some alphabet writing systems use various diacritical marks, we find it more practical to write names containing such diacriticals as they are rendered in english. therefore, we refer all foreign chinese transliteration to english chinese transliteration, or e#c. in this paper we discuss the limitations of such an approach and address its problems by firstly proposing a paradigm that allows direct orthographic mapping (dom), secondly further proposing a joint source channel model as a realization of dom. this paper is organized as follows  in section #, we present the transliteration problems. in section #, we relate our algorithms to other reported work. finally, we conclude the study with some discussions. in this paper, we propose a new framework (dom) for transliteration. integrated into the decoding process in n gram tm, which allows us to achieve a joint optimization of alignment and transliteration automatically. we expect to see the proposed model to be further explored in other related areas.
alignment based discriminative string similarity. we propose an alignment based discriminative framework for string similarity. we gather features from substring pairs consistent with a character based alignment of  the two strings. this approach achieves  exceptional performance  on nine separate  cognate identi cation experiments using six   language pairs, we more than double the precision of traditional orthographic measures  like longest common subsequence ratio  and dice s coef cient. we also show strong   improvements over other recent discriminative and heuristic similarity functions. we propose an alignment based, discriminative  approach to string similarity and evaluate this approach on cognate identi cation. in  section #, we explain our technique for automatically creating a cognate identi cation training set. in section #, we describe our bitext and dictionary based experiments on six language pairs, including three based on non roman alphabets. in section #, we show signi cant improvements over traditional approaches, as well as signi cant gains  over more recent techniques by ristad and yianilos (####), tiedemann (####), kondrak (####), and klementiev and roth (####). we have introduced and successfully applied an  alignment based framework for discriminative similarity that consistently demonstrates improved performance in both bitext and dictionary based cognate identi cation on six language pairs. our improved approach can be applied in any of the diverse applications where traditional similarity measures like edit distance and lcsr are prevalent. we have also made available our cognate identi cation data sets, which will be of interest to general string similarity researchers. furthermore, we have provided a natural framework for future cognate identi cation research. phonetic, semantic, or syntactic features could be included within our discriminative infrastructure to aid  in the identi cation of cognates in text. in particular, we plan to investigate approaches that do not require the bilingual dictionaries or bitexts to generate  training data. we may also compare alignmentbased discriminative string similarity with a more  complex discriminative model that learns the alignments as latent structure (mccallum et al., ####).
named entity transliteration and discovery from multilingual comparable corpora. this paper   presents an algorithm to automatically discover named entities (nes) in a resource  free language, given a bilingual corpora  in which it is weakly temporally aligned   with a resource rich language. we observe that nes have similar time distributions across such corpora, and that they   are often transliterated, and develop an algorithm that exploits both iteratively. we evaluate the algorithm  on an english russian corpus, and show  high level of nes discovery in russian. in this work, we make two independent observations about named entities encountered in such corpora, and use them to develop an algorithm that extracts pairs of nes across languages. speci cally, given a bilingual corpora that is weakly temporally aligned, and a capability to annotate the text in one of the languages with nes, our algorithm identi es the corresponding nes in the second language text, and annotates them with the appropriate type, as in the source text. , figure # shows a histogram of the number of occurrences of the word  hussein and its russian transliteration in our bilingual news corpus spanning years #### through late ####. we can exploit such weak synchronicity of nes across languages as a way to associate them. in order to score a pair of entities across languages, we compute the similarity of their time distributions. we introduce an algorithm we call co ranking which exploits these observations simultaneously to match nes on one side of the bilingual corpus to their counterparts on the other. we use a discrete fourier transform (arfken, ####) based metric for computing similarity of time distributions, and we score nes similarity with a linear transliteration  model. that is, we attempt to choose a candidate which is both a good transliteration (according to the current model) and is well aligned with the ne. thus, in order to rely on the time sequences we obtain, we need to be able to group variants of  the same ne into an equivalence class, and collect their aggregate mention counts. we would then score time sequences of these equivalence classes. for instance, we would like to count the aggregate number of occurrences of  herzegovina, hercegovina on the english side in order to map it accurately to the equivalence class of that ne s variants we may see on the russian side of our corpus (e.g. in order to effectively rely on the quality of time sequence scoring, we used a simple, knowledge poor approach to group ne variants for russian. in the rest of the paper, whenever we refer to a named entity, we imply an ne equivalence class. note that although we expect that better use of language speci c knowledge would improve the results, it would defeat one of the goals of this work. we have proposed a novel algorithm for cross lingual ne discovery in a bilingual weakly temporally aligned corpus. we have demonstrated that using  two independent sources of information (transliteration and temporal similarity) together to guide ne extraction gives better performance than using either of them alone (see figure #). we developed a linear discriminative transliteration model, and presented a method to automatically generate features. for time sequence matching, we  used a scoring metric novel in this domain. as supported by our own experiments, this method outperforms other scoring metrics traditionally used (such as cosine (salton and mcgill, ####)) when corpora are not well temporally aligned. in keeping with our objective to provide as little language knowledge as possible, we introduced a simplistic approach to identifying transliteration  equivalence classes, which sometimes produced erroneous groupings (e.g. we expect that language speci c knowledge used to discover accurate equivalence classes would result in performance improvements.
active sample selection for named entity transliteration. this paper introduces a new method for  identifying named entity (ne) transliterations   within bilingual corpora. we show   how to effectively train an accurate transliteration classi er using very little data, obtained   automatically. to perform this task, we introduce a new active sampling paradigm for guiding and adapting the sample selection process. we also investigate how to improve the classi er by identifying repeated patterns in the   training data. we evaluated our approach using english, russian and hebrew corpora. this paper presents a new approach for constructing a discriminative transliteration model. our approach is fully automated and requires little knowledge of the source and target languages. in our settings, we have access to source language ne and the ability to label the data upon request. we introduce a new active sampling paradigm that  aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. unlike other approaches,our approach is based on minimizing the distance between the feature distribution of a comprehensive reference set and the sampled set. in this paper we presented a new approach for constructing a transliteration model automatically and ef ciently by selectively extracting transliteration samples covering relevant parts of the feature space and focusing the learning process on these features. we show that our approach can outperform systems requiring supervision, manual intervention and a considerable amount of data. we propose a new measure for selective sample selection which can be  used independently. we currently investigate applying it in other domains with potentially larger feature learning russian hebrew train  feature top top top top ing weights one  ve one  ve + + #.## #.## #.## #.##   + #.## #.## #.## #.## +   #.## #.## #.## #.##  space than used in this work. another aspect investigated is using our selective sampling for adapting  the learning process for data originating from different sources  using the a reference set representative of the testing data, training samples, originating from a different source , can be biased towards the testing data.
prototype driven learning for sequence models. we investigate prototype driven learning for primarily unsupervised sequence modeling. for example, we can  achieve an english part of speech tagging accuracy  of ##.#  using only three examples of each tag  and no dictionary constraints. we also compare to  semi supervised learning and discuss the system s  error trends. we therefore consider here how to learn models with the least effort. in particular, we argue for a certain kind of semi supervised learning, which we call prototype driven learning. in prototype driven learning, we specify prototypical examples for each target label or label con guration, but do not necessarily label any documents or sentences. for example, when learning a model for  penn treebank style part of speech tagging in english, we may list the ## target tags and a few examples of each tag (see  gure # for a concrete prototype list for this task). in this paper, we consider three sequence modeling tasks  part of speech tagging in english and chinese and a classi ed ads information extraction task. our general approach is to use distributional  similarity to link any given word to similar prototypes. we then encode these prototype links as features in a log linear generative model, which is trained to  t unlabeled data (see  section #.#). we have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.
guiding semi supervision with constraint driven learning. in this paper,   we suggest a method for incorporating domain knowledge in semi supervised learning algorithms. our novel framework uni es  and can exploit several kinds of task speci c   constraints. this paper proposes a novel constraints based  learning protocol for guiding semi supervised learning. we develop a formalism for constraints based learning that uni es several kinds of constraints  unary, dictionary based and n ary constraints, which encode structural information and interdependencies  among possible labels. one advantage of our formalism is that it allows capturing different levels of constraint violation. our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data while learning (see section #. in the experimental part of this paper we use hmms as the underlying model, and exhibit signi cant reduction in the number of training examples required in two information extraction problems. our algorithm pushes this intuition further, in that the use of constraints allows us to better exploit domain information as a way to label, along with the current learned model,  unlabeled examples. given a small amount of labeled data and a large unlabeled pool, our framework initializes the model with the labeled data and then repeatedly  (#) uses constraints and the learned model to label the instances in the pool. this way, we can generate better  training  examples during the semi supervised learning process. the core of our approach, (#), is described in section #. we proposed to use constraints as a way to guide  semi supervised learning. moreover, our framework is a useful tool when the domain knowledge cannot be expressed by the model.
incremental integer linear programming for non projective dependency parsing. we present an   approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. this approach is applied to dutch dependency  parsing and we show how the addition  of linguistically motivated constraints can   yield a signi cant improvement over stateof the art. similarly, if we  nd that one coordination argument is a noun, then the other argument cannot be a verb. in this paper we present a method which extends the applicability of ilp to a more complex set of problems. instead of adding all the constraints we wish to capture to the formulation, we  rst solve the program with a fraction of the constraints. we apply this dependency parsing approach to dutch due to the language s non projective nature, and take the parser of mcdonald et al. (####b) as a starting point for our model. in the following section we introduce dependency parsing and review previous work. in section # we present our model and formulate it as  an ilp problem with a set of linguistically motivated constraints. we include details of an incremental algorithm used to solve this formulation. our experimental set up is provided in section # and is followed by results in section # along  with runtime experiments. we  nally discuss future research and potential improvements to our approach. in this paper we have presented a novel approach for inference using ilp. while previous approaches which use ilp for decoding have solved  each integer linear program in one run, we incrementally add constraints and solve the resulting program until no more constraints are violated. this allows us to ef ciently use ilp for dependency parsing and add constraints which provide  a signi cant improvement over the current stateof the art parser (mcdonald et al., ####b) on the dutch alpino corpus (see bl row in table #). although slower than the baseline approach, our method can still parse large sentences (more than ## tokens) in a reasonable amount of time  (less than a minute). we have shown that parsing time can be signi cantly reduced using a  simple approximation which only marginally degrades performance. furthermore, we believe that the method has potential for further extensions and applications.
a linear programming formulation for global inference in natural language tasks. given a collection of discrete random variables   representing outcomes of learned local predictors in natural language, e.g., named entities   and relations, we seek an optimal global assignment to the variables in the presence of  general (non sequential) constraints. we develop a linear programming formulation for this problem  and evaluate it in the context of simultaneously   learning named entities and relations. our approach allows us to ef ciently incorporate domain and task speci c constraints at decision  time, resulting in signi cant improvements in  the accuracy and the  human like  quality of  the inferences. rather than being restricted on sequential data, we study a fairly general setting. the problem is de ned  in terms of a collection of discrete random variables representing binary relations and their arguments  we seek an optimal assignment to the variables in the presence of the constraints on the binary relations between variables and the relation types. following this work, we model inference as an optimization problem, and show  how to cast it as a linear program. our approach could be contrasted with other approaches to sequential inference or to general markov random  eld approaches (lafferty et al., ####  taskar et al., ####). in our approach, predictors do not need to be learned in the context of the decision tasks,  but rather can be learned in other contexts, or incorporated as background knowledge. this way, our approach allows the incorporation of constraints into decisions in a dynamic fashion and can therefore support task speci c inferences. the signi cance of this is clearly shown in our experimental results. we develop our models in the context of natural language inferences and evaluate it here on the problem of simultaneously recognizing named entities and relations between them. in our model, we  rst learn a collection of  local  predictors, e.g., entity and relation identi ers. at decision time, given a sentence, we produce a global decision that optimizes over the suggestions of the classi ers that are active in the sentence, known constraints among them  and, potentially, domain or tasks speci c constraints relevant to the current decision. thus, there are l n # possible assignments, which is too large even for a small n. when evaluated on simultaneous learning of named entities and relations, our approach not only provides a signi cant improvement in the predictors  accuracy  more importantly, it provides coherent solutions. while many statistical methods make  stupid  mistakes (i.e., inconsistency among predictions), that no human ever  makes, as we show, our approach improves also the quality of the inference signi cantly. section # formally de nes our problem and section # describes the  computational approach we propose.
computer programs for detecting and correcting spelling errors. in this report, we provide a stochastic model for string edit distance. our stochastic model allows us to learn a string edit distance function from a corpus of examples. we illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. in this application, we learn a string edit distance with nearly one fifth the error rate of the untrained levenshtein distance. our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes. in this report, we provide a stochastic model for string edit distance. our stochastic interpretation allows us to automatically learn a string edit distance from a corpus of examples. we illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in the switchboard corpus of conversational speech [#] . in this application, we learn a string edit distance that reduces the error rate of the untrained levenshtein distance by a factor of #.#, to within # percent of the minimum error rate achievable by any classifier. let us first define our notation. for convenience, we abbreviate the unit length substring x i i as x i and the length t prefix of x t as x t .
unsupervised topic modelling for multi party spoken discourse. we present a method for unsupervised  topic modelling which adapts methods  used in document classi cation (blei et  al., ####  grif ths and steyvers, ####) to   unsegmented multi party discourse transcripts. we show how bayesian inference in this generative model can be   used to simultaneously address the problems of topic segmentation and topic  identi cation  automatically segmenting   multi party meetings into topically coherent segments with performance which   compares well with previous unsupervised segmentation only methods (galley   et al., ####) while simultaneously extracting topics which rate highly when assessed  for coherence by human judges. we also  show that this method appears robust in  the face of off topic dialogue and speech  recognition errors. in this paper we present a method for unsupervised topic modelling which allows us to approach both problems simultaneously, inferring a set of  topics while providing a segmentation into topically coherent segments. we show that this model  can address these problems over multi party discourse transcripts, providing good segmentation  performance on a corpus of meetings (comparable to the best previous unsupervised method that we are aware of (galley et al., ####)), while also  inferring a set of topics rated as semantically coherent by human judges. we then show that its  segmentation performance appears relatively robust to speech recognition errors, giving us con dence that it can be successfully applied in a real speech processing system. section # then describes the model we use here. section # then details our experiments and results, and conclusions are drawn in section #.
integrating topics and syntax. we present a generative model that uses both kinds of  dependencies, and is capable of simultaneously  nding syntactic classes   and semantic topics despite having no knowledge of syntax or semantics beyond statistical dependency. in addition to producing clean syntactic and  semantic classes and identifying function and content words, our composite model is competitive in quantitative tasks, such as part of speech tagging and document classi cation, with models specialized to detect only one kind of dependency. first, we introduce the approach, considering the general question of how syntactic and semantic generative models might be combined, and arguing that a composite model is necessary to capture the different roles that words can play in a document. we then de ne a generative model of this form, and describe a markov chain monte carlo algorithm for inference in this model. finally, we present results illustrating the quality of the recovered syntactic classes and semantic topics. the composite model we have described captures the interaction between short  and longrange dependencies between words.
latent dirichlet allocation. we describe latent dirichlet allocation (lda), a generative probabilistic model for collections of  discrete data such as text corpora. we present  ef cient approximate inference techniques based on variational methods and an em algorithm for  empirical bayes parameter estimation. we report results in document modeling, text classi cation,  and collaborative  ltering, comparing to a mixture of unigrams model and the probabilistic lsi  model. in this paper we consider the problem of modeling text corpora and other collections of discrete data. the plsi approach, which we describe in detail in section #.#, models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of  topics. to see how to proceed beyond plsi, let us consider the fundamental probabilistic assumptions underlying the class of dimensionality reduction methods that includes lsi and plsi. thus, if we wish to consider exchangeable representations for documents and words, we need to consider mixture models that capture the exchangeability of both words and documents. this line of thinking leads to the latent dirichlet allocation (lda) model that we present in the current paper. we aim to demonstrate in the current paper that, by taking the de finetti theorem seriously, we can capture signi cant intra document statistical structure via the mixing distribution. thus, while the work that we discuss in the current paper focuses on simple  bag of words  models, which lead to mixture distributions for single words (unigrams), our methods are also applicable to richer models that involve mixtures for larger structural units such as n grams or paragraphs. in section # we introduce basic notation and terminology. we discuss inference and parameter estimation for lda in section #. finally, section # presents our conclusions.
hidden topic markov models. in this pa per , we pr op ose modeling the topics of words in the document as a markov  chain. speci cally, we assume that all words  in the same sentence have the same topic, and  successive sentences are more likely to have   the same topics. we show that incorporating this  dependency allows us to learn better topics  and to disambiguate words that can be long  to different topics. quantitatively, we show   that we obtain better perplexity in modeling documents with only a modest increase  in learning and inference complexity. we give necessary and su cient conditions for uniqueness of the support vector solution for the  problems of pattern recognition and regression estimation, for a general class of cost functions. we show that  if the solution is not unique, all support vectors are necessarily at bound, and we give so me simple examples  of non unique solu  tions. we note that uniqueness of the primal (dual) solution does not nec essarily imply  uniqueness of the dual (primal) solution. we s how how to compute the threshold b when the solution is unique,  but when all support vectors are at bound, in which case the usual method for determining b do e s not work. we give necessary and su cient conditions for uniqueness of the support vector solution for the  problems of pattern recognition and regression estimation, for a g e neral class of cost functions. we show that  if the solution is not unique, all support vectors are necessarily at bound, and we give some simple examples  of non unique solu  tions. we note that uniqueness of the primal (dual) solution does not necessarily imply  uniqueness of the dual (primal) solution. we show how to compute the threshold b w hen the so lution is unique,  but when all support vectors are at bound, in which case the usual method for determining b doe s not work. htmm is consistently better than lda but the difference in perplexity is signi cant only for n   ## observed words (the average length of a document after  our prepr ocessing was about #### words). on the bottom we see a section taken from the end of the paper,   consisting of the end of the discussion, acknowledg ements and the beginning of the references. we see that lda assigns different topics to different words within the same sentence   and therefore it is not suitable for topical segmentation. thus we can use it to disambiguate words that have  several meanings according to the context. however, we see that the topics found by the two  models are different in nature. we follow the same lines,  while we allow markovian relations between the hidden aspe c ts. we strive to extract latent aspects from documents by making use of the informatio n conveyed in the division into documents as well as the particular order of words in each document. following these lines we propose in this paper a novel and consistent probabilistic model we call the hidden topic markov model (htmm). we s how that the  htmm model outperforms the lda model in its predictive performance and can be used for text parsing and word sense disambiguation purposes.
catching the drift  probabilistic content models, with applications to generation and summarization. we consider the problem of modeling the content structure of texts within a speci c domain, in terms of the topics the texts address  and the order in which these topics appear. we  rst present an effective knowledge lean   method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for hidden markov models. we then apply our method to two complementary tasks  information ordering and extractive summarization. our experiments show   that incorporating content models in these applications yields substantial improvement over  previously proposedmethods. the focus of our  work, however,is on an equally fundamental but domaindependent dimension of the structure of text  content. our use of the term  content  corresponds roughly to the notions of topic and topic change. we desire models that can specify, for example, that articles about earthquakes typically contain information about quake strength, location, and casualties, and that descriptions of casualties usually precede those of rescue efforts. but rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un annotated texts via analysis of word distribution patterns. # in this paper, we investigate the utility of domainspeci c content models for representing topics and topic shifts. we  rst describe an ef cient, knowledge lean method  for learning both a set of topics and the relations between topics directly from un annotated documents. our technique incorporates a novel adaptation of the standard hmm induction algorithm that is tailored to the task of modeling content. then, we apply techniques based on content models to  two complex text processing tasks. first, we consider information ordering,that is, choosing a sequence in which  to present a pre selected set of items  this is an essential step in concept to text generation, multi document summarization, and other text synthesis problems. in our experiments, content models outperform lapata s (####) state of the art ordering method by a wide margin   for  one domain and performance metric, the gap was ## percentage points. second, we consider extractive summarization  the compression of a document by choosing  a subsequence of its sentences. for this task, we develop a new content model based learning algorithm for sentence selection. these observations,  taken together with the fact that content models are conceptually intuitive and ef ciently learnablefrom raw document collections, suggest that the formalism can prove useful in an even broader range of applications than we  have considered here  exploring the options is an appealing line of future research. in this paper, we present an unsupervised method for the induction of content models, which capture constraints  on topic selection and organization for texts in a particular domain. an important future direction lies in studying the correspondence between our domain speci c model and  domain independent formalisms, such as rst. by automatically annotating a large corpus of texts with discourse relations via a rhetorical parser (marcu, ####   soricut and marcu, ####), we may be able to incorporate domain independent relationships into the transition structureof our contentmodels. this study coulduncover interesting connections between domain speci c stylistic constraints and generic principles of text organization. in the future,we plan to investigate  how to bootstrap the induction of hierarchical models using labeled data derived from our content models. we  would also like to explore how domain independent discourse constraints can be used to guide the construction of the hierarchical models. acknowledgments we are grateful to mirella lapata for providingus the results of her system on our data, and to dominic jones and cindi thompson for supplying us  with their document collection. we also thank eli barzilay, sasha blair goldensohn, eric breck, claire cardie, yejin choi, marcia davidson, pablo duboue, no mie  elhadad, luis gravano, julia hirschberg, sanjeev khudanpur, jon kleinberg, oren kurland, kathy mckeown,  daniel marcu, art munson, smaranda muresan, vincent ng, bo pang, becky passoneau, owen rambow, ves stoyanov, chao wang and the anonymous reviewers for helpful comments and conversations. this paper is based upon  work supported in part by the national science foundation under grants itr im iis ####### and iis ####### and by an alfred p. sloan research fellowship.
modeling online reviews with multi grain topic models. in this paper we present a novel framework for extracting   the ratable aspects of objects from online user reviews. our models are based on extensions to standard topic modeling methods such as lda and plsa to   induce multi grain topics. we argue that multi grain models are more appropriate for our task since standard models  tend to produce topics that correspond to global properties  of objects (e.g., the brand of a product type) rather than  the aspects of an object that tend to be rated by a user. the models we present not only extract ratable aspects, but  also cluster them into coherent topics, e.g., waitress and  bartender are part of the same topic staff for restaurants. we evaluate the multi grain models both  qualitatively and quantitatively to show that they improve  signi cantly upon standard topic models. in this study we focus on online user reviews that have  been provided for products or services, e.g., electronics, ho  this work was done while at google inc. mittee (iw#c#). in this paper we focus on improved models for the  rst phase   ratable aspect  extraction from user reviews. in particular, we focus on unsupervised models for extracting these aspects. the model we describe can extend both probabilistic latent semantic http   www.zagat.com # http   www.tripadvisor.com analysis [##] and latent dirichlet allocation (lda) [#]   both of which are state of the art topic models. we start by showing that standard topic modeling methods, such as lda and plsa, do not model the appropriate aspects of  user reviews. to combat this we extend both plsa and lda to induce multi grain topics. speci cally, we allow the models to generate terms from either a global topic, which is chosen based on the document level context, or a local topic, which is chosen based on a sliding window context over the text. we evaluate the models both qualitatively and quantitatively. for the qualitative analysis we present a number of topics generated by both standard topic models and our new  multi grained topic models to show that the multi grain topics are both more coherent as well as better correlated with ratable aspects of an object. for the quantitative analysis we will show that the topics generated from the multi grained topic model can signi cantly improve multi aspect ranking  [##], which attempts to rate the sentiment of individual aspects from the text of user reviews in a supervised setting. in the rest of the section we introduce a multi grain model as a way to address the discovered limitations of plsa and  lda. in section # we provide an empirical evaluation of the proposed method. we conclude in section # with an examination of related work. throughout this paper we use the term aspect to denote properties of an object that are rated by a reviewer. other terms in the literature include features and dimensions, but we opted for aspects due to ambiguity in the use of alternatives.
topic modeling  beyond bag of words. while such models may use conditioning contexts of arbitrary length, this paper deals only with bigram models i.e., models that predict each word based on the immediately preceding word. in this paper, i present a hierarchical bayesian model  that integrates bigram based and topic based approaches to document modeling.
a uni ed local and global model for discourse coherence. note to readers  we have recently  detected a software bug which affects the   results of our standalone entity grid experiments. (the bug was in our syntactic  analysis code, which incorrectly failed to  label the second object of a conjoint vp   in the phrase  wash the dishes and clean   the sink ,  dishes  would be correctly labeled as o but  sink  mislabeled as x.) the results in table # above the  line are incorrect  our relaxed entity grid  does not outperform the naive grid on the  discriminative test. this implies that our  argument motivating the relaxed model at   the end of section # is misguided. we present a model for discourse coherence which combines the local entitybased approach of (barzilay and lapata,  ####) and the hmm based content model  of (barzilay and lee, ####). unlike the  mixture model of (soricut and marcu,  ####), we learn local and global features   jointly, providing a better theoretical explanation of how they are useful. as the  local component of our model we adapt  (barzilay and lapata, ####) by relaxing   independence assumptions so that it is effective when estimated generatively. our   model performs the ordering task competitively with (soricut and marcu, ####),  and signi cantly better than either of the  models it is based on. we attempt here to unify the two approaches by constructing a model with both sentence to sentence dependencies providing local cues, and a hidden  topic variable for global structure. our local features are based on the entity grid model of (barzilay and lapata, ####  lapata and barzilay, ####). this  model has previously been most successful in a conditional setting  to integrate it into our model, we  rst relax its independence assumptions to improve its performance when used generatively. our global model is an hmm like that of barzilay and lee (####), but with emission probabilities drawn from the entity grid. we present results for two tasks, the ordering task, on which global models usually  do well, and the discrimination task, on which local models tend to outperform them. our model improves on purely global or local approaches on both tasks. moreover, since the  model we describe uses a strict subset of the features used in the component models of (soricut and  marcu, ####), we suspect that adding it to the mixture would lead to still further improved results.
inferring strategies for sentence ordering in multidocument news summarization. in this paper, we propose  a methodology for studying the properties of ordering information in the news genre and  describe experiments done on a corpus of multiple acceptable orderings we developed for  the task. based on these experiments, we implemented a strategy for ordering information   that combines constraints from chronological order of events and topical relatedness. evaluation of our augmented algorithm shows a signi cant improvement of the ordering over  two baseline strategies. but in other cases, when information in a summary is  poorly ordered and readers cannot make sense of the text, we observed through interviews  with the readers that they tend to blame it on content selection rather than on ordering,  even if the content is not the issue. in this paper, we provide a corpus based methodology for studying ordering. our goal was to develop a good ordering strategy in the context of multidocument summarization  targeted for the news genre. the  rst question we addressed is the importance of ordering. we conducted experiments which show that ordering signi cantly affects the reader s comprehension of a text. our experiments also show that although there is no single ideal  ordering of information, ordering is not an unconstrained problem  the number of good orderings for a given text is limited. existing corpus based methods, such as supervised learning, are not easily applicable to our problem in part because of lack of training data. given that there are multiple possible orderings, a corpus providing one ordering for each  set of information does not allow us to differentiate between sentences which must be together and sentences which happen to be together. this led us to develop a corpus of data sets, each of which contains multiple acceptable orderings of a single text. instead, we used a hybrid corpus analysis strategy that  rst automatically identi es commonalities across orderings. finally, we evaluated plausible ordering strategies by asking humans to judge the results. our set of experiments together suggests an ordering algorithm that integrates constraints from an approximation of the temporal sequence of the underlying events and relatedness between content elements. our evaluation of plausible strategies measured the usefulness of a chronological ordering algorithm used in previous summarization systems (mckeown et al., ####  lin   amp  hovy, ####) as well as an alternative, original strategy, majority ordering. our evaluation showed that the two ordering algorithms alone do not yield satisfactory results. our automatic analysis revealed that topical relatedness is an important constraint  groups of related sentences tend to appear together. our algorithm combines chronological ordering with constraints from topical relatedness. in the following sections, we  rst show that the way information is ordered in a summary can critically affect its overall quality. we then give an overview of our summarization system, multigen. we next describe the two naive ordering algorithms and evaluate them, followed by a study of multiple orderings produced by humans. this allows us to determine how to improve the chronological ordering algorithm using cohesion as an additional constraint. in this paper we investigated information ordering constraints in multidocument summarization in the news genre. we evaluated two alternative ordering strategies, chronological ordering (co) and majority ordering (mo). our experiments show that mo performs well only when all input texts follow similar organization of the information. but in the news genre we cannot make this assumption  thus it is not an appropriate solution. our experiments, using a corpus that we collected of multiple alternative summaries each of multiple documents, show that cohesion is an important constraint contributing to ordering. we developed an operational algorithm that integrates cohesion as part of the co algorithm, and implemented it as part of the multigen summarization system. our evaluation of the system shows signi cant improvement in summary quality. while in this paper we focused on augmenting the co algorithm, we believe that mo is a promising strategy and should not be neglected. for our future work, we plan to build on the approach we used for the duc #### evaluation, where we developed a summarizer that would use different algorithms for summary generation depending on the type of input text. we suspect that ordering strategies may differ also, depending on the type of summary. our work will  rst investigate whether we can use our augmented algorithm for other summary types. if the algorithm does not yield good orderings, we will investigate through corpus analysis other summary type speci c constraints. we suspect that our augmented algorithm may apply, for instance, to biographical summaries, since the information being summarized is a mixture of event based information that can be chronologically ordered along with descriptive information about the person. we also plan to identify the types of summaries which would bene t from using the mo algorithm or an augmented version of it (the same way the co algorithm was augmented with the cohesion constraint).
probabilistic text structuring  experiments with sentence ordering. in this paper we propose an approach to  information ordering that is particularly   suited for text to text generation. we describe a model that learns constraints on   sentence order from a corpus of domainspeci c texts and an algorithm that yields   the most likely order among several alternatives. we evaluate the automatically  generated orderings against authored texts   from our corpus and against human subjects that are asked to mimic the model s  task. we also assess the appropriateness of   such a model for multidocument summarization. in this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus. (####) we construct an acceptable ordering rather than the best possible one. we propose an automatic method of evaluating the  orders generated by our model by measuring closeness or distance from the gold standard, a collection of orders produced by humans. the remainder of this paper is organized as follows. section # introduces our model and an algorithm for producing a possible order. section # describes our corpus and the estimation of the model  parameters. our experiments are detailed in section #. we conclude with a discussion in section #.
evaluating centering based metrics of coherence for text structuring using a reliably annotated corpus. we use a reliably annotated corpus to compare   metrics of coherence based on centering theory with respect to their potential usefulness for  text structuring in natural language generation.
finding scientific topics. we describe a generative model for documents introduced by blei ng jordan [blei d. m. ng a. y.   amp  jordan m. i. we then present a markov chain monte carlo algorithm for inference in this model. we use this algorithm to analyze abstracts from pnas by using bayesian model selection to establish the number of topics. we show that the extracted topics capture meaningful structure in the data consistent with the class designations pro vided by the authors of the articles outline further applica tions of this analysis including identifying   hot topics   by exam ining temporal dynamics tagging abstracts to illustrate semantic content. our algorithm to a corpus consisting of abstracts from pnas from #### to #### determining the number of topics needed to account for the information contained in this corpus ex  tracting a set of topics. we use these topics to illustrate the relationships between different scientific disciplines assessing trends   hot topics   by analyzing topic dynamics using the assignments of words to topics to highlight the semantic content of documents. if we have t topics we can write the probability of the ith word in a given document as p wi       j # t p wi  zi   j p zi   j  [#] where z iis a latent variable indicating the topic from which theunder the jth topic.
adaptation of maximum entropy capitalizer  little data can help a lot. we study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data driven approaches since vast amounts of  training  data are easily obtainable by simply wiping the case information in text. a case frequently encountered in practice is that of using mismatched   out of domain, in this particular case we used broadcast news    test data. in the  capitalization case we have studied, the relative performance improvement of the memm capitalizer over the # gram baseline drops from in domain   wsj   performance of ##  to ## ##  when used on the slightly mismatched bn data. in order to take advantage of the adaptation data in our scenario, a maximum a posteriori (map)  adaptation technique for maximum entropy (maxent) models is developed. we have also presented a general technique for adapting maxent probability models. as future work we plan to investigate the best way to blend increasing amounts of less speci c background training data with speci c, in domain data for this and other problems. another interesting research direction is to explore the usefulness of the map adaptation of maxent models for other problems among which we wish to include language modeling, part of speech tagging, parsing, machine translation, information extraction, text routing.
a statistical model for multilingual entity detection and tracking. in this paper, we present a statistical   language independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted  textdocuments,andchainingthem into clusters  corresponding to each logical entity present in  the text. we will instead  adopt the nomenclature of the automatic content extraction program # (nist, ####a)  we will call the instances of textual references to objects or abstractions mentions, which can be either named (e.g. in this paper, we present a general statistical frameworkfor entity detection and tracking in unrestricted text. we separate the edt task into a mention detection part   the task of  nding all mentions  in the text   and an entity tracking part   the task of combining the detected mentions into groups of references to the same object. we present and evaluate empirically statistical models for both mention detection and entity tracking problems. for mention detection we use approaches based on maximum entropy (maxent henceforth) (berger et al., ####) and robust risk minimization (rrm henceforth)  (zhang et al., ####). we investigate a wide array of lexical, syntactic and semantic features to perform the mention detection and classi cation task including,  for all three languages,features based on pre existingstatistical semantic taggers, even though these taggers have  beentrained ondifferentcorporaand use differentsemantic categories. we propose a novel maxent based model for predicting whether a mention should or should not be linked to an existing entity, and show how this model can be used to build entity chains. the framework presented in this paper is languageuniversal   the classi cation method does not make any  assumption about the type of input. section # presents our approach to entity tracking. this paper presents a language independent framework for the entity detection and tracking task, which is shown  to obtain top tier performance on three radically different languages  arabic, chinese and english.
supervised and unsupervised pcfg adaptation to novel domains. this paper investigates adapting a lexicalized  probabilistic context free grammar (pcfg) to  a novel domain, using maximum a posteriori   (map) estimation. in contrast to the results in gildea (####),  we show f measure parsing accuracy gains of as   much as #.#  for high accuracy lexicalized parsing through the use of out of domain treebanks,   with the largest gains when the amount of indomain data is small. we will show below that weighted count merging is a special case of map adaptation  hence the approach of gildea (####) cited above is also a special case of map adaptation, with a particular parameterization of the prior. we will present empirical results for multiple map adaptation schema, both starting from the penn wall st. journal treebank and adapting to the  brown corpus, and vice versa. we will compare our supervised adaptation performance with the results presented in gildea (####). with a manually annotated treebank, we will present results  for unsupervised adaptation, i.e. we investigate a number of unsupervised approaches, including multiple iterations, increased sample sizes, and self adaptation. what we have demonstrated in this paper is that maximum a  posteriori (map) estimation can make out of domain training data bene cial for statistical parsing. first, a question that is not addressed in this paper is how to best combine both supervised and unsupervised adaptation data. hence, we would like to investigate automatic methods for choosing mixing parameters, such as em. instead, we can conclude that, just as in other statistical estimation problems, there are generalizations to be had from these out of domain trees, providing more robust estimates, especially in the face of sparse training data.
domain adaptation for statistical classi ers. we consider the  common case in which labeled out of domain data is plentiful, but labeled in domain data is  scarce. we introduce a statistical formulation of this problem in terms of a simple mixture  model and present an instantiation of this framework to maximum entropy classi ers and  their linear chain counterparts. we present e cient inference algorithms for this special  case based on the technique of conditional expectation maximization. our experimental  results show that our approach leads to improved performance on three real world tasks  on four different data sets from the natural language processing domain. we do not seek to eliminate the annotation of in domain data, but instead seek to minimize the amount of new annotation effort required to achieve good performance. in this paper, we present a novel framework for understanding the domain adaptation problem. the key idea in our framework is to treat the in domain data as drawn from a mixture of two distributions  a  truly in domain  distribution and a  general domain  distribution. we apply this framework in the context of conditional classi cation models and conditional linear chain sequence labeling models, for which inference may be e ciently solved using the technique  of conditional expectation maximization. we apply our model to four data sets with varying degrees of divergence between the  in domain  and  out of domain  data and obtain predictive accuracies higher than any of a large number of baseline systems and a second model proposed in the literature for this problem. in this paper, we have presented the mega model for domain adaptation in the discriminative (conditional) learning framework. we have described e cient optimization algorithms based on the conditional em technique. we have experimentally shown, in four data sets, that our model outperforms a large number of baseline systems, including the current state of the art model, and does so requiring signi cantly less in domain data. although we focused speci cally on discriminative modeling in a maximum entropy framework, we believe the novel, basic idea on which this work is founded to break the in domain distribution p (i) and out of domain distribution p (o) into three distributions, q (i) , q (o) and q (g)  is general. our model can be seen as a constrained experts model, with three experts, where the constraints specify that in domain data can only come from one of two experts, and out of domain data can only come from one of two experts (with a single expert overlapping between the two). most attempts to build discriminative mixture of experts models make heuristic approximations in order to perform the necessary optimization (jordan   amp  jacobs, ####), rather than apply conditional em, which gives us strict guarantees that we monotonically increase the data (incomplete) log likelihood of each iteration in training.
domain adaptation with structural correspondence learning. for many nlp tasks,  however, we are confronted with new  domains in which labeled data is scarce  or non existent. in such cases, we seek   to adapt existing models from a resourcerich source domain to a resource poor  target domain. we introduce structural  correspondence learning to automatically  induce correspondences among features   from different domains. we test our technique on part of speech tagging and show  performance gains for varying amounts  of source and target training data, as well  as improvements in target domain parsing  accuracy using our improved tagger. however, in many situations we may have a source domain with plentiful labeled training data,  but we need to process material from a target domain with a different distribution from the source domain and no labeled data. in such cases, we must take steps to adapt a model trained on the  and acero, ####  ando, ####  lease and charniak, ####  daum  iii and marcu, ####). we hypothesize that a discriminative model trained in the source domain using  this common feature representation will generalize better to the target domain. this representation is learned using a method we call structural correspondence learning (scl). non pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. even on the unlabeled data, the co occurrence statistics of pivot and non pivot features are likely  to be sparse, and we must model them in a compact way. in this work we  choose to use the technique of structural learning (ando and zhang, ####a  ando and zhang,  ####b). we demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it. here,  we investigate its use in part of speech (pos) tagging (ratnaparkhi, ####  toutanova et al., ####). we show how to use scl to transfer a pos tagger from the wall street journal ( nancial news) to medline(biomedical abstracts), which  use very different vocabularies, and we demonstrate not only improved pos accuracy but also improved end to end parsing accuracy while using the improved tagger. an important but rarely explored setting in domain adaptation is when we have no labeled training data for the target domain. we  rst demonstrate that in this situation sclsigni cantly improves performance over both supervised and semi supervised taggers. in the case when some in domain labeled training data is available, we show how to use scl together with the classi er combination techniques of florian et al. in the next section, we describe a motivating example involving  nancial news and biomedical  data. we discuss related work on domain adaptation in section # and conclude in section #. finding correspondences involves estimating the correlations between pivot and non pivot feautres, and we adapt structural learning (aso) (ando and zhang, ####a  ando and zhang, ####b) for this  task. we showed results using scl to transfer a pos tagger from the wall street journal to a corpus  of medline abstracts. we also showed how to combine an scl tagger with target domain labeled data using the classi er combination techniques from florian et  al. finally, we improved parsing performance in the target domain when using the scl pos tagger. one of our next goals is to apply scl directly  to parsing. we are also focusing on other potential applications, including chunking (sha and pereira, ####), named entity recognition (florian et al., ####  ando and zhang, ####b  daum  iii and marcu, ####), and speaker adaptation (kuhn learning when we have labeled data from both  source and target domains.
mining domain specific thesauri from wikipedia  a case study. we show  how the classic thesaurus structure of terms and links can  be mined automatically from wikipedia, a vast, open  encyclopedia. in a comparison with a professional  thesaurus for agriculture (agrovoc) we find that  wikipedia contains a substantial proportion of its  domain specific concepts and semantic relations   furthermore it has impressive coverage of a collection of  contemporary documents in the domain. we present a case study that uses agrovoc, a manually created professional thesaurus in the domain of agriculture, as the gold standard. we compare wikipedia articles and links to the terms and semantic relations encoded in agrovoc. we also analyze its coverage of terms that occur in a sample document collection in the domain, and compare this with agrovoc s coverage. sections # and # describe our experiments and the results we obtained  section # compares the two thesaurus structures and section # evaluates their coverage of a domain specific document corpus. finally we debate the advantages and dangers of mining folksonomies, and discuss the tremendous possibilities they open up. this paper has shown how to construct domain  and corpus specific thesauri from the collaborative encyclopedia wikipedia. surprisingly, we have found that wikipedia outperforms a professional thesaurus in supporting a domain specific document collection. wikipedia, with its interwoven tapestry of articles in many languages, is a huge mine of valuable information about words and concepts.
overcoming the brittleness bottleneck using wikipedia  enhancing text categorization with encyclopedic knowledge. analyzing and accessing wikipedia as a lexical semantic resource in this paper, we analyze wikipedia as an emerging lexical semantic resource that is growing exponentially. we extend this work by focusing on the analysis of wikipedia content, in particular its category structure, and present a highly ecient java based api to use wikipedia in large scale nlp. at rst, we compare wikipedia with conventional lexical semantic resources such as dictionaries, thesauri, semantic wordnets or paper bound encyclopedias. we show that dierent parts of wikipedia reect dierent aspects of these resources. our analysis reveals that wikipedia additionally contains a vast amount of knowledge about named entities, domain specic terms or specic word senses that cannot be easily found in other freely available lexical semantic resources. we also show that the redirect system of wikipedia can be used as a dictionary for synonyms, spelling variations and abbreviations. next, we perform a detailed analysis of the category graph of wikipedia employing graph theoretic methods. we analyze the category graph, as it can be regarded as an important lexical semantic resource of its own. from this analysis, we draw some conclusions for adapting algorithms from semantic wordnets to the wikipedia category graph.
utilizing wikipedia categories for document classi cation. this paper introduces our technique for integrating  wikipedia as a broad coverage knowledge base for   use in document classi cation. we outline an algorithm for integrating the wikipedia categories found   from named entities in the articles. we then demonstrate this algorithm on a toy corpus, where we are  able to successfully classify our documents. going back to the copper example, if the  article mentions bhp billiton and european minerals (both mining companies) several times, then we should have greater evidence that this is an article  about mining, rather than about a particular company due to the overlap in concepts among the entities mentioned. however, if we want to make an even  ner distinction, say among closely related concepts (whose surface words will  overlap), then more information is needed. the development of wikipedia has, however, given us a cheap,  broad scoped resource to capture some of the concepts associated with these entities. in this paper, we propose to use wikipedia to extract the hidden  information in entities for more accurate classi cation than would be able if we used surface features alone. the organization of our paper is as follows  in section #, we discuss similar attempts to use wikipedia as a knowledge source. we outline our algorithm in section # before introducing our data set and evaluating our algorithm s performance in section #. we conclude in section # and discuss future work in this area. in this paper, we have introduced a method for document categorization using wikipedia as a largescale knowledge base for information about named entities. we have also demonstrated this algorithm on a toy dataset, where it successfully performs the expected categorization. our  rst step is to rebuild the wikipedia database. while mediawiki was able to successfully populate our mysql database, the provided database new england browns green bay packers packers bengals patriots romeo crennel green bay green bay marvin lewis bill belichick charlie frye matt hasselbeck forrest gregg cleveland tom brady terry pluto brett favre milwaukee chris perry chicago georgia dome seattle william perry browns structure is not optimal for our needs. article links,  wikipedia categories and other pertinent information should be pre computed and available within our sql queries. in our current system, we discard entities that need disambiguation. additionally, we wish to run our algorithm on additional data sources. our toy corpus was used to demonstrate the initial concept, but we need to see how this might work in a more general classi cation context. to this end, we have obtained the rcv# corpus, and will be running experiments to see how our algorithm works on this corpus. finally, we should investigate additional weighting algorithms. then, we should be able to use the link structure to  nd a the relatedness of different articles, based on what we have seen before. also, we would like to implement the gabrilovich and markovitch (####) algorithm, which would provide us with the closest comparison to state of the art in this  eld.
computing semantic relatedness using wikipedia based explicit semantic analysis. we propose explicit semantic analysis   (esa), a novel method that represents the meaning of texts in a high dimensional space of concepts  derived from wikipedia. we use machine learning  techniques to explicitly represent the meaning of  any text as a weighted vector of wikipedia based  concepts. we proposed a novel approach to computing semantic relatedness of natural language texts with the aid of very large scale knowledge repositories. we use wikipedia and the odp, the largest knowledge repositories of their kind, which contain hundreds of thousands of human de ned concepts and provide a cornucopia of information about each concept. our approach is called explicit semantic analysis, since it uses concepts explicitly de ned and described by humans. compared to lsa, which only uses statistical cooccurrence information, our methodology explicitly uses the knowledge collected and organized by humans. compared to  lexical resources such as wordnet, our methodology leverages knowledge bases that are orders of magnitude larger and more comprehensive.
placing search in context  the concept revisited. in this work we present experiments on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets. existing relatedness measures perform better using wikipedia than a baseline given by google counts, and we show that wikipedia outperforms wordnet when applied to the largest available dataset designed for that purpose. we also show that including wikipedia improves the performance of an nlp application processing naturally occurring texts.
a proposal to automatically build and maintain gazetteers for named entity recognition by using wikipedia. this paper describes a method to automatically create and maintain gazetteers for  named entity recognition (ner). our approach is   based on the analysis of an on line encyclopedia entries by using a noun hierarchy   and optionally a pos tagger. gazetteers could be automatically created and maintained by  extracting the necessary information from available linguistic resources, which we think is a promising line of future research. we agree with (magnini et al., ####) that in order to automatically create and maintain trigger gazetteers, using a hierarchy of common nouns is a good approach. therefore, we want to focus on  the automatically creation and maintenance of entity gazetteers. for example, if we refer to presidents, the trigger word used might be  president  and it is uncommon that the trigger used to refer to them changes over time. our aim is to  nd a method which allow us to automatically create and maintain entity gazetteers  by extracting the necessary information from linguistic resources. an important restriction though, is that we want our method to be as independent of language as possible. the rest of this paper is structured as follows. in the next section we discuss about our proposal. section three presents the results we have obtained  and some comments about them. finally, in section four we outline our conclusions and future work. we have presented a method to automatically create and maintain entity gazetteers using as resources an encyclopedia, a noun hierarchy and, optionally, a pos tagger. in our opinion, the principal drawback of our  system is that it has a low precision for the con guration for which it obtains an acceptable value of recall. on the positive side, we can conclude that our  method is helpful as it takes less time to automatically create gazetteers with our method and after  that to supervise them than to create that dictionaries from scratch. another important fact is that the method has  a high degree of language independence  in order to apply this approach to a new language, we need a version of wikipedia and wordnet for that language, but the algorithm and the process does not change. therefore, we think that our method  can be useful for the creation of gazetteers for lanentity type number of instances percentage none #### loc ### ## org ## # per ### ## k loc org per prec rec f   # prec rec f   # prec rec f   # # ##.## ##.## ##.## ##.## ##.## ##.## ##.## ##.## ##.## # ##.## ##.## ##.## ##.## #.## #.## ##.## ##.## ##.## k loc org per prec rec f prec rec f   # prec rec f   # # ##.## ##.## ##.## ##.## ##.## ##.## ##.## ##.## ##.## # ##.## ##.## ##.## ##.## ##.## ##.## ##.## ##.## ##.## guages in which ner gazetteers are not available but have wikipedia and wordnet resources. regarding the task we have developed, we consider to carry out new experiments incorporating features that wikipedia provides such as links between pairs of entries. following with this, we consider to test more complex weighting techniques for our algorithm. besides, we think that the resulting gazetteers for the con gurations that provide high precision and low recall, although not being appropriate for  building gazetteers for ner systems, can be interesting for other tasks. as an example, we consider to use them to extract verb frequencies for the entity categories considered which can be later used as features for a learning based named entity recogniser.
large scale named entity disambiguation based on wikipedia data. this paper presents a large scale system for the  recognition and semantic disambiguation of  named entities based on information extracted  from a large encyclopedic collection and web   search results. our aim has been to build a named entity recognition and disambiguation system that employs a comprehensive list of entities and a vast amount of world knowledge. thus, we turned our attention to the wikipedia collection, the largest organized knowledge repository on the web (remy, ####). they employed several of the disambiguation resources discussed in this paper (wikipedia entity pages, redirection pages,  categories, and hyperlinks) and built a contextarticle cosine similarity model and an svm based on a taxonomy kernel. the system discussed in this paper performs both named entity identification and disambiguation. however, while nominator made heavy use of heuristics and lexical clues  to solve the structural ambiguity of entity mentions, we employ statistics extracted from wikipedia and web search results. we augment the wikipedia category information with information automatically extracted from wikipedia list pages and use it in conjunction with the context information in a vectorial model that employs a novel disambiguation method. we presented a large scale named entity disambiguation system that employs a huge amount of  information automatically extracted from wikipedia over a space of more than #.# million entities. the system described in this paper has been fully implemented as a web browser (figure #), which can analyze any web page or client text document. the application on a large scale of such an entity extraction and disambiguation system could result in a move from the current space of words to a space of concepts, which enables several paradigm shifts and opens new research directions, which we  are currently investigating, from entity based indexing and searching of document collections to  personalized views of the web through entitybased user bookmarks.
exploiting wikipedia as external knowledge for named entity recognition. we explore the use of wikipedia as external   knowledge to improve named entity recognition (ner). our method retrieves the corresponding wikipedia entry for each candidate word sequence and extracts a category label from the  rst sentence of the entry, which can be thought of as a de nition   part. we demonstrate using the conll #### dataset that the  wikipedia category labels extracted by such   a simple method actually improve the accuracy of ner. we have recently seen a rapid and successful growth of wikipedia (http   www.wikipedia.org), which is an open, collaborative encyclopedia on the web. we think that extracting knowledge from wikipedia for natural language processing is one of the promising ways towards enabling large scale, real life applications. as a  rst step towards such approach, we demonstrate in this paper that category labels extracted from the  rst sentence of a wikipedia article, which  can be thought of as the de nition of the entity described in the article, are really useful to improve the accuracy of ner. we extract  politician  from this sentence as the category label for  franz fischler . we use  such category labels as well as matching information as features of a crf based ne tagger. in our experiments using the conll #### ner dataset (tjong et al., ####), we demonstrate that we can  improve performance by using the wikipedia features by #.## points in f measure from the baseline, and by #.## points from the model that only uses the gazetteers provided in the conll #### dataset. our  nal model incorporating all features achieved  ##.## in f measure, which means a #.## point improvement over the baseline, which does not use any gazetteer type feature. the studies most relevant to ours are bunescu and pas ca (####) and toral and mu   noz (####). the difference however is that our method tries to use wikipedia features for ner, not for disambiguation which assumes that entity regions are already found. also, our method does not disambiguate ambiguous entities, since accurate disambiguation is dif cult and possibly introduces noise. we only focused on the second case and did not utilize disambiguation pages in this study. we focus on the  rst noun phrase after be in the  rst sentence, while they used all the nouns in the sentence. we on the other hand use the obtained category labels directly as features, since we think the mapping performed automatically by a crf model is more precise than the mapping by heuristic methods. we  rst explain the structure of wikipedia in section #. next, we introduce our method of extracting and using category labels in section #. we then show the experimental results on the conll #### ner  dataset in section #. finally, we discuss the possibility of further improvement and future work in section #. we tried to exploit wikipedia as external knowledge to improve ner. we extracted a category label from the  rst sentence of a wikipedia article and used it  as a feature of a crf based ne tagger. however, disambiguation techniques will become more important as wikipedia grows or if we aim at more  negrained ner. we thus would like to incorporate a disambiguation technique into our method in future  work.
using encyclopedic knowledge for named entity disambiguation. we present anew method for detecting and   disambiguating named entities in open domain text. #.# approach the main goal of the research reported in this paper is to develop a named entity disambiguation method that is intrinsically linked to a dictionary  mapping proper names to their possible named entitiy denotations. we show that the structure of wikipedia lends itself to a set of  useful features for the detection and disambiguation of named entities. we conclude with future work and conclusions. we have presented a novel approach to named entity detection and disambiguation that exploited  the untapped potential of an online encyclopedia.
on the uni cation of syntactic annotations under the stanford dependency scheme  a case study on bioinfer and genia. in this   paper, we present a step towards such uni cation by creating a conversion from the   link grammar to the stanford scheme. further, we create a version of the bioinfer corpus with syntactic annotation in this scheme. we present an application oriented evaluation of the transformation and assess the  suitability of the scheme and our conversion  to the uni cationof the syntactic annotations  of bioinfer and the genia treebank. we  nd that a highly reliable conversion is   both feasible to create and practical, increasing the applicability of both the parser and  the corpus to information extraction. in this paper, we present a step towards unifying the diverse syntax schemes in use in ie systems and corpora such as the genia treebank # and the recently introduced bioinfer corpus (pyysalo et  al., ####). to assess this choice, we develop a set of conversion  rules for transforming the link grammar (lg) dependency scheme (sleator and temperley, ####) to the stanford scheme and then create a version of  the bioinfer corpus in the stanford scheme by applying the conversion rules and manually correcting the errors. by making the bioinfer corpus available in the stanford scheme, we also increase the value  of the corpus for biomedical ie. finally, to assess the practical value  of the conversion method and of the bioinfer syntactic annotation in the stanford scheme, we compare the charniak lease constituency parser # (charniak and lease, ####) and biolg, # an adaptation of lg (pyysalo et al., ####), on the newly uni ed dataset  combining the constituency annotated genia treebank with the dependency annotated bioinfer corpus. we have presented a step towards unifying syntactic annotations under the stanford dependency scheme and assessed the feasibility of this uni cation by developing and evaluating a conversion from link grammar to the stanford scheme. we  nd that a  highly reliable transformation can be created, giving a precision and recall of ##.#  and ##.# , respectively, when compared against our manually annotated gold standard version of the bioinfer corpus. we also  nd that the performance of the biolg parser is not adversely affected by the conversion. based on these results, we conclude that converting to the stanford scheme is both feasible and practical. further, we have developed a version of the bioinfer corpus annotated with the stanford scheme, thereby increasing the usability of the corpus. we applied the lg sf conversion to the original lg  bioinfer annotation and manually corrected the errors. we have also demonstrated that the uni cation permits direct parser comparison that was previously  impossible. however, we found that there is a certain accumulation of errors caused by the conversion, particularly in a case when two distinct rule sets are applied. in our case, we estimate this error  to be on the order of several percentage units, nevertheless, we were able to establish the relative performance of the parses with a strong statistical significance. we note that an authoritative de nition of the stanford scheme would further increase its value.
formalism independent parser evaluation with ccg and depbank. in this paper we evaluate a ccg parser on  depbank, and demonstrate the dif culties   in converting the parser output into depbank grammatical relations. in addition we   present a method for measuring the effectiveness of the conversion, which provides  an upper bound on parsing accuracy. we compare the ccg   parser against the rasp parser, outperforming rasp by over #  overall and on the majority of dependency types. in this paper we evaluate a ccg parser (clark  and curran, ####b) on the briscoe and carroll version of depbank (briscoe and carroll, ####). however, we found that performing such a conversion is a time consuming and non trivial task. the contributions of this paper are as follows. first, we demonstrate the considerable dif culties  associated with formalism independent parser evaluation, highlighting the problems in converting the  output of a parser from one representation to another. second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used  this method can be adapted by other researchers to strengthen their own parser comparisons. and  third, we provide the  rst evaluation of a widecoverage ccg parser outside of ccgbank, obtaining impressive results on depbank and outperforming the rasp parser (briscoe et al., ####) by over #  overall and on the majority of dependency types. a contribution of this paper has been to highlight the dif culties associated with cross formalism parser comparison. despite the dif culties, we have given the  rst evaluation of a ccg parser outside of ccgbank, outperforming the rasp parser by over #  overall and on the majority of dependency types. hence we challenge other parser developers to map their own parse output into the version of depbank used here. one aspect of parser evaluation not covered in this paper is ef ciency. using a cluster of ## machines we have also parsed the entire gigaword corpus in less than  ve days. hence, we conclude that accurate, large scale, linguistically motivated nlp is now practical with ccg.
challenges in mapping of syntactic representations for framework independent parser evaluation. we explore some of the issues and challenges created by the incompatibility of diverse representation schemes for syntactic  parsing. in particular, we examine the  problem of output format conversion for  evaluation of parsers that use different   formalisms. we discuss recent related efforts, and present an evaluation of different  parsers that use representations that vary  not only in formalisms, but also in depth of   syntactic information. we attempt to compare these parsers in a domain widely used   for parser evaluation, the wall street journal section of the penn treebank, and in  the academic biomedical literature, where  the use of parsing technologies is expected  to contribute in practical applications, such  as information extraction and text mining. in this paper we explore some of the issues and  challenges created by the incompatibility of representation schemes for syntactic parsing. we examine the problem of output conversion for evaluation of parsers that use different formalisms. we discuss recent efforts to establish common criteria  for parser evaluation, and present a case study involving parsers that use representations that vary  not only in formalisms, but also in depth of syntactic information. we attempt to compare these  parsers in a domain widely used for parser evaluation, the wall street journal section of the penn  treebank, and in the academic biomedical literature, where the use of parsing technologies is expected to contribute in practical applications, such as information extraction and text mining. we have explored the issue of evaluation across different parsing frameworks through mapping of parser output to different representations. our  evaluation using carroll et al. we have also found that converting from the phrase structure output of a deep parser to shallow ptb phrase structures can be done with relatively high accuracy. we also found that although sd may be more useful in some applications than phrase structures, its use as an evaluation metric added little information when a detailed (gr) and  a shallow evaluation (ptb) are already being performed.
international journal of medical informatics. we further present an annotation scheme that combines key annotation types with a detailed definition of entity relationships as well as present two new biomedical ontologies that define bioentity and relationship types.
speed and accuracy in shallow and deep stochastic parsing. this paper reports some experiments that compare the accuracy and performance of two   stochastic parsing systems. we measured the accuracy of both systems against a  gold standard of the parc ### dependency  bank, andalso measuredtheir processingtimes. we found the deep parsing system to be more  accurate than the collins parser with only a  slight reduction in parsing speed. this paper reports on some experiments that put this  conventional wisdom to an empirical test. we investigated the accuracy of recovering semantically relevant  grammatical dependencies from the tree structures produced by the collins parser, comparing these dependencies to gold standard dependencies which are available  for a subset of ### sentences randomly drawn from section ## of the wall street journal (see king et al. we compared the output of the xle system, a deep grammar based parsing system using the english lexical functional grammar previously constructed as part of the pargram project (butt et al., ####), to the  same gold standard. as part of our assessment, we also measured the parsing speed of the two systems, taking into account all stages of processing that each system requires to produce its output. for example, since the collins parser depends on a prior part of speech tagger (ratnaparkhi, ####), we  included the time for pos tagging in our collins measurements. for both xle and the collins parser we wrote conversion programs to transform the normal (tree or fstructure) output into the corresponding relations of the dependency bank. however, a certain amount of skill and intuition was required to provide a fair conversion of the collins trees  we did not want to penalize con gurations in the  collins trees that encoded alternative but equally legitimate representations of the same linguistic properties (e.g. whether auxiliaries are encoded as main verbs or aspect features), but we also did not want to build into the conversion program transformations that compensate  for information that collins cannot provide without appealing to additional linguistic resources (such as identifying the subjects of in nitival complements). we did not  include the time for dependency conversion in our measures of performance. we presented some experiments that compare the accuracy and performance of two stochastic parsing systems, the shallow collins parser and the deep grammar based xle system. we measured the accuracy of both systems  against a gold standard derived from the parc ### dependencybank, andalso measuredtheir processingtimes. contrary to conventional wisdom, we found that the shallow system was not substantially faster than the deep  parser operating on a core grammar, while the deep system was signi cantly more accurate. our experiment is comparable to recent work on reading off propbank style (kingsbury and palmer, ####)  predicate argument relations from gold standard treebank trees and automatic parses of the collins parser. from this perspective, the nearly  ##  f score that is achieved for our deterministic rewriting of collins  trees into dependencies is remarkable, even if the results are not directly comparable. our scores and gildea and palmer s are both substantially lower than the ##  typically cited for evaluations based on labeled  or unlabeled bracketing, suggesting that extracting semantically relevant dependencies is a more dif cult, but we think more valuable, task.
using the penn treebank to evaluate non treebank parsers. this paper describes a method for conducting evaluations of treebank and non treebank parsers alike against the english language u.  penn treebank (marcus et al., ####) using a metric that focuses on the accuracy of relatively non controversial aspects of parse  structure. our conjecture is that if we focus on maximal projections of heads (mph), we are likely to find much broader agreement  than if we try to evaluate based on order of attachment. we hope that this method may find wider acceptance and be useful in  establishing a generally applicable framework for evaluation in natural language parsing. we employ this method in an evaluation of  nlpwin (heidorn, ####), a parser developed at microsoft research without reference to the penn treebank, and, for comparison, the  well known statistical treebank parser of charniak (####). we introduce a method for  conducting evaluations of both treebank and nontreebank parsers using a metric that focuses on the accuracy of relatively non controversial aspects of parse structure. data we are using the english language university of pennsylvania treebank v.# from the ldc. included in the treebank are two datasets of interest to us  the wall street journal (wsj) and the brown corpus. to date, there is no standard division of the brown corpus treebank, so we have provided one for this evaluation. we will elaborate on the division of the brown corpus treebank after discussing the metric. maximal projections of heads faced with this lack of comparability between nontreebank parses and treebank parses, how can we proceed? our conjecture is that if we focus on maximal projections of heads (mph), we are likely to find much broader agreement than if we try to evaluate based on order of attachment or on the granularity of intermediate projections. while there are some theoretical differences that are not normalized by looking only at mph brackets (e.g., whether  small clauses  exist or not), we proceed under the assumption that if only mph brackets are evaluated, the remaining systematic differences are few enough to be dealt with on a case by case basis. in particular, to address tokenization issues, we added a post process to nlpwin to tokenize in the manner of the treebank, and we exclude part of speech tags and constituent labels from our evaluation. # to rectify this, we will employ a set of head labeling rules and compute maximal projections of heads for the reference or  gold  treebank trees. we can then have a comparable reference set containing only maximal projections against which to compute bracket precision we observe that if we leave the treebank trees untouched and recall (black, ####), as well as crossing brackets. we elaborate on this procedure in the next section. we have introduced a method for conducting evaluations of both treebank and non treebank parsers using a metric that focuses on the accuracy of relatively non controversial aspects of parse structure, namely unlabeled precision and recall of maximal projections of heads. we hope that this method may find wider acceptance and be useful in establishing a generally applicable framework for evaluation in natural language parsing.
the impact of parse quality on syntactically informed statistical machine translation. we investigate the impact of parse quality   on a syntactically informed statistical machine translation system applied to technical text. we vary parse quality by varying the amount of data used to train the  parser. with these questions and concerns, let us begin. following some background discussion we describe a set of experiments intended to elucidate the impact of parse quality on smt. we return now to the questions and concerns raised in the introduction. we have shown that such a system is sensitive to the quality of error category regress improve attachment of prep #  ##  root identi cation ##  ##  final punctuation ##  ##  coordination #  ##  dependent verbs ##  ##  arguments of verb #  ##  np identi cation ##  ##  dependent of prep #  #  other attachment #  ##   the input syntactic analyses. in the introduction we mentioned the concern that others have raised when we have presented  our research  syntax might contain valuable information but current parsers might not be of suf cient quality. it is certainly true that the accuracy of the best parser used here falls well short of what we might hope for. to date our research has focused on translation from english to other languages. one concern in applying the  treelet smt framework to translation from languages other than english has been the expense  of data annotation  would we require ##,### sentences annotated for syntactic dependencies, i.e., an amount comparable to the penn treebank, in  order to train a parser that was suf ciently accurate to achieve the machine translation quality that we have seen when translating from english? as more data is annotated with syntactic dependencies and more accurate parsers  are trained, we would hope to see similar improvements in machine translation output. we challenge others who are conducting research on syntactically informed smt to verify  whether or to what extent their systems are sensitive to parse quality.
towards framework independent evaluation of deep linguistic parsers. this paper describes practical issues in the framework independent evaluation of deep and shallow parsers. we focus on the use of two dependencybased syntactic representation formats in parser evaluation, namely, carroll   et al. our approach is to convert the output of parsers  into these two formats, and measure the accuracy of the resulting converted  output. through the evaluation of an hpsg parser and penn treebank phrase  structure parsers, we found that mapping between different representation   schemes is a non trivial task that results in lossy conversions that may obscure important differences between different parsing approaches. we discuss sources of disagreements in the representation of syntactic structures   in the two dependency based formats, indicating possible directions for improved framework independent parser evaluation. while the use of bracketing precision and recall in simpli ed trees from the penn treebank (marcus et al.,  ####) fueled much of the development of current wide coverage data driven parsing by providing a way to evaluate parsers on a common test set, it is now too we thank john carroll for providing the gold standard gr data, and for numerous insightful dislimited to deal with recent developments that go beyond what is represented in that test set # hence, framework independent parser evaluation is necessary not only for informed development of nlp applications, where different types of parsers may be more or less suited for certain nlp tasks (clegg and shepherd, ####), but also for progress in parsing research itself, where it would allow for a more direct comparison between different parsing approaches (clark and curran, ####). this paper discusses several challenges and practical issues in frameworkindependent evaluation of syntactic parsers. speci cally, we focus on two existing proposals for representing syntactic relationships between words, and examine practical issues through the evaluation of parsing accuracy of a deep parser  based on head driven phrase structure grammar (hpsg), enju (miyao and tsujii, ####). the  rst representation scheme we consider is grammatical relations (gr) (carroll et al., ####  carroll and briscoe, ####  briscoe, ####), which aims to provide a better parser evaluation framework than parseval measures (black et al., ####) of constituent bracketing precision and recall. however, because of its recent use in the evaluation of shallow ptb style parsers in the biomedical domain (clegg and shepherd, ####  pyysalo et al., ####a), and the availability of a conversion tool that uses shallow ptb style trees # as input, we investigate the use of sd as a scheme for framework independent parser evaluation. our basic strategy for the evaluation of enju is to establish a program for converting enju s output into these two formats, and measure accuracy of converted output. we also develop a conversion program from sd to gr, which allows for gr based evaluation of ptb style parsers (collins, ####  charniak, ####), since a conversiontool from shallow ptb style output to sd is available. we can therefore compare the performances of enju and shallow ptb parsers directly, in addition to previously reported results for rasp (briscoe and carroll, ####  briscoe et al., ####) and the c  amp c ccg parser (clark and curran, ####). in fact, however, our experiments revealed that format  conversion is not trivial. we had to implement complex mapping rules for enjuto gr sd and sd to gr conversion, and there remain a lot of disagreements for  which resolution is unlikely and which may obscure not just differences in performance among individual parsers, but also differences in the strengths of general parsing approaches. the results in this paper add to this discussion by focusing on actual challenges in format conversion, providing in depth analyses of sources of format disagreements. it is our hope that such work  will provide the direction for the development of a better scheme for frameworkindependent evaluation of deep and shallow parsers.
evaluating the accuracy of an unlexicalized statistical parser on the parc depbank. we evaluate the accuracy of an unlexicalized statistical parser, trained on #k  treebanked sentences from balanced data  and tested on the parc depbank. we   demonstrate that a parser which is competitive in accuracy (without sacri cing processing speed) can be quickly tuned without reliance on large in domain manuallyconstructed treebanks. the comparison of systems using depbank is not  straightforward, so we extend and validate   depbank and highlight a number of representation and scoring issues for relational  evaluation schemes. we evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section ## of the wsj using a relational evaluation scheme. we demonstrate that a parser which is competitive in accuracy (without sacri cing processing speed) can be quickly developed without reliance on large in domain manually constructed treebanks. we de ne a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and or bilexical relations  over tree con gurations. the parser we deploy, like the xle one, is based on a  manually de ned feature based uni cation grammar. here we compare the accuracy of our parser with kaplan et al. s results, by repeating  their experiment with our parser. we, therefore, extend depbank with a set of grammatical relations derived from our own system output and highlight how issues of representation and scoring can affect results and their interpretation. in  #, we describe our development methodology and the resulting system in greater detail. # describes the extended depbank that we have  developed and motivates our additions. #.# discusses how we trained and tuned our current system and describes our limited use of information  derived from wsj text. we have demonstrated that an unlexicalized parser with minimal manual modi cation for wsj text   but no tuning of performance to optimize on this dataset alone, and no use of ptb   can achieve  accuracy competitive with parsers employing lexicalized statistical models trained on ptb. we speculate that we achieve these results because our system is engineered to make minimal use of lexical information both in the grammar and in parse ranking, because the grammar has been developed to constrain ambiguity despite this lack  of lexical information, and because we can compute the full packed parse forest for all the test sentences ef ciently (without sacri cing speed of processing with respect to other statistical parsers). in future work, we hope to improve the accuracy of the  system by adding lexical information to the statistical parse selection component without exploiting in domain treebanks. we have also highlighted dif culties for  relational evaluation schemes and argued that presenting individual scores for (classes of) relations  and features is both more informative and facilitates system comparisons.
proceedings of the beyond parseval workshop of the third lrec conference. we present a method for evaluating their accuracy using an intermediate representation based on dependency graphs, in which the semantic relationships important in most information extraction tasks are closer to the surface. we also demonstrate how this method can be easily tailored to various application driven criteria.
uiuc  a knowledge rich approach to identifying semantic relations between nominals. this paper describes a supervised,   knowledge intensive approach to the automatic identi cation of semantic relations  between nominals in english sentences. in this paper we present a type b system that relies on various sets of new and previously used linguistic features employed in a supervised learning model. this paper describes a method for the automatic identi cation of a set of seven semantic relations  based on support vector machines (svms).
ucb  system description for semeval task  #. by combining these web features with words from the sentence context,  our team was able to achieve the best results  for systems of category c and third best for  systems of category a. semantic relation classi cation is an important but understudied language problem arising in many  nlp applications, including question answering, information retrieval, machine translation, word sense disambiguation, information extraction, etc. in the present paper we describe the ucb system which took part in that competition.
uc#m  classification of semantic relations between nominals using sequential minimal optimization. this paper presents a method for automatic classification of semantic relations  between nominals using sequential  minimal optimization. we participated  in the four categories of semeval task   # (a  no query, no wordnet  b  wordnet, no query  c  query, no wordnet    d  wordnet and query) and for all training datasets. in the present work and for the purpose of the semeval task #, our scope is limited to the semantic relationships between nominals. by this definition, we understand it is the process of discovering the underlying relations between two concepts expressed by two nominals. section # is dedicated to the description of the set of features applied in our experiments. in section #, we discuss the experiment s results compared to the baselines of the semeval task and the top scores. finally, we summarize our approach, pointing out conclusions and future directions of our work. in our first approach to automatic classification of semantic relations between nominals and as expected from the training phase, our system achieved its best performance using wordnet information. in general, we obtained better scores in category # (size of training  # to ###), i.e., when all the training examples are used. on the other hand, overfitting the training data (most probably due to the small size of training dataset) is the main reason behind the low scores obtained by our system. these facts lead us to the conclusion that semantic features from wordnet, in general, play a key role in the classification task. thus, we consider that a level # wordnet  vector is rather abstract to represent each nominal. furthermore, we consider introducing a word sense disambiguation module to obtain the corresponding synsets of the nominals.
melb kb  nominal classi cation as noun compound interpretation. in this paper, we outline our approach to  interpreting semantic relations in nominal   pairs in semeval #### task  #  classi cation of semantic relations between nominals. we build on two baseline approaches   to interpreting noun compounds  sense collocation, and constituent similarity. our two systems attained an  average f score over the test data of ##.#   and ##.# , respectively. this paper describes two systems entered in semeval #### task  #  classi cation of semantic relations between nominals. a key contribution of this research is that we examine the compatibility of noun compound (nc) interpretation methods over the extended task of nominal classi cation, to gain empirical insight into the relative complexity of the two tasks. the proposed task is a generalisation of the more conventional task of interpreting noun compounds (ncs), in which we take a nc such as cookie jar and interpret it according to a pre de ned inventory of semantic relations (levi, ####  vanderwende, ####   barker and szpakowicz, ####). our approach to the task was to  (#) naively treat  all nominal pairs as ncs (e.g. that is, we take all positive training instances for each sr and pool them together into a single training dataset. for each test instance, we make a prediction according to one of the seven relations in the task, which we then map onto a binary classi cation for  nal evaluation purposes. we make three (deliberately naive) assumptions in our approach to the nominal interpretation task. first, we assume that all the positive training inlearning task, this makes the task considerably more  dif cult, as the performance for the standard base  lines drops considerably from that for the binary tasks. second, we assume that each nominal pair maps onto a nc.this is clearly a misconstrual of the task, and intended to empirically validate whether  such an approach is viable. in line with this assumption, we will refer to nominal pairs as ncs for the remainder of the paper. third and  nally, we assume  that the sr annotation of each training and test instance is insensitive to the original context, and use only the constituent words in the nc to make our  prediction. this is for direct comparability with earlier research, and we acknowledge that the context (and word sense) is a strong determinant of the sr in practice. our aim in this paper is to demonstrate the effectiveness of general purpose sr interpretation over the nominal classi cation task, and establish a new baseline for the task. the remainder of this paper is structured as follows. we present our methods in section # and depict the system architectures in section #. we then  describe and discuss the performance of our methods in section # and conclude the paper in section #. in this paper, we presented two systems entered in  the semeval #### classi cation of semantic relations between nominals task. our results compare favourably with the established baselines, and demonstrate that nc interpretation methods are compatible with the more general task of nominal classi cation.
ucd fc  deducing semantic relations using wordnet senses that occur frequently in a database of noun noun compounds. this paper describes a system for classifying semantic relations among nominals, as  in semeval task #. this paper describes a system for deducing the  correct semantic relation between a pair of nominals in a sentence, as in semeval task # (girju, hearst, nakov, nastase, szpakowicz, turney,   amp  yuret, ####). this paper has described a system for automatically seslecting relations between nominals which uses the pro algorithm and compound corpus to form  features for pairs of nominals (consisting of candidate relations and sense pairs co occurring with those relations), and uses a naive bayes algorithm to learn to identify relations between nominals from those features.
. this paper introduces latent relational analysis   (lra), a method for measuring semantic similarity. this paper describes the  lra algorithm and experimentally compares lra   to vsm on two tasks, answering college level multiple choice word analogy questions and classifying semantic relations in noun modifier expressions. this paper has introduced a new method for calculating  relational similarity, latent relational analysis. just as attributional similarity measures have proven to  have many practical uses, we expect that relational similarity measures will soon become widely used. lra may be a step towards the black box that we imagined in section #, with many potential applications in text processing. in future work, we plan to investigate some potential applications for lra.
classifying the semantic relations in noun compounds via a domain speci c lexical hierarchy. we are developing corpus based techniques for identifying semantic relations at an intermediate level of  description (more speci c than those used in case   frames, but more general than those used in traditional knowledge representation systems). in this   paper we describe a classi cation algorithm for identifying relationships between two word noun compounds. we  nd that a very simple approach using  a machine learning algorithm and a domain speci c   lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on  the words themselves. we are exploring empirical methods of determining semantic relationships between constituents in natural language. our current project focuses on biomedical text, both because it poses interesting challenges, and because it should be possible to make  inferences about propositions that hold between scienti c concepts within biomedical texts (swanson and smalheiser, ####). and before we tackle the prepositional phrase attachment problem, we must  nd  a way to analyze the meanings of the noun compounds. our goal is to extract propositional information  from text, and as a step towards this goal, we classify constituents according to which semantic relationships hold between them. for example, we want  to characterize the treatment for disease relationship between the words of migraine treatment versus the method of treatment relationship between the words of aerosol treatment. note that because we are concerned with the semantic relations that hold between the concepts, as  opposed to the more standard, syntax driven computational goal of determining left versus right association, this has the fortuitous effect of changing the problem into one of classi cation, amenable to standard machine learning classi cation techniques. we have found that we can use such algorithms to  classify relationships between two word noun compounds with a surprising degree of accuracy. by taking advantage of lexical ontologies, we achieve strong results on noun compounds for which neither word is present in the training set. thus, we think this is a  promising approach for a variety of semantic labeling tasks. the reminder of this paper is organized as follows  section # describes related work, section # describes the semantic relations and how they were chosen,  and section # describes the data collection and ontologies. in section # we describe the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method. we have presented a simple approach to corpusbased assignment of semantic relations for noun  compounds. in this task of multi class classi cation (with ## classes) we achieved an accuracy of about ## . we have shown that a class based representation performes as well as a lexical based model despite  the reduction of raw information content and despite a somewhat errorful mapping from terms to concepts. we have also shown that representing the  nouns of the compound by a very general representation (model #) achieves a reasonable performance  of aout ##  accuracy on average. our results seem to indicate that we do not lose much in terms of accuracy using the more compact mesh representation. we have also shown how mesh besed models out  perform a lexical based approach when the number of training points is small and when the test set consists of words unseen in the training data. our approach handles  mixed class  relations naturally. our results also indicate that the second noun (the head) is more important in determining the relationships than the  rst one. in future we plan to train the algorithm to allow different levels for each noun in the compound. we  also plan to compare the results to the tree cut algorithm reported in (li and abe, ####), which allows different levels to be identi ed for different subtrees. we also plan to tackle the problem of noun compounds containing more than two terms.
semantic relation classi cation using physical sizes. this paper proposes a novel way to  accomplish the task  a system that captures  a physical size of an entity. experimental  results revealed that our proposed method is  feasible and prevents the problems inherent  in other methods. within this context, our goal was to utilize information speci c to an entity. although entities contain many types of information, we focused on the physical size of an entity. for example, we consider book to have a physical size of ## ## cm, and book to have a size of ## ## m, etc. we chose to use physical size for the following reasons  #. our next problem was how to determine physical sizes. first, we used google to conduct web searches using queries such as  book ( cm x cm)  and  library ( m x m) . next, we extracted numeric  expressions from the search results and used the average value as the physical size. experimental results revealed that our proposed  approach is feasible and prevents the problems inherent in other methods. we brie y presented a method for obtaining the size of an entity and proposed a method for classifying semantic relations using entity size. if we are able to estimate enrel# recall #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) (instrument agency) f   # #.## #.## #.## #.## rel# recall #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) (product producer) f   # #.## #.## #.## #.## rel# recall #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) (origin entity) f   # #.## #.## #.## #.## rel# recall #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) (theme tool) f   # #.## #.## #.## #.## rel# recall #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) (part whole) f   # #.## #.## #.## #.## rel# recall #.## (## ##) #.## ( ## ##) #.## ( ## ##) #.## ( ## ##) (content container) f   # #.## #.## #.## #.## tity sizes more precisely in the future, the system will become much more accurate.
ilk  machine learning of semantic relations with shallow features and almost no data. this paper summarizes our approach to the   semeval #### shared task on  classi cation of semantic relations between nominals . our overall strategy is to develop  machine learning classi ers making use of   a few easily computable and effective features, selected independently for each classi er in wrapper experiments. we train two   types of classi ers for each of the seven relations  with and without any wordnet information. we interpret the task of determining semantic relations between nominals as a classi cation problem that can be solved, per relation, by machine learning algorithms. we aim at using straightforward features that are easy to compute and relevant to preferably all of the seven relations central to the task. the starting conditions of the task provide us with a very small amount of training data, which further stresses the need for robust, generalizable features,  that generalize beyond surface words. we therefore hypothesize that generic information on the lexical semantics of the entities involved in the relation is crucial. we developed two systems, based on two sources of semantic information. we also entered a second system, which did not rely on wn but instead made use of automatically generated semantic clusters (decadt and daelemans, ####) to model the semantic classes of the entities. for both systems we trained seven binary classi ers  one for each relation. from a pool of easily computable features, we selected feature subsets  for each classi er in a number of wrapper experiments, i.e. along with feature subsets we also chose the machine learning method independently for each classi er. we have shownthat a machine learning approach using shallow and easily computable features performs  quite well on this task. however we end by noting that the amount of training and test data in this shared task should be  considered too small to base any reliable conclusions on.
models for the semantic classi cation of noun phrases. this paper presents an approach for detecting  semantic relations in noun phrases.
placing search in context  the concept revisited. in this work we present experiments on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets. existing relatedness measures perform better using wikipedia than a baseline given by google counts, and we show that wikipedia outperforms wordnet when applied to the largest available dataset designed for that purpose. we also show that including wikipedia improves the performance of an nlp application processing naturally occurring texts.
towards terascale knowledge acquisition. in this paper, we study the  challenges of working at the terascale. we present  an algorithm, designed for the terascale, for mining  is a relations that achieves similar performance to a   state of the art linguistically rich method. we focus on the accuracy of these two systems as a function of processing time and corpus size. in the last decade, we have seen an explosion in the amount of available digital text resources. in this paper, we make a step towards acquiring semantic knowledge from terabytes of data. we present an algorithm for extracting is a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (pantel and ravichandran ####). we show that by simply utilizing more data on this task, we  can achieve similar performance to a linguisticallyrich approach. instead of  using a syntactically motivated co occurrence approach as above, our system uses lexico syntactic rules. many algorithms have recently  been proposed to automatically mine is a (hyponym hypernym) relations between words. here, we focus on is a relations that are characterized by the questions  what who is x? for example, table # shows a sample of ## is a relations discovered by the algorithms presented in this paper. in this table,  we call azalea, tiramisu, and winona ryder instances of the respective concepts flower, dessert and actress. the main contribution of this paper is a comparison of the quality of our pattern based and co  occurrence models as a function of processing time and corpus size. we will show that, for very small or very large corpora or for situations where recall is valued over precision, the pattern based approach is best. in this light, we see an interesting need to develop fast, robust, and scalable methods to mine semantic information from the web. this paper compares and contrasts two methods for extracting is a relations from corpora. we presented a novel pattern based algorithm, scalable to the terascale, which outperforms its more informed syntactical co occurrence counterpart on very small and very large data. our biggest challenge as we venture to the terascale is to use our new found  wealth not only to build better systems, but to improve our understanding of language.
automatic discovery of part whole relations. ### ion # abstraction # no glory # past #  # linear  measure # measure # yes ## # centimeter # decimeter #  abstraction # entity # no age # earth #  # shape # artifact # yes point # knife #  #.# shape # structure # no ##.## ## diameter # plug #  #.# shape # surface # no ##.## ## square # pegboard #  abstraction #group # no history # regiment #  # abstraction # biological  group # yes ##.## ## year # montia #  ## relation # arrangement # yes ##.## # medium  frequency #  electromagnetic spectrum # abstraction # phenomenon # no cause # death #  ## shape # physical  phenomenon # yes dewdrop # dew #  abstraction # psychological  feature # no amount # work #  ## measure # structure # yes ##.## ## august # gregorian  calendar # entity # phenomenon # no keeper #  ame #  ## point # physical  phenomenon # yes storm center # storm #  ## object # process # yes ferric  oxide # rust # phenomenon # entity # no  ## process # organism # yes meiosis # anapsid #  ### ion # abstraction # no art # advertising #  #time period # time period # yes afternoon # wednesday #  # message # written  communication # yes index # back matter #  # written  communication # message # yes zip code # address #  abstraction # entity # no address # restaurant #  # communication # musical  composition # yes ## # lyric # ballad #  # written  communication # creation # yes ## # zip code # address #  abstraction # psychological  feature # no theorem # decomposition #  # attribute # information # yes ## # head ## abscess #  act #group # no consolidation # school #  # act # people # yes president # class #  entity # abstraction # no book # recipe #  # surface # communication # yes ##.## ## head ## coin #  #.# horizontal  surface # no dais # medal #  entity # entity # no advocate # child #  # object # body  of water # yes water # pond #  ## covering # instrumentality # yes ##.## ## roof  car #  ## way # structure # yes ##.## ## stairway # building #  ## opening ## artifact # yes ##.## ## window # bus #  ## covering # structure # yes ##.## ## roof # building #  ## artifact # covering # yes ##.## ## top ## roof #  ## instrumentality # covering # yes lock # lid #  ## instrumentality # instrumentality # yes accelerator # car #  ##.# conveyance # instrumentality # no ##.## ## alarm # seismograph #  ##.# furnishings # instrumentality # no ##.## ## stand # magazine #  ##.# means # instrumentality # no ##.## ## magazine # telescope #  ### ion # abstraction # no force # past #  # statement # speech # yes announcement # news  conference # # signal # message # yes letter # alphabet #  # writing # writing # yes addendum # back  matter # # linear  measure # measure # yes ## # inch # foot #  entity # entity # no child # husband #  # artifact # artifact # yes door # room #  #.# creation # artifact # no ##.## ## book # audiocassette #  #.# artifact # creation # no ##.## ## charcoal # watercolor #  #.# fabric # artifact # no ##.## ## knit # tie #  #.# artifact #  oat # no ##.## ## outboard  motor # raft # #.# artifact # excavation # no ##.## ## adit # mine #  #.# artifact # surface # no ##.## ## cargo  container # main deck #  #.# line ## artifact # no ##.## ## rope # walkway #  #.# way # way # no ##.## ## path # door #  #.# container # furnishings # no ##.## ## bottle # wardrobe #  # natural  object # natural object # yes pistil #  ower #  # region # object # yes ##.## ## foot # shoe #  # region # artifact # yes ## # seat # hall #  # part # object # yes ## # auto  accessory # car # entity #group # no weapon # troop #  ## causal  agent # social group # yes member # association #  group #group # no delegation # washington #  ## people # social  group # yes youth # high school #  default no automobile # garage #  ### ion # abstraction # no glory # past #  # linear  measure # measure # yes ## # centimeter # decimeter #  # communication # communication # yes act # play #  #.# written  communication # written communication # no text # act #  #.#.# writing # writing # yes new  te st am en t # bible #  #.#.#.# matter # no ##.## # text # act #  #.# indication # message # no ##.## ## copy # recommendation #  #.# message # communication # no ##.## # irony # play #  #.# time # abstraction # yes ##.## # carboniferous # paleozoic #  abstraction # entity # no age # earth #  # shape # artifact # yes point # knife #  #.# shape # structure # no ##.## ## diameter # plug #  #.# shape # surface # no ##.## ## square # pegboard #  # measure # object # yes drumstick # bird #  #.# de nite  quantity # artifact # no dozen # videotape #  #.# inde nite  quantity # artifact # no lot # throttle #  #.# linear  measure # artifact # no mile # quarters #  #.# system  of measurement # object # no ##.## # bandwidth # receiver #  #.# relative  quantity # object # no ##.## # nothing # refrigerator #  # position # artifact # yes circle # theater #  #.# placement # no ##.## ## density # pattern #  # written  communication # instrumentality # yes ##.## ## by line # writing arm #  # shape # location # yes ##.## ## point # arrowhead #  abstraction #group # no history # regiment #  ### ion # phenomenon # no cause # death #  ## shape # physical  phenomenon # yes dewdrop # dew #  abstraction # psychological  feature # no amount # work #  ## measure # structure # yes ##.## ## august # gregorian  calendar # entity # phenomenon # no keeper #  ame #  ## point # physical  phenomenon # yes storm center # storm #  ## object # process # yes ferric  oxide # rust # event # entity # no  ## periodic  event # object # yes ##.## ## wave # waveguide #  event #event # no rerun # television  show # ## happening # periodic  event # yes ##.## #  ood #  ood tide #  phenomenon # abstraction # no omission # pronoun #  ## atmospheric  phenomenon # communication # yes gentle breeze #   beaufort scale # phenomenon # entity # no  ## process # organism # yes meiosis # anapsid #  ##.# process # person # no ##.## # growth # child #  phenomenon # phenomenon # no in uence # action #  ## natural  phenomenon # natural phenomenon # yes meteor # meteor shower #  possession # entity # no cost # home #  ## territory # entity # yes ##.## # united  states virgin islands #   virgin islands # possession #possession # no liquid  assets # capital # ## assets # transferred  property # yes cut # loot #  psychological feature # psychological feature # no  ## knowledge  domain # knowledge domain # yes agrology # agronomy #  ### np have np clauses, (#) noun compounds, and np pp phrases.
semantic taxonomy induction from heterogenous evidence. we propose a novel algorithm for inducing semantic taxonomies. by contrast, our algorithm    exibly incorporates evidence from multiple classi ers over heterogenous relationships to optimize   the entire structure of the taxonomy, using knowledge of a word s coordinate terms to help in determining its hypernyms, and vice versa. we apply our  algorithm on the problem of sense disambiguated  noun hyponym acquisition, where we combine the   predictions of hypernym and coordinate term classi ers with the knowledge in a preexisting semantic taxonomy (wordnet #.#). we add ##, ### novel   synsets to wordnet #.# at ##  precision, a relative error reduction of ##  over a non joint algorithm using the same component classi ers. finally, we show that a taxonomy built using our algorithm shows a ##  relative f score improvement   over wordnet #.# on an independent testset of hypernym pairs. our approach simultaneously provides a solution to the problems of jointly considering evidence about multiple relationships as well as lexical ambiguity within a single probabilistic framework. # we have presented an algorithm for inducing semantic taxonomies which attempts to globally optimize the entire structure of the taxonomy. our probabilistic architecture also includes a new model for learning coordinate terms based on (m, n) cousin classi cation.
automatic acquisition of hyponyms  om large text corpora. we describe a method for the automatic acquisition  of the hyponymy lexical relation from unrestricted   text. we identify   a set of lexico syntactic patterns that are easily recognizable, that occur iyequently and across text genre  boundaries, and that indisputably indicate the lexical   relation of interest. we describe a method for discovering these patterns and suggest that other lexical  relations will also be acquirable in this way. in order to find terms and expressions that are not defined in mrds we must turn to other textual resources. for this purpose, we view a text corpus not only as a source of information, but also as a source of information about the language it is written in. instead of interpreting everything in the text in great detail, we can searcil for specific lexical  relations that are expressed in well known ways. however, the semantics of the lexico syntactic construction indicated by the pattern  (la) npo ..... h as  np#, np# .... (and ior)  np,, are such that they imply (lb) for all np,, #   lt  i  lt  n, hyponym(npi, npo) thus from sentence (si) we conclude we use the term hyponym similarly to the sense used in (miller et el. for example, in this paper are real text, taken from grolter  apos s amerwan acaderntc encyclopedia(groher tg##) (markowitz e  al. thus one could say by interpreting sentence (s#) according to (in b) we are applying pattern based relation recognition to general texts. however, we  have identified a set of lexico syntactic patterns, including the one shown in (in) above, that indicate the hyponymy relation and that satisfy the following desiderata  (i) they occur frequently and in many text genres. the technique introduced in this paper can be seen as having a similar goal but an entirely different  approach, since only one sample need be found in order to determine a salient relationship (and that sample may be infrequently occurring or nonexistent). we observe that terms that occur in a list are often related semantically, whether they occur in a hyponymy relation or not. in the next section we outline a way to discover these lexico syntactic patterns as well as illustrate those we have found. we have described a low cost approach for automatic acquisition of semantic lexical relations from uurestricted text. our approach is complementary to statistically based approaches that find semantic relations between terms, iu that ours requires a single specially expressed instance of a relation while the others require a statistically significant number of generally expressed relations. we  apos ve shown that our approach is also useful as a critiquing component for existing knowledge bases and lexicons. we plan to test the pattern discovery algorithm on more relations and on languages other than english (depending on the corpora available). we would also like to do some analysis of the noun phrases that are acquired, and to explore the effects of various kinds of modifiers on the appropriateness of the noun phrase. we plan to do this in the context of analyzing environmental impact reports.
expressing implicit semantic relations without supervision. we present an unsupervised learning algorithm that mines large text corpora for   patterns that express implicit semantic relations. for a given input word pair  yx   with some unspecified semantic  relations, the corresponding output list of  patterns m pp ,,  # in a widely cited paper, hearst (####) showed that the lexico syntactic pattern  y such as the x  can be used to mine large text corpora for word pairs yx   in which x is a hyponym (type) of y. for example, if we search in a large corpus using the pattern  y such as the x  and we find the string  bird such as the ostrich , then we can  infer that  ostrich  is a hyponym of  bird . berland and charniak (####) demonstrated that the patterns  y s x  and  x of the y  can be used to mine corpora for pairs yx   in which x is a meronym (part) of y (e.g.,  wheel of the car ). here we consider the inverse of this problem  given a word pair yx   with some unspecified  semantic relations, can we mine a large text corpus for lexico syntactic patterns that express the implicit relations between x and y ? for example, if we are given the pair ostrich bird, can we discover the pattern  y such as the x ? we are particularly interested in discovering high quality patterns that are reliable for mining further word pairs with the same semantic relations. in our experiments, we use a corpus of web pages containing about ## ###  english words (terra and clarke, ####). from co occurrences of the pair ostrich bird in this corpus, we can generate ### patterns of the form  x ... y  and ### patterns of the form  y ... x . for a given input word pair yx   with some  unspecified semantic relations, we rank the corresponding output list of patterns m pp ,, #   in order of decreasing pertinence. we define pertinence more precisely in section #. our algorithm is also applicable to  these tasks. to calculate pertinence, we must be able to measure relational similarity. our measure is based on latent relational analysis (turney, ####). given a word pair yx   , we want our algorithm to rank the corresponding list of patterns pp ,, #   according to their value for mining text, in support of semantic network construction and similar tasks. therefore  our experiments are based on two tasks that provide objective performance measures. we discuss the results in section # and conclude in section #. latent relational analysis (turney, ####) provides a way to measure the relational similarity  between two word pairs, but it gives us little insight into how the two pairs are similar. the main contribution of this paper is the idea of pertinence, which allows  us to take an opaque measure of relational similarity and use it to find patterns that express the implicit semantic relations between two words.
espresso  leveraging generic patterns for automatically harvesting semantic relations. in this paper, we present espresso, a  weakly supervised, general purpose,  and accurate algorithm for harvesting   semantic relations. we present an empirical comparison of espresso with various state of  the art systems, on different size and  genre corpora, on extracting various   general and specific relations. experimental results show that our exploitation of generic patterns substantially  increases system recall with small effect  on overall precision. with seemingly endless amounts of textual data  at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. to our knowledge, no previous harvesting algorithm addresses all these properties concurrently. in this paper, we present espresso, a generalpurpose, broad, and accurate corpus harvesting algorithm requiring minimal supervision. we propose a method to automatically detect generic  patterns and to separate their correct and incorrect instances. below is a summary of the main contributions of this paper    algorithm for exploiting generic patterns  unlike previous algorithms that require significant manual work to make use of generic patterns, we propose an unsupervised webfiltering method for using generic patterns  and   principled reliability measure  we propose a new measure of pattern and instance reliability which enables the use of generic patterns. we proposed a weakly supervised, generalpurpose, and accurate algorithm, called espresso, for harvesting binary semantic relations from raw text. we have empirically compared espresso s precision and recall with other systems on both a small domain specific textbook and on a larger  corpus of general news, and have extracted several standard and specific semantic relations  isa, part of, succession, reaction, and production. for the former, we plan to investigate  the use of wordnet to automatically learn selectional constraints on generic patterns, as proposed by (girju et al. we expect here that  negative instances will play a key role in determining the selectional restrictions. espresso is the first system, to our knowledge, to emphasize concurrently performance, minimal supervision, breadth, and generality. it remains  to be seen whether one could enrich existing ontologies with relations harvested by espresso, and it is our hope that these relations will benefit nlp applications.
open information extraction from the web. this paper introduces open ie (oie), a new extraction paradigm where the system makes a single  data drivenpass over its corpus and extracts a large  set ofrelational tuples without requiring any human  input. we report on experiments over a #,###,### web  page corpus that compare textrunner with  knowitall, a state of the art web ie system. we report statistics  on textrunner s ##,###,###highest probability  tuples, and show that they contain over #,###,###   concrete facts and over#,###,### more abstract assertions. this paper introduces open information extraction (oie)   a novel extraction paradigm that facilitates domainindependent discovery of relations extracted from text and readily scales to the diversity and size of the web corpus. below, we consider these challenges in more detail. the main contributions of this paper are to    introduce open information extraction (oie) a new extraction paradigm that obviates relation speci city by automatically discovering possible relations of interest while making only a single pass over its corpus. section # reports on our experimental results. this paper introduces open ie from the web, an unsupervised extraction paradigm that eschews relation speci c extraction in favor of a single extraction pass over the corpus  during which relations of interest are automatically discovered and ef ciently stored. we have shown that textrunner is able to match the recall of the knowitall  state of the art web ie system, while achieving higher precision. in the future, we plan to integrate scalable methods for detecting synonyms and resolving multiple mentions of entities in textrunner. finally we plan to unify tuples output by  textrunner into a graph based structure, enabling complex relational queries. we thank the following people for helpful comments on previous drafts  dan weld, eytan adar, and doug downey. the  rst author of this paper received additional support thanks to a google anita borg memorial scholarship.
clustering for unsupervised relation identification. in this  paper we compare several clustering setups, some of them novel  and others already tried. in order to do the  comparison, we develop a clustering evaluation metric,  specifically adapted for the relation identification task. our   experiments demonstrate significant superiority of the singlelinkage hierarchical clustering with the novel threshold selection  technique over the other tested clustering algorithms. we have found that  using both kinds of features with the best of the algorithms  produces very high precision results, significantly improving over  the previous work. we have presented the results of several experiments, comparing different clustering algorithms and different feature extraction and selection methods for unsupervised relation identification, using an evaluation metric adapted for the task. we also demonstrated the importance of using complex features, and relying on features that are based both on individual entities and on combination of entities within pairs. unlike most of the previous works, our system performs without a separate named entity recognition component, using only a general purpose noun phrase chunker. in the future research we plan to test the sample based feature selection, which if successful would allow us to use much bigger initial sets of features, and thus to work with bigger candidate sets. currently, very big candidate sets are inaccessible to our system, because the number of features that need to be extracted is too large. we also plan to test our clustering scheme in other unsupervised clustering settings, such as clustering of relations between individual words, for the field of unsupervised language acquisition.
support vector machines applied to the classi cation of semantic relations in nominalized noun phrases. this paper presents a method for the  automatic classi cation of semantic relations  in nominalized noun phrases. this paper discusses the automatic labeling of semantic relations in nominalized noun phrases (nps) using a support vector machines learning algorithm. based on the classi cation provided by the new webster s grammar guide (semmelmeyer and bolander ####) and our observations of noun phrase patterns on large text collections, the most frequently occurring np level constructionsare  (#) compoundnominals consist  ing of two consecutive nouns (eg pump drainage   an instrument relation), (#) adjective noun constructions where the adjectival modi er is derived from a noun (eg parental refusal   agent), (#) genitives (eg tone of conversation   a property relation), (#) adjective phrases  in which the modi er noun is expressed by a prepositional phrase which functions as an adjective (eg amusement in the park   a location relation), and (#) adjec  where the head noun is modi ed by a relative clause (eg the man who was driving the car   an agent relation between man and driving). while these systems focus on verb argument semantic  relations, called semantic roles, in this paper we investigate predicate argument semantic relations in nominalized noun phrases and present a method for their automatic detection in open text. #.# approach we approach the problem top down,namely identify and study  rst the characteristics or feature vectors of each noun phrase linguistic pattern and then develop models for their semantic classi cation. the word sense disambiguation is done manually for training and automatically for testing with a state of the art wsd module, an improved version of a system with which we have participated successfully in senseval # and which has an accuracy of ##  when disambiguating nouns in open domain.
proc. we study the performance of two representations of word meaning in learning noun modifier semantic relations. we experimented with decision trees, instance based learning and support vector machines. we report high precision, recall and f score, and small variation in performance across several ## fold cross validation runs. in this paper we consider a specific supervised learning task  assign semantic relations to noun modifier pairs in base noun phrases (base nps), composed only of a noun and its modifier. to identify such noun modifier relations we can rely only on semantic and morphological information about words themselves. for example, in the base we experiment with two methods of representing the words in a base np, to be used in ml experiments for learning semantic relations between nouns and their modifiers. in brief, we use hypernyms to describe in more and more general terms the sense of a word in a pair. we use grammatical collocations # (as opposed to proximity based co occurrences) extracted from the british national corpus, to describe each word in a pair. the data we work with consist of noun modifier pairs labeled with ## fine grained semantic relations, grouped into five relation classes. experiments presented in this paper are based on the five class coarse grained grouping.
fully unsupervised discovery of concept speci c relationships by web mining. we present a web mining method for discovering and enhancing relationships in which a  speci ed concept (word class) participates. we discover a whole range of relationships  focused on the given concept, rather than   generic known relationships as in most previous work. our method is based on clustering patterns that contain concept words and  other words related to them. we evaluate the  method on three different rich concepts and   nd that in each case the method generates a   broad variety of relationships with good precision. in this paper, we introduce a novel extension of this problem  given a particular concept (initially represented by two seed words), discover relations in which it participates, without specifying their types in advance. we will generate a concept class and a variety of natural binary relations involving that class. an advantage of our method is that it is particularly suitable for web mining, even given the restrictions on query amounts that exist in some of today s leading search engines. in the next section we will de ne more precisely the problem  we intend to solve. in section #, we will consider related work. in section # we will provide an overview of our solution and in section # we will consider the details of the method. in section # we will illustrate  and evaluate the results obtained by our method. finally, in section # we will offer some conclusions and considerations for further work.
unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions. we present a novel framework for the discovery and representation of general semantic  relationships that hold between lexical items. we propose that each such relationship can be   identi ed with a cluster of patterns that captures this relationship. we give a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges high  frequency words based patterns around randomly selected hook words. to assess the quality  of discovered relationships, we use the pattern   clusters to automatically generate sat analogy questions. we also compare to a set of   known relationships, achieving very good results in both methods. the evaluation (done  in both english and russian) substantiates the  premise that our pattern clusters indeed re ect  relationships perceived by humans. in this paper, unlike the majority of studies that  use patterns in order to  nd instances of given relationships, we use sets of patterns as the de nitions  of lexical relationships. we introduce pattern clusters, a novel framework in which each cluster corresponds to a relationship that can hold between the lexical items that  ll its patterns  slots. we present  a fully unsupervised algorithm to compute pattern clusters, not requiring any, even implicit, prespeci cation of relationship types or word pattern  seeds. our algorithm does not utilize preprocessing such as pos tagging and parsing. first, we randomly select hook words and create a context corpus (hook corpus) for each hook word. second, we de ne a meta pattern using high frequency words and punctuation. third, in each hook corpus, we use the meta pattern to discover concrete patterns and target words co appearing with the hook word. fourth, we cluster the patterns  in each corpus according to co appearance of the target words. finally, we merge clusters from different hook corpora to produce the  nal structure. we also propose a way to label each cluster by word pairs that represent it best. since we are dealing with relationships that are unspeci ed in advance, assessing the quality of the  resulting pattern clusters is non trivial. our evaluation uses two methods  sat tests, and comparison to known relationships. we used instances of  the discovered relationships to automatically generate analogy sat tests in two languages, english and russian # human subjects answered these and real sat tests. english grades were ##  for our test and ##  for the real test (##  and ##  for russian),  showing that our relationship de nitions indeed re ect human notions of relationship similarity. in addition, we show that among our pattern clusters there are clusters that cover major known noun compound and verb verb relationships. in the present paper we focus on the pattern cluster resource itself and how to evaluate its intrinsic quality. in (davidov and rappoport, ####) we show  how to use the resource for a known task of a totally different nature, classi cation of relationships  between nominals (based on annotated data), obtaining superior results over previous work. section # describes the corpora we used and the algorithm s parameters in detail. we have proposed a novel way to de ne and identify generic lexical relationships as clusters of patterns. we showed how such pattern clusters can be obtained automatically  from text corpora without any seeds and without relying on manually created databases or languagespeci c text preprocessing. in an evaluation based on an automatically created analogy sat test we showed on two languages that pairs produced by our  clusters indeed strongly re ect human notions of relation similarity. we also showed that the obtained  pattern clusters can be used to recognize new examples of the same relationships. in an additional  test where we assign labeled pairs to pattern clusters, we showed that they provide good coverage for known noun noun and verb verb relationships for both tested languages. while our algorithm shows good performance, there is still room for improvement. in this study we applied our algorithm to a generic domain, while the same method can be used for more restricted domains, potentially discovering useful domain speci c relationships.
workshop on wordnet and other lexical resources, naacl. we explore the semantic similarity between base noun phrases in clusters determined by a comprehensive set of semantic relations. we use various m+ chine learning tools to find combinations of attributes that explain the similarities in each category. introduction we consider the nature of semantic relations in base noun phrases (base nps) consisting of a head noun and one modifier (adverb, adjective, noun). the list with which we work consists of ## semantic relations, and was developed by unifying three separate lists of relations, for the syntactic levels of multi clause sentences, clauses and noun phrases ( nastase and szpakowicz, ####a ). we look at descrintions of modifiers and head nouns in lexical resources to find the attributes that make them similar with respect to our semantic relations. we explore available lexical resources ( wordnet and roget s thesaurus ) to provide the attributes, and machine learning (ml) tools to find the most salient combinations of attributes. the choice of ml tools is driven by the type of output we want  symbolic rules, easy to understand  and the type of processing of the attributes imposed by our task. in principle we look for a generalization in the ontology that underlies wordnet or roget s.
proc new zealand institute of agricultural science and the new zealand society for horticultural science annual convention, hawke s bay.
semeval #### task ##  classi cation of semantic relations between nominals. we present an   evaluation task designed to provide a framework for comparing different approaches to   classifying semantic relations between nominals in a sentence. we de ne the task, describe the training test data   and their creation, list the participating systems and discuss their results. this paper describes a new semantic evaluation task,  classi cation of semantic relations between nominals. we have accomplished our goal of providing a framework and a benchmark data set to allow for  comparisons of methods for this task. by making this collection freely accessible, we encourage further research into  this domain and integration of semantic relation algorithms in high end applications.
object recognition as machine translation  learning a lexicon for a fixed image vocabulary. we describe a model of object recognition as machine  translation. for the implementation  we describe, these words are nouns taken from a large vocabulary. we show how to cluster words that individually are di cult to predict  into clusters that can be predicted well   for example, we cannot  predict the distinction between train and locomotive using the current  set of features, but we can predict the underlying concept. (not usually addressed explicitly)  and which objects are indistinguishable using our  features? this paper describes a model of recognition that offers some purchase on each of the questions above, and demonstrates systems built with this model. however, in this form auto annotation does not tell us which image structure gave rise to which word, and so it is not really recognition. this paper shows that it is possible to learn which region gave rise to which word. we have a representation of one form (image regions  french)  and wish to turn it into another form (words  english). in particular, our models will act as lexicons, devices that predict one representation (words  english), given another representation (image regions  french). datasets consisting of annotated images are aligned bitexts   we have an image, consisting of regions, and a set of text. while we know the text goes with the image, we don t know which word goes with which region. as the rest of this paper shows, we can learn this correspondence using a variant of em. in this model, we can attack  what counts as an object? by saying that words that can be reliably attached to image regions are easy to recognise and those that cannot, are not  and which objects are indistinguishable using our features?
modeling annotated data. we consider the problem of modeling annotated data data  with multiple types where the instance of one type (such as  a caption) serves as a description of the other type (such as   an image). we describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in  correspondence latent dirichlet allocation, a latent variable  model that is effective at modeling the joint distribution of   both types and the conditional distribution of the annotation given the primary type. we conduct experiments on   the corel database of images and captions, assessing performance in terms of held out likelihood, automatic annotation, and text based image retrieval. in this paper, we consider probabilistic models for documents that consist of pairs of data streams. our focus is on personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the  rst page. in particular, the task of annotating an unannotated image can be viewed formally as a classi cation problem for each word  in the vocabulary we must make a yes no decision. with these issues in mind, we approach the annotation problem within a framework that exploits  the best of both the generative and the discriminative traditions. in this paper, we build a set of increasingly sophisticated models for a database of annotated images, culminating in correspondence latent dirichlet allocation (corr lda), a model that  nds conditional relationships between latent variable representations of sets of image regions and sets  of words. we show that, in this class of models, only corrlda succeeds in providing both an excellent  t of the joint data and an effective conditional model of the caption given  an image. we demonstrate its use in automatic image annotation, automatic region annotation, and text based image retrieval.
proc. we briefly discuss aspects of system engineering  databases, system architecture, and evaluation. we briefly discuss aspects of system engineering  databases, system architecture, and evaluation. in the concluding section, we present our view on  the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap. in the concluding section, we present our view on  the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.
sakakura   scene retrieval on an image database of full color paintings. we propose a method to make a relationship between images and words. we adopt two processes in the method, one is a process to uniformly divide each image into sub images with key words, and the other is a process to carry out vector quantization of the sub images. in this paper, a method of image to word transformation is proposed based on statistical learning from images to which words are attached. however , with the exception of a very small vocabulary, we cannot nd such learning data nor can we prepare them.
multiple bernoulli relevance models for image and video annotation. here, we  show how we can do both automatic image annotation and  retrieval (using one word queries) from images and videos   using a multiple bernoulli relevance model. we show experiments on both images from a  standard corel data set and a set of video key frames from  nist s video trec. the results also show that our model signi cantly out  performs previously reported results on the task of image  and video annotation. here, we propose approaches to automatically annotating and retrieving images videos by learning a statistical generative model called a relevance model using a set of annotated training images. we then learn a joint probability model for (continuous) image features and words called a relevance model and use this model to annotate test images which we have not  seen. we test this model using a corel dataset provided by [#] and show that it outperforms previously reported results on other models. we believe that annotation text has very different characteristics than full text in documents and hence a bernoulli distribution is more appropriate. if we want to  nd images of people, when rank ordering these images by probability the second image would be preferred to the  rst although there is no reason for preferring one image over another. it has been argued [##] that the corel dataset is much easier to annotate and retrieve and does not really capture the dif culties inherent in more challenging (real) datasets  like the news videos in trec video [##] we therefore, experimented with a subset of news videos (abc, cnn) from the  trec video dataset. we show that in fact we obtain comparable or even better performance (depending on the task) on this dataset and that again the bernoulli model outperforms a multinomial model. the focus of this paper is on models and not on features. we use features similar to those used in [#, #] the rest of this paper is organized as follows. we  rst  discuss the multiple bernoulli relevance model and its relation to the multinomial relevance model. finally, we conclude the paper. we have proposed a multiple bernoulli relevance model for  image annotation, to formulate the process of a human annotating images.
a model for learning the semantics of pictures. we propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve  images based on text queries. we do this using a formalism that models   the generation of annotated images. we assume that every image is divided into regions, each described by a continuous valued feature vector. given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the  probability of generating a word given the image regions. experiments show that our model signi cantly outperforms the  best of the previously reported results on the tasks of automatic image  annotation and retrieval. for example, one should be able to pose a query like   nd me cars on a race track . we propose a model which looks at the probability of associating words with image regions. our model computes a joint probability of image features over different regions in an image using a training set and uses this joint probability to annotate and retrieve images. more formally, we propose a statistical generative model to automatically learn the semantics of images   that is, for annotating and retrieving images based on a training set of images. we assume that an image is segmented into regions (although the regions could  simply be a partition of the image) and that features are computed over each of these regions. given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the features computed over  different regions in an image. we show that the continuous relevance model   a statistical generative model related to relevance models in information retrieval   allows us to derive  these probabilities in a natural way. our model permits us to automatically associate semantics (in terms of words) with pictures and is an important building step in performing automatic object recognition. we have proposed a new statistical generative model for learning the semantics of images. we showed that this model works signi cantly better than a number of other models for  image annotation and retrieval. our model works directly on the continuous features. we believe this is needed both for better coverage and an evaluation of how such algorithms extend to large data sets.
proc. using binary bayesian classifiers, we attempt to capture high level concepts from low level image features under the constraint that the test image does belong to one of the classes. specifically, we consider the hierarchical classification of vacation images  at the highest level, images are classified as indoor or outdoor  outdoor images are further classified as city or landscape  finally, a subset of landscape images is classified into sunset, forest, and mountain classes. we demonstrate that a small vector quantizer (whose optimal size is selected using a modified mdl criterion) can be used to model the class conditional densities of the features, required by the bayesian methodology. our system achieved a classification accuracy of ##.#  for indoor outdoor, ##.#  for city landscape, ##.#  for sunset forest   mountain, and ##  for forest mountain classification problems. we further develop a learning method to incrementally train the classifiers as additional data become available. we also show preliminary results for feature reduction using clustering techniques. our goal is to combine multiple two class classifiers into a single hierarchical classifier. publisher item identifier s #### ####(##)##### #. these databases more useful, we need to develop schemes for indexing and categorizing the humungous data.
an information theoretic de nition of similarity. we present an informationtheoretic de nition of similarity that is applicable as long as there is a probabilistic model. we  demonstrate how our de nition can be used to  measure the similarity in a number of different  domains. this paper presents a de nition of similarity that achieves two goals   universality  we de ne similarity in informationtheoretic terms. since probability theory can be integrated with many kinds of knowledge representations, such as  rst order logic [ bacchus, #### ] and semantic networks [ pearl, #### ] , our definition of similarity can be applied to many different domains where very different similarity measures had previously been proposed. the remainder of this paper is organized as follows. sections # through # demonstrate the universality of our proposal by applying it to different domains. in this paper, we present a universal de nition of similarity in terms of information theory.
workshop on wordnet and other lexical resources, naacl. we explore the semantic similarity between base noun phrases in clusters determined by a comprehensive set of semantic relations. we use various m+ chine learning tools to find combinations of attributes that explain the similarities in each category. introduction we consider the nature of semantic relations in base noun phrases (base nps) consisting of a head noun and one modifier (adverb, adjective, noun). the list with which we work consists of ## semantic relations, and was developed by unifying three separate lists of relations, for the syntactic levels of multi clause sentences, clauses and noun phrases ( nastase and szpakowicz, ####a ). we look at descrintions of modifiers and head nouns in lexical resources to find the attributes that make them similar with respect to our semantic relations. we explore available lexical resources ( wordnet and roget s thesaurus ) to provide the attributes, and machine learning (ml) tools to find the most salient combinations of attributes. the choice of ml tools is driven by the type of output we want  symbolic rules, easy to understand  and the type of processing of the attributes imposed by our task. in principle we look for a generalization in the ontology that underlies wordnet or roget s.
corpus based learning of analogies and semantic relations. we present an algorithm for learning from unlabeled text, based on the  vector s pace model (vsm) of information retrieval, that can solve verbal analogy  questions of the kind found in t he sat college entrance exam. we motivate  this research by applying it to a di cult problem in natural language processing,  determining semantic relations in noun modi er pairs. we use a supervised nearestneighbour algorithm that assigns a class to a given noun modi er pair by  nding the  most analogous noun modi er pair in the training data. in this paper, we take a different  approach, based on the idea that analogical r easoning can be approximated to some extent by a cosine measure of vector similarity, where # turney and littman the vectors are derived from statistical analysis of a large corpus of text. we d emonstr ate this approach with two real world problems, answering multiple choice verbal analogy questions and classifying nou n modi er  semantic relations. this work is only a  rst step, and analogical reasoning is still very far from being a solved problem, but we believe that our  results are encouraging. in our research, we have used multiple choice questions, developed for educational testing, as a tool for objective analysis of verb al analogies. our approach to verbal analogies is inspired by the vector space model (vsm) of information retrieval (salton and mcgill, ####  salton, ####). we us e a vector of numbers to represent the semantic relation between a pair of words. the similarity between two word pairs, a b and c d, is measured by the cosine of the angle between the vector that represents a b and the vector that represents c d. as we discuss in section #.#, the vsm was originally developed for use in information r etrieval. our approach uses learning from unlabeled text, with a very large corpus of web pages (about one hundr ed billion words)  we do not us e a lexicon or knowledge base. we present the details of our learning algorithm in section #, including an experimental evaluation of the algorithm on ### college level sat style verbal analogy questions. we also discuss how the algorithm might be extended from recognizing analogies to generating analogies. to motivate research on verbal analogies, we give an example of a practical app lication, the task of classifying the semantic relations of noun modi er pairs. in section #.#, we argue that an algorithm for classi cation of noun modi  er relations would be useful  in machine translation, information extraction, and word sense disambiguation. an algorithm for solving sat style verbal analogies can be applied to classi cation of noun modi er semantic relations, as we demonstrate in section #. given an unclassi ed noun modi er pair, we can search  through a set of labeled training data for the most analogous nounmodi er pair. we apply a supervised nearest neighbour learning algorithm, where the measure of distance (similarity) is the cosine of the vector angles. we also consider a simpler form of the data, in which the ## classes have been collapsed to # classes. we believe that analogy and metaphor play a central role in human cognition and language (lakoff and johns on, ####  hofstadter et al.,  ####  french, ####). much of our everyday language is metaphorical, so progress in this area is important for computer processing of natural language. in this paper, we have shown how the cosine metric in the vector  space model can be us ed to solve analogy questions and to classify semantic relations. however, the results indicate that these challenging tasks are tractable and we expect further improvements. we believe that the vsm can play a useful role in an ens emble of algorithms for learning analogies and semantic relations.
expressing implicit semantic relations without supervision. we present an unsupervised learning algorithm that mines large text corpora for   patterns that express implicit semantic relations. for a given input word pair  yx   with some unspecified semantic  relations, the corresponding output list of  patterns m pp ,,  # in a widely cited paper, hearst (####) showed that the lexico syntactic pattern  y such as the x  can be used to mine large text corpora for word pairs yx   in which x is a hyponym (type) of y. for example, if we search in a large corpus using the pattern  y such as the x  and we find the string  bird such as the ostrich , then we can  infer that  ostrich  is a hyponym of  bird . berland and charniak (####) demonstrated that the patterns  y s x  and  x of the y  can be used to mine corpora for pairs yx   in which x is a meronym (part) of y (e.g.,  wheel of the car ). here we consider the inverse of this problem  given a word pair yx   with some unspecified  semantic relations, can we mine a large text corpus for lexico syntactic patterns that express the implicit relations between x and y ? for example, if we are given the pair ostrich bird, can we discover the pattern  y such as the x ? we are particularly interested in discovering high quality patterns that are reliable for mining further word pairs with the same semantic relations. in our experiments, we use a corpus of web pages containing about ## ###  english words (terra and clarke, ####). from co occurrences of the pair ostrich bird in this corpus, we can generate ### patterns of the form  x ... y  and ### patterns of the form  y ... x . for a given input word pair yx   with some  unspecified semantic relations, we rank the corresponding output list of patterns m pp ,, #   in order of decreasing pertinence. we define pertinence more precisely in section #. our algorithm is also applicable to  these tasks. to calculate pertinence, we must be able to measure relational similarity. our measure is based on latent relational analysis (turney, ####). given a word pair yx   , we want our algorithm to rank the corresponding list of patterns pp ,, #   according to their value for mining text, in support of semantic network construction and similar tasks. therefore  our experiments are based on two tasks that provide objective performance measures. we discuss the results in section # and conclude in section #. latent relational analysis (turney, ####) provides a way to measure the relational similarity  between two word pairs, but it gives us little insight into how the two pairs are similar. the main contribution of this paper is the idea of pertinence, which allows  us to take an opaque measure of relational similarity and use it to find patterns that express the implicit semantic relations between two words.
proceedings of the ##th annual meeting of the association for computational linguistics.
. this paper introduces latent relational analysis   (lra), a method for measuring semantic similarity. this paper describes the  lra algorithm and experimentally compares lra   to vsm on two tasks, answering college level multiple choice word analogy questions and classifying semantic relations in noun modifier expressions. this paper has introduced a new method for calculating  relational similarity, latent relational analysis. just as attributional similarity measures have proven to  have many practical uses, we expect that relational similarity measures will soon become widely used. lra may be a step towards the black box that we imagined in section #, with many potential applications in text processing. in future work, we plan to investigate some potential applications for lra.
similari ty of semantic relations. when two words have a high degree of att ributional similarity we call them synonyms. when two pairs of words have a high degree of relational similarity we say that their relations are anal paper introduces latent relational analysis (lra) a method for measuring relational similar ity.
classifying the semantic relations in noun compounds via a domain speci c lexical hierarchy. we are developing corpus based techniques for identifying semantic relations at an intermediate level of  description (more speci c than those used in case   frames, but more general than those used in traditional knowledge representation systems). in this   paper we describe a classi cation algorithm for identifying relationships between two word noun compounds. we  nd that a very simple approach using  a machine learning algorithm and a domain speci c   lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on  the words themselves. we are exploring empirical methods of determining semantic relationships between constituents in natural language. our current project focuses on biomedical text, both because it poses interesting challenges, and because it should be possible to make  inferences about propositions that hold between scienti c concepts within biomedical texts (swanson and smalheiser, ####). and before we tackle the prepositional phrase attachment problem, we must  nd  a way to analyze the meanings of the noun compounds. our goal is to extract propositional information  from text, and as a step towards this goal, we classify constituents according to which semantic relationships hold between them. for example, we want  to characterize the treatment for disease relationship between the words of migraine treatment versus the method of treatment relationship between the words of aerosol treatment. note that because we are concerned with the semantic relations that hold between the concepts, as  opposed to the more standard, syntax driven computational goal of determining left versus right association, this has the fortuitous effect of changing the problem into one of classi cation, amenable to standard machine learning classi cation techniques. we have found that we can use such algorithms to  classify relationships between two word noun compounds with a surprising degree of accuracy. by taking advantage of lexical ontologies, we achieve strong results on noun compounds for which neither word is present in the training set. thus, we think this is a  promising approach for a variety of semantic labeling tasks. the reminder of this paper is organized as follows  section # describes related work, section # describes the semantic relations and how they were chosen,  and section # describes the data collection and ontologies. in section # we describe the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method. we have presented a simple approach to corpusbased assignment of semantic relations for noun  compounds. in this task of multi class classi cation (with ## classes) we achieved an accuracy of about ## . we have shown that a class based representation performes as well as a lexical based model despite  the reduction of raw information content and despite a somewhat errorful mapping from terms to concepts. we have also shown that representing the  nouns of the compound by a very general representation (model #) achieves a reasonable performance  of aout ##  accuracy on average. our results seem to indicate that we do not lose much in terms of accuracy using the more compact mesh representation. we have also shown how mesh besed models out  perform a lexical based approach when the number of training points is small and when the test set consists of words unseen in the training data. our approach handles  mixed class  relations naturally. our results also indicate that the second noun (the head) is more important in determining the relationships than the  rst one. in future we plan to train the algorithm to allow different levels for each noun in the compound. we  also plan to compare the results to the tree cut algorithm reported in (li and abe, ####), which allows different levels to be identi ed for different subtrees. we also plan to tackle the problem of noun compounds containing more than two terms.
the descent of hierarchy, and selection in relational semantics. we explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. in this paper we present the   results of an analysis of this method on twoword noun compounds from the biomedical domain, obtaining classi cation accuracy of approximately ## . since lexical hierarchies are  not necessarily ideally suited for this task, we   also pose the question  how far down the hierarchy must the algorithm descend before all   the terms within the subhierarchy behave uniformly with respect to the semantic relation in  question? we  nd that the topmost levels of the  hierarchy yield an accurate classi cation, thus   providing an economic way of assigning relations to noun compounds. thus we explore the use of a large corpus (medline) and a large lexical hierarchy  (mesh, medical subject headings) to determine the relations that hold between the words in noun compounds. surprisingly, we  nd that we can simply use the juxtaposition of category membership within the lexical hierarchy to determine the relation that holds between pairs of nouns. from these we can declare that the relation that holds between the words is  located in . using this technique on a subpart of the category space, we obtain ##  accuracy overall. in the following sections we discuss the linguistic motivations behind this approach, the characteristics of the lexical ontology mesh, the use of a corpus to examine  the problem space, the method of determining the relations, the accuracy of the results, and the problem of ambiguity. we have provided evidence that the upper levels of a lexical hierarchy can be used to accurately classify the relations that hold between two word technical noun compounds. in this paper we focus on biomedical terms using the biomedical lexical ontology mesh. it may be that  such technical, domain speci c terminology is better behaved than ncs drawn from more general text  we will  have to assess the technique in other domains to fully assess its applicability. first, we need to ensure that this technique works across the full spectrum of the lexical hierarchy. we have demonstrated the likely usefulness of such an exercise, but all of our analysis was done by hand. we will need to distinguish between ncs such as  acute migraine treatand oral migraine treatment , and handle the case  when the relation must  rst be found between the leftmost words. finding noun compound relations is part of our larger  effortto investigate what wecall statistical semantic parsing (as in (burton and brown, ####)  see grishman (####) for a nice overview). for example, we would like to be able to interpret titles in terms of semantic relations, for example, transforming  congenital anomalies of trainto a form that allows  questions to be answered such as  what kinds of irregularities can occur in lung structure? we hope that by  compositional application of relations to entities, such inferences will be possible. acknowledgements we thank kaichi sung for her work on the relation labeling, steve maiorano for his support of this research, and the anonymous reviewers  for their comments on the paper.
verbocean  mining the web for fine grained semantic verb relations. for the majority of our tasks, we find that simple, unsupervised models perform better when n gram counts are obtained from the web rather than from a large corpus. we argue that web based models should therefore be used as a baseline for, rather than an alternative to, standard supervised models. for each of the tasks we investigate, we also explore models that combine web counts and corpus counts. we propose two combination schemes  backoff and interpolation . method web counts following keller and lapata [####] , we obtain web counts for n grams using a simple heuristic based on queries to a web search engine.
semi automatic recognition of noun modifier relationships. this merely shifts the problem   elsewhere  how do we define the lexical semantics and build large semantic lexicons? this is the  way we chose, but it too has drawbacks. we present a semi automatic system that  identifies semantic relationships in noun   phrases without using precoded noun or adjective semantics. we have built a system for noun modifier relationship (nmr) analysis that assigns semantic relationships in complex noun phrases.
interpreting semantic relations in noun compounds via verb semantics. we propose a novel method for automatically interpreting compound nouns based  on a prede ned set of semantic relations. first we map verb tokens in sentential contexts to a  xed set of seed verbs using  wordnet  similarity and moby s  thesaurus. we then match the sentences   with semantic relations based on the semantics of the seed verbs and grammatical  roles of the head noun and modi er. based  on the semantics of the matched sentences,  we then build a classi er using timbl. the performance of our  nal system at interpreting ncs is ##.# . our method avoids any such assumptions while outperforming previous methods. the aim of this paper is to develop an automatic  method for interpreting ncs based on semantic relations. we interpret semantic relations relative to a  xed set of constructions involving the modi er and head noun and a set of seed verbs for each semantic relation  e.g. we then attempt to map all instances of the modi er and head  noun as the heads of nps in a transitive sentential context onto our set of constructions via lexical similarity over the verb, to arrive at an interpretation  e.g. we would hope to predict that possess is suf ciently similar to own that (the) family  possesses (a) car would be recognised as supporting evidence for the possessor relation. we use  a supervised classi er to combine together the evidence contributed by individual sentential contexts of a given modi er head noun combination, and arrive at a  nal interpretation for a given nc. mapping the actual verbs in sentences to appropriate seed verbs is obviously crucial to the  success of our method. this is particularly important as there is no guarantee that we will  nd large numbers of modi er head noun pairings in the sorts of sentential contexts required by our method, nor that we will  nd attested instances  based on the seed verbs. in this paper, we experiment with the use of wordnet (fellbaum, ####) and word clusters (based on moby s thesaurus) in mapping attested verbs to the seed verbs. we also make use of corelex in dealing with the semantic relation time and the  rasp parser (briscoe and carroll, ####) to determine the dependency structure of corpus data. the data source for our set of ncs is binary ncs (i.e. we deliberately choose to ignore ncs with multiple modi ers on the grounds that  (a) ##.#  of nc types in the wall street journal component of the penn treebank and ##.#  of nc types in the british national corpus are binary  and (b) we expect to be able to interpret ncs with multiple modi ers by decomposing them into binary ncs. another simplifying assumption we make is to remove ncs incorporating proper nouns since  (a) the lexical resources we employ in this research do not contain them in large numbers  and (b)  there is some doubt as to whether the set of semantic relations required to interpret ncs incorporating proper nouns is that same as that for common nouns. section # details the set of nc semantic relations that is used in our  research, section # presents an extended discussion of our approach, section # brie y explains the tools we use, section #.# describes how we gather and process the data, section #.# explains how we map the verbs to seed verbs, and section # and section # present the results and analysis of our  approach. finally we conclude our work in section #. in this paper, we proposed a method for automatically interpreting noun compounds based  on seed verbs indicative of each semantic relation. for a given modi er and head noun, our method extracted corpus instances of the two nouns in a range of constructional contexts, and then mapped the original verbs onto seed verbs based on lexical similarity derived from  wordnet  similarity, and moby s thesaurus. we also experimented with varying numbers of seed verbs, and found that generally the more seed verbs, the better the performance.
discovery of inference rules for question answering. in this paper, we present an unsupervised algorithm for  discovering inference rules from text. our algorithm is based on an extended version of   harris  distributional hypothesis, which states that words that occurred in the same contexts tend to be similar. instead of using this hypothesis on words, we apply it to paths in  the dependency trees of a parsed corpus. essentially, if two paths tend to link the same set  of words, we hypothesize that their meanings are similar. we use examples to show that  our system discovers many inference rules easily missed by humans. we call  x wrote y   x is the author of y  an inference rule. in this paper, we use the term inference rule because we also want to include relationships that are not exactly paraphrases, but are nonetheless related and are potentially useful to question answering systems. our goal is to automatically discover such rules. in this paper, we present an unsupervised algorithm, dirt, for discovering  inference rules from text. our algorithm is a generalization of previous algorithms for finding similar words (hindle, ####  pereira, ####  lin, ####). instead of applying the distributional hypothesis to words, we apply it to paths in dependency trees. essentially, if two paths tend to link the same sets of words, we hypothesize that their meanings are similar. since a path represents a binary relationship, we generate an inference rule for each pair of similar paths. the remainder of this paper is organized as follows. in the next section, we review previous work. in section #, we define paths in dependency trees and describe their extraction from a parsed corpus. a comparison of our system s output with manually generated paraphrase expressions is shown in section #. finally, we conclude with a discussion of future work. to the best of our knowledge, this is the first attempt to discover such knowledge automatically  from a large corpus of text. we introduced the extended distributional hypothesis, which states that paths in dependency trees have similar meanings if they  tend to connect similar sets of words. treating paths as binary relations, our algorithm is able to generate inference rules by searching for similar paths. our experimental results show that the extended distributional hypothesis can indeed be used to discover very useful inference rules, many of which, though easily recognizable, are difficult for humans to recall. in another work (lin and pantel, ####), we constructed semantic classes from text corpus with an unsupervised algorithm. for example, the following are two  classes generated by our program (nq#### and nq#### are automatically generated class names)  nq####  coating, resin, adhesive, sealant, plastic, material, chemical, polymer, product,   quot specialty chemical  quot , paint, varnish, packaging, lubricant, ceramic, laminate, dye, film, glue, reagent, compound, pigment, wax, epoxy, sealer, lacquer, ink, gasket, covering, insulator nq####    quot booster rocket  quot , booster, rocket,   quot rocket engine  quot , engine, vehicle, motor,   quot propulsion system  quot , tank,   quot fuel cell  quot , injector these classes may be used to extend paths with constraints on the inference rule s variables. for example, instead of generating a rule  x manufactures y   x s y factory , we may want to generate a rule with an additional clause   x manufactures y   x s y factory, where y is in nq#### or nq#### or   .
on the semantics of noun compounds. this paper provides new insights on the semantic characteristics of two and three noun compounds. an   analysis is performed using two sets of semantic classi cation categories  a list of # prepositional paraphrases previously proposed by lauer [designing statistical language learners  experimen ts on noun compounds, ph.d. thesis, macquarie university, australia] and a new set of ## semantic relations introduced  by us. we show the distribution of these semantic categories on a corpus of noun compounds and present   several models for the bracketing and the semantic classi cation of noun compounds. in this paper we describe various domain independent models that use supervised machine learning techniques and a set of linguistic features. we focus only on two and three noun compositional compounds, i.e., those whose meaning can be derived from the meaning of the constituent nouns (e.g.,   door knob  ), and tackle both the bracketing and the interpretation tasks. however, we check if the constructions are lexicalized (non compositional), ie. we present empirical observations on the distribution of a core set of semantic relations in noun compounds and provide a mapping between two sets of semantic classi cation categories. we also compare our results for bracketing and interpretation tasks against two baselines and against two state of the art interpretation systems.
using verbs to characterize noun noun relations. we present a novel, simple, unsupervised method for characterizing the semantic relations that hold between nouns in noun noun  comp ounds. in this paper we focus on the problem of determining the semantic relation(s) that holds within two word english noun compounds. we introduce a novel approach for this problem  use paraphrases posed against an enormous text collection as a way to determine which predicates, represented as verbs, best characterize the relationship between the nouns. our idea is to try to uncover the relationship between the noun pairs by, in essence, rewriting or paraphrasing the noun compound in such a way as to be able to determine the predicate(s) holding between the nouns. here we extend that idea by applying it to determining semantic relations. in our approach, we pose paraphrases for a given noun compound by rewriting it as a phrase that contains a wildcard where the verb would go. for example, we rewrite neck vein as   quot vein that   neck  quot , send this as a query to a web search engine, and then parse the resulting snippets to  nd the verbs that appear in the place of the wildcard. in the remainder of this paper we  rst describe related work, then give details of the algorithm, present preliminary results as compared to other work in the literature, and discuss potential applications. we have presented a simple unsupervised approach to noun compound interpretation in terms of predicates characterizing the hidden relation, which could be useful for many nlp tasks. a signi cant bene t of our approach is that it does not require knowledge of the meanings of constituent nouns in order to c orrectly assign relations. in future we plan to apply full parsing to reduce the errors caused by shallow parsing and pos errors. we will also assess the results against a larger collection  of manually labeled relations, and have an independent evaluation of the appropriateness of the verbs for those relations. we also plan to combine this work  with the structural ambiguity resolution techniques of [#], and determine semantic relations among multi word terms. finally, we want to test the approach on some of the above mentioned nlp tasks.
generalizing automatically generated selectional patterns. we report on measurements of   the degree of selectional coverage obtained with ditt  rent sizes of corpora. we then describe a technique for  using the corpus to identify selectionally similar terms,  and for using tiffs similarity to broaden the seleetional  coverage for a tixed corpus size. we attempt to shed to some light on this question by processing a large corpus of text from a broad domain (business news) and observing how selectional coverage increases with domain size. we therefore also consider how coverage can be increased using a tixed amount of text. we wish to avoid this manual component by auto  maritally identifying semantically related words. this is the approach we have adopted for our current experiments [#], and which has also been employed by ##)agan et al. we corl lttttl e from the co+occurrence data a    quot confitsion matrix  quot , which measures the interchangeability of words in particular contexts. we then use  the confllsion matrix directly to geueralize the selllantic patterns. we ]tltve demonstrated how select.tonal p  tl erns ca.n lye attt otn tic dly acquired from it corpus,  md how selee tiond e, )w ragc gr t(hlally it ere . tses with the size (,f the tra.ining eorl+us. we h tve +dso domonst r tted that i  apos  )r it given corpus size eovcr++ge can i)e sig,tilicantly improved #)y using #,he corpus # o identify selectionally related ternts, and using these simila.rit.ies to generltlize the patterns tybserved in the training eorums. we believe th+tl  lhese lt ehniques e+tn i)e ft,t  apos ther improved in several wa.ys. the exl)erin,ent.s rel)orl.e+l  above ha+ve only get  apos  ra]ized over t, ht, lit  apos st  (head) positioi, of t,he triples  we need to melrstu  apos o the eil  apos   lt  ct of gcncrldizhtg ow r the al  apos gum  lt mt l osh ion as w  lt ql.
 acquisition of lexical information from a large textual italian corpus. we present an automatic method for weighting the contributions of preference functions used in disambiguation. we then focus on one class of preference function, those based on semantic lexical collocations. in particular we deene a function that performs signiicantly better than ones based on mutual information and likelihood ratios of lexical associations. in this paper we address two issues relating to the application of preference functions. combining multiple preference functions the rst problem we address is that of combining diierent functions, each of which is supposed to ooer some contribution to selecting the best among a set of analyses of a sentence. in our case, the problem is formulated as follows. we refer to the coeecients, or weights, used in this linear combination as the  scaling factors  for the functions. we determine these scaling factors automatically in order both to avoid the need for expert hand tuning and to achieve performance that is at least locally optimal. we start with the solution to minimizing a squared error cost function, a well known technique applied to many optimisation and classiication problems.
automatic paraphrase acquisition from news articles. here we describe a procedure for obtaining paraphrases from news article. we exploit this feature by using named  entity recognition. our basic approach is based on the assumption  that named entities are preserved across paraphrases. we applied   our method to articles of two domains and obtained notable examples. although this is our initial attempt to automatically extracting  paraphrases from a corpus, the results are promising. so we hope to build a paraphrase database to  nd expressions which have the same meaning. so we are trying to create a system that automatically acquires paraphrases from given corpora of a speci c domain. in this paper, we describe an approach to automatic paraphrase acquisition from corpora. our main focus is information extraction (ie). using a paraphrase database, we can connect one pattern to another. we expect this will reduce the cost of creating ie knowledge by hand. although our approach aims to collect paraphrases for ie applications, our method can be applied to other purposes also.
citances  citation sentences for semantic analysis of bioscience text. we propose the use of the text of the sentences surrounding  citations as an important tool for semantic interpretation  of bioscience text. we hypothesize several different uses of  citation sentences (which we call citances), including the  creation of training and testing data for semantic analysis  (especially for entity and relation recognition), synonym set  creation, database curation, document summarization, and  information retrieval generally. we illustrate some of these  ideas, showing that citations to one document in particular  align well with what a hand built curator extracted. we also  show preliminary results on the problem of normalizing the  different ways that the same concepts are expressed within  a set of citances, using and improving on existing techniques  in automatic paraphrase generation. as part of the biotext project [##] we are interested in utilizing the large volume of available bioscience text when designing information extraction and retrieval tools. in this paper we put forward a new vision for the path towards robust and large coverage algorithms for semantic interpretation of bioscience articles. we suggest using the sentences that surround the citations to related work as the data from which to build semantic interpretation models. we also introduce a neologism, citances, to mean the sentence(s) surrounding the citation within a document. we believe that citations have great potential to be a valuable resource in mining the bioscience literature. in particular, we identify the following promising applications of citation analysis    a source for unannotated comparable corp ora. in section # we demonstrate the use of citances as comparable corpora for automatic paraphrase extraction. in fact, we b elieve that a paper that is cited enough times can b e summarized using only the citances pointing to it. [##]), we propose to  rst cluster related citances, and then display to the user only a summary of each cluster. citances provide us a way to build a model of many of the different ways to express a relationship type r between entities of type a and b. we can seed learning algorithms with several examples using concepts that are semantically similar to a and similar to b, for which relation r is known to hold. then we can train a mo del to recognize this kind of relation for situations for which the relation is not known. we hypothesize that citances contain the most important information expressed in the cited document, and therefore contain the information that curators would want to make use of. we have found support for this hypothesis with two sample p apers being used by a cancer researcher who is recording information about the process of apoptosis. in the next sections we  rst describe related work in the analysis of citation  sentences, and then describe some of the challenges in processing such sentences. finally, we conclude with future work. we have motivated and discussed the potentially enormous  role that the use of sentences surrounding citations, or citances, can have for automated analysis of bioscience literature. in work not yet reported, we have found that citances align very well with rich information being curated by hand by a molecular biologist, and suspect they will be equally useful for other curation tasks. we also hypothesize that it  will be a gold mine of data for training algorithms to perform semantic analysis of bioscience text, and will improve the results of querying the bioscience literature. we have demonstrated some initial results in paraphrasing citances that discuss the same topic, but more work remains to be done to improve results, and to group similar citances together. in future work, we plan to thoroughly explore the possibilities surrounding the analysis and use of citances for bioscience text analysis.
semantic integration in text  from ambiguous names to identi able entities. this paper presents a  rst step towards this goal by studying semantic integration in natural language texts   identifying whether different mentions  of real world entities, within and across documents, actually represent the same  concept. we present a machine learning study of this problem. our second approach is a global generative model, at the  heart of which is a view on how documents are generated and how names (of  different entity types) are  sprinkled  into them. in its most general form, our  model assumes  (#) a joint distribution overentities(e.g., adocumentthatmentions   president kennedy  is more likely to mention  oswald  or   white house  than   roger clemens ), (#) an  author  model, that assumes that at least one mention  of an entity in a document is easily identi able, and then generates other mentions  via (#) an appearance model, governing how mentions are transformed from the   representative  mention. we show that both approaches perform very accurately, in the range of ##     ##  f  # measure for different entity types, much better than previous approaches  to (some aspects of) this problem. this abstract de nition also captures the problem of semantic integration in texts, which we call here the problem of  robust reading . our goal in this article is to describe a  rst step in the direction of semantic integration of semi structured and unstructured information, a mechanism that can automatically identify concepts in text and link textual segments representing concepts ( mentions  of concepts) to real world objects. other documents may state that  john f. kennedy, jr. was born on november ##, #### , but this fact refers to our target entity s son. the problem we address here has  a different goal and a much wider scope. we aim at the identi cation and disambiguation of real world concepts from its multiple and ambiguous mentions both within and across documents. our problem has broader relevance to the problem of intelligent information access and semantic integration across resources. this paper presents the  rst attempt to apply a uni ed approach to all major aspects of the problem of robust reading. we present two conceptually different machine learning approaches [lmr##a], compare them to existing approaches and show that an unsupervised learning approach can be applied very successfully to this problem, provided that it is used along with strong but realistic assumptions on the usage of names in documents. our  rst model is a discriminative approach that models the problem as that of deciding whether any two names mentioned in a collection of documents represent the same entity. this straightforward modelling of the problem results in a classi cation problem   as has been done by several other authors [crf##, bm##]   allowing us to compare our results with these. this is a standard pairwise classi cation task, and a classi er for it can be trained in a supervised manner  our main contribution in this part  is to show how relational (string andtoken level) features and structural features, representing transformations between names, can improve the performance of this classi er. the results of these attempts were not conclusive and we provide some explanation for it. first, we show that, in general, a clustering algorithm used in this situation may in fact hurt the results achieved by the pairwise classi er. then, we argue that attempting to use a locally trained pairwise classi er as a similarity metric  might be the wrong choice for this problem. our experiments concur with this. however, as we show, splitting data in some coherent way   e.g., to groups of documents originated at about the same time period   prevents some of these problems and aids clustering signi cantly. this observation motivates our second model, which better exploits structural and global assumptions. here we brie y illustrate one of its instantiations and concentrate on its basic assumptions, the experimental study and a comparison to the discriminative model. at the heart of our unsupervised approach is a view on how documents are generated and how names (of different types) are  sprinkled  into them. in its most general form, our model assumes  (#) a joint distribution over entities, so that a document that mentions  president kennedy  is more likely to mention  oswald  or  white house   than  roger clemens   (#) an  author  model, which makes sure that at least one mention of a name in a document is easily identi able (after all, that is the author s goal),  and then generates other mentions via (#) an appearance model, governing how mentions are transformed from the  representative  mention. our experimental results are somewhat surprising  we show that the unsupervised approach can solve the problem accurately, giving accuracies (f # ) above ## , and better than our discriminative classi er (obviously, with a lot more data). after discussing some related work, the rest of this article  rst presents the experimental methodology in our evaluation, in order to present a more concrete instantiation of the problem at hand. it then describes the design of our pairwise name classi er, a comparison to other classi ers in the literature, and a discussion of clustering on top  of a pairwise classi er. finally, we present the generative model and compare the discriminative and generative approaches along several dimensions. this paper describes one of the  rst efforts towards sematic integration in unstructured textual data, providing a promising perspective on the integration of structured databases with unstructured or semi structured information. this paper presents two learning approaches to the  robust reading  problem   the  problem of cross document identi cation and tracing of names of different types, overcoming their ambiguous appearance in the text. in addition to a standard modelling  of the problem as a classi cation task, we developed a model that aims at better exploiting the natural generation process of documents and the process of how names are  sprinkled  into them, taking into account dependencies among entities across types  and an  author  model. we have shown that both models gain signi cantly from incorporating structural and relational information   features in the classi cation model   coherent data splits for clustering and the natural generation process in the the probabilistic model. the robust reading problem is a very signi cant barrier in any effort towards supporting intelligent information access to texts and, speci cally, on our way toward supporting semantic integration of structured information with unstructured information. the reliable and accurate results we show are thus very encouraging. also important  is the fact that our unsupervised model performs so well, since the availability of annotated data is, in many cases, a signi cant obstacle to good performance in semantic integration. ## in addition to further studies of the discriminative model, including going beyond the current noisy supervision (given at a global annotation level, although learning is done locally), exploring how much data is needed for a supervised model to perform as well as the unsupervised model, and whether the initialization of the unsupervised model can gain from supervision, there are several other critical issues we would like to  address from the robust reading perspective. we are grateful to anhai doan for numerous useful comments on this article.
disambiguating web appearances of people in a social network. furthermore, if we are looking for multiple   p eople who are related in some way, how can we best leverage this social network? this paper presents two unsupervised frameworks for solving this problem  one based on   link structure of the web pages, another using agglomerative conglomerative double clustering (a cdc) an application of a recently introduced multi way distributional  clustering method. to evaluate our methods, we collected  and hand lab eled a dataset of over #### web pages retrieved   from google queries on ## personal names appearing together in someones in an email folder. on this dataset our  metho ds outperform traditional agglomerative clustering by  more than ## , achieving over ##  f measure. we face an era not only of an information explosion, but also a tremendous increase in the extent of our relations to  other people. we are constantly presented with new people names, chances to meet and communicate with people, and opportunities to add people to our social network in  our work, from the media, and from our social and business use of the internet. it is now common that we do not actually meet (or even phone) our acquaintances  instead we communicate through email, chatrooms and discussion mittee (iw#c#). we correspond with hundreds of people simultaneously. our social network is tens of times larger than that of our grandparents, and will likely grow more with time. even when we have trouble tracking all these connections, we (intentionally or unintentionally) add new ones. we are in need of personalized tools that will help us manage our social network both to track people we know already, and also to tell us about new people we meet. for example, when we receive email messages from people whose names we do not know, it would be useful to be able to rapidly search for any public facts about them. this may help us know how to rate the importance of the message, or prioritize our effort in making replies. can we  nd not just a few pages, but a comprehensive set of pages? for example, consider david mulford, # the us ambassador to india. if we are looking for information about a particular person, we want to  lter out information about other namesakes, while also preserving the maximum amount of  relevant information. an example name actually appearing in our dataset described in section #. ically populating a database of contact information of people in a user s social network. given a personal name extracted  out of a user s mailbox, we queried go ogle in order to locate the person s homepage. we then applied conditional random  elds [##] to extract institution, job title, address,  phone, fax, email and other information from the homepage. the main problem of our homepage  nding approach  was that we used a simple heuristic for disambiguating person names, which sometimes failed. so, in some cases we extracted the contact information of namesakes of people from the user s social network. in this paper, we address the problem not simply of  nding homepages, but  nding all search engine hits corresponding to a person, and separating them from hits about namesakes. we look beyond homepages because signi cant further information is often found elsewhere. moreover, the person s  homepage may be old and abandoned, containing out ofdate information, and this may be discovered if we have a broader view on the person s web appearances. rather than using simple heuristics, we present results  with two statistical frameworks for addressing this problem  one based on link structure, and another based on the recently introduced multi way distributional clustering  metho d [#]. furthermore, and crucially, rather than searching for people individually, we leverage an existing social network of people, or lists of people who are known to be somewhat connected, and use this extra information to aid the disambiguation. this paper is the  rst attempt to approach the problem of   nding web appearances of a group of people. we have prop osed two relatively straightforward statistical methods for solving this problem. for evaluation purposes we built a large annotated dataset that is publicly available to the scienti c community. we are now working on more sophisticated probabilistic  mo dels for solving this problem that would capture the relational structure of the class of relevant pages.
extracting key phrases to disambiguate personal names on the web. how can we disambiguate these   different people with the same name? this paper presents an unsupervised algorithm which produces key phrases for the different people with  the same name. the algorithm we propose does not require any biographical or social information regarding the person. although there are some previous work  in personal name disambiguation on the web, to our knowledge, this is the   rst attempt to extract key phrases to disambiguate the different persons  with the same name. to evaluate our algorithm, we collected and hand  labeled a dataset of over #### web pages retrieved from google using  personal name queries. our experimental results shows an improvement  over the existing methods for namesake disambiguation. we send simple text queries to search engines and retrieve web pages. in the case of personal names, we may receive web pages to other people with the same name (namesakes). for example if we search for michael jackson in google, among the top hundred hits we get a beer expert and a gun dealer along with the famous singer. in this paper we explore a method which uses terms extracted from web pages to represent the context of namesakes. for example, in the case of michael jackson, terms such as music, album, trial associate with the famous singer, whereas we get beer, travel, hunter as terms for the other (beer expert) namesake of michael jackson. we could use this difference in context to discriminate the namesakes. we do not know in advance the exact number of namesakes for a name on the web. this paper presents an unsupervised clustering framework, which uses a robust similarity metric to overcome these di culties. although our main focus is on disambiguating people with one speci c web appearance, we also explore the posibilities of our algorithm to identify the different web apperances of individuals. this paper is structured as follows. first we give an overview of the related work in this area. then we explain the different components in our system. finally we show experimental results for the proposed method and conclude this paper.
fingerprinting by random polynomials. we have developed an efficient way to determine the syntactic similarity of files and have applied it to every document on the world wide web. using this mechanism, we built a clustering of all the documents that are syntactically similar. introduction the web has undergone exponential growth since its birth, and this expansion has generated a number of problems  in this paper we address two of these  #. the basis of our approach is a mechanism for discovering when two documents are  roughly the same   that is, for discovering when they have the same content except for modifications such as formatting, minor corrections, webmaster signature, or logo. similarly, we can discover when a document is  roughly contained  in another.
measuring semantic similarity between words using web search engines. we propose a robust semantic similarity measure that uses  the information available on the web to measure similarity  b etween words or entities. we de ne various similarity scores for two given  words p and q, using the page counts for the queries p, q  and p and q. moreover, we propose a novel approach to  compute semantic similarity using automatically extracted  lexico syntactic patterns from text snippets. we prop ose an automatic method to measure semantic  similarity between words or entities using web search engines. therefore, no guarantee exists that all the information we need to measure semantic similarity between a given pair of words is contained in the top ranking snippets. for example, let us consider the following snippet from go ogle for the query jaguar and cat. from the previous example, we form the pattern x is the largest y, where we replace the two words jaguar and cat by two wildcards x and y. our contributions in this paper are two fold    we propose an automatically extracted lexico syntactic patterns based approach to compute semantic similarity using text snippets obtained from a web search engine. we integrate different web based similarity measures using wordnet synsets and support vector machines to create a robust semantic similarity measure. to the best of our knowledge, this is the  rst attempt to combine both wordnet synsets and web content to leverage a robust semantic similarity measure. in  section # we discuss previous works related to semantic similarity measures. we then describe the proposed method in section #. in order to evaluate the ability of the proposed method in capturing semantic  similarity between real world entities, we apply it in a community mining task. finally, we show that the proposed metho d is useful for disambiguating senses in ambiguous named entities and conclude this paper.
web based linkage. in this paper, toward this problem, we propose a novel approach that uses the web as the collective knowledge source  in addition to contents of entities. our hypothesis is that  if an entity e  # is a duplicate of another entity e  # , and if e  # frequently appears together with information i on the web,  then e  # may appear frequently with i on the web. by using  search engines, we analyze the frequency, urls, or contents  of the returned web pages to capture the information i of  an entity. extensive experiments verify that our hypothesis  holds in many real settings, and the idea of using the web as  the additional source for the linkage problem is promising. our proposal shows ##  (on average) and ###  (at best)   improvement in precision recall compared to a baseline approach. # let us denote that, among the duplicate entities, the single authoritative one as canonical entity while the rest as duplicate entities or duplicates in short. note  that in this paper our focus is on how to detect duplicate entities with similar names. since two problems are different in nature, we do not consider the name disambiguation problem any further in this paper. we argue that the web is a good representation of what people think. that is, we propose to seek for additional information of an entity from the web. our hypothesis is the following  hypothesis #. therefore, by measuring how the information i appears together with e # on the web, we can determine if e # is a duplicate of e # or not. in particular, we propose various methods to capture the information i of an entity from the web. since our methods rely on search engines such as google or msn to draw additional information of an entity, compared to a baseline method, it is our hope that ours be simpler and more practical. the contribution of our work is  (#) we propose the idea of using the web as a source for additional knowledge of an entity for the linkage problem, (#) to capture the additional information of an entity returned from a search engine, we  propose various methods such as using the frequency of representative terms, url information or the content of returned web pages, and (#) we empirically validate our hypothesis with extensive experiments. in this paper, we propose a novel approach toward the  (record) linkage problem to identify duplicate named entities with insu cient description or contents. unlike other approaches that use textual similarity of contents or name, our proposal unearths the hidden knowledge from the web. experimental results verify that our proposal improves the recall as high as ###  at best, and outperforms the baseline approach for a majority of test cases. our proposal relies on the information on the web. as the web evolves, we expect the information to get better.
psnus  web people name disambiguation by simple clustering with rich features. we describe about the system description of  the psnus team for the semeval #### web  people search task. we consider the problem of disambiguating person names in a web searching scenario as described by  the web people search task in semeval #### (artiles et al., ####). we are to correctly estimate the number of  namesakes for a given person name and group documents referring to the same individual. we have taken several approaches to analyze different sources of information provided with the input data, and also compared strategies to combine these individual features together. the con guration  that achieved the best performance (which were submitted for our run) used a single named entity feature as input to clustering. in the remainder of this paper, we  rst describe our system in terms of the  clustering approach used and alternative features investigated. we then analyze the results on the training set before concluding the paper. we described our psnussystem that disambiguates people mentions in web pages returned by a web search scenario, as de ned in the inaugural web people search task. as such, we mainly focus on extracting various kinds of information from web  pages and utilizing them in the similarity computation of the clustering algorithm.
the semeval #### weps evaluation  establishing a benchmark for the web people search task. this paper presents the task de nition, resources, participation, and comparative results for the web people search task, which  was organized as part of the semeval ####  evaluation exercise. and this is, in essence, the weps (web people search) task we have proposed to semeval #### participants  systems receive a set of web pages  (which are the result of a web search for a person name), and they have to cluster them in as many sets as entities sharing the name. a couple of differences make our problem different. our task is rather a case of word sense discrimination, because the number of  senses  (actual people) is unknown a priori, and it is in average much higher than in the wsd task  (there are ##,### different names shared by ### million people according to the u.s. census bureau). there is also a strong relation of our proposed  task with the co reference resolution problem, focused on linking mentions (including pronouns) in  a text. our task can be seen as a co reference resolution problem where the focus is on solving interdocument co reference, disregarding the linking of all the mentions of an entity inside each document. the weps task ended with considerable success in terms of participation, and we believe that a careful analysis of the contributions made by participants  (which is not possible at the time of writing this report) will be an interesting reference for future research. first of  all, the variability across test cases is large and unpredictable, and a system that works well with the  names in our test bed may not be reliable in practical, open search situations. partly because of that,  our test bed happened to be unintentionally challenging for systems, with a large difference between the average ambiguity in the training and test datasets. we hope to address these problems in a forthcoming edition of the weps task.
computing semantic relatedness using wikipedia based explicit semantic analysis. we propose explicit semantic analysis   (esa), a novel method that represents the meaning of texts in a high dimensional space of concepts  derived from wikipedia. we use machine learning  techniques to explicitly represent the meaning of  any text as a weighted vector of wikipedia based  concepts. we proposed a novel approach to computing semantic relatedness of natural language texts with the aid of very large scale knowledge repositories. we use wikipedia and the odp, the largest knowledge repositories of their kind, which contain hundreds of thousands of human de ned concepts and provide a cornucopia of information about each concept. our approach is called explicit semantic analysis, since it uses concepts explicitly de ned and described by humans. compared to lsa, which only uses statistical cooccurrence information, our methodology explicitly uses the knowledge collected and organized by humans. compared to  lexical resources such as wordnet, our methodology leverages knowledge bases that are orders of magnitude larger and more comprehensive.
improving author coreference by resource bounded information gathering from the web. this paper presents several methods for increasing accuracy by gathering and integrating additional evidence from the  web. we formulate the coreference problem as one  of graph partitioning with discriminatively trained   edge weights, and then incorporate web information either as additional features or as additional  nodes in the graph. since the web is too large to   incorporate all its data, we need an ef cient procedure for selecting a subset of web queries and   data. we formally describe the problem of resource bounded information gathering in each of   these contexts, and show signi cant accuracy improvement with low cost. [ etzioni et al., #### ] , [ mc  callum and li, #### ] , [ dong et al., #### ] however, it is impossible to query for the entire web, and this gives rise to  the problem of ef ciently selecting which queries will provide the most bene t. we refer to this problem as resourcebounded information gathering from the web. we examine this problem in the domain of entity resolution. following previous work, we thus formulate entity resolution as graph partitioning with edge weights based on many features with parameters learned by maximum entropy [ mccallum and wellner, #### ] , and in this paper explore a relational, graph based approach to resource bounded information gathering. the speci c entity resolution domain we address is research paper author coreference. the vertices in our coreference graphs are citations, each containing an author name with the same last name and  rst initial. in this paper, we present two different mechanisms for augmenting the coreference graph partitioning problem by incorporating additional helpful information from the web. for example,  the value of information, as studied in decision theory, measures the expected bene t of queries [ zilberstein and lesser, #### ] budgeted learning, rather than selecting training instances, selects new features [ kapoor and greiner, #### ] resource bounded reasoning studies the trade offs between  computational commodities and value of the computed results [ zilberstein, #### ] active learning aims to request human labeling of a small set of unlabeled training examples [ thompson et al., #### ] , for example, aiming to reduce label entropy on a sample [ roy and mccallum, #### ] in this paper we employ a similar strategy, and compare it with two baseline approaches, showing on ## different data sets that leveraging web queries can reduce f#  error by ##.## , and furthermore that, by using our proposed resource bounded approach, ##.#  of this gain can be achieved with only #  of the web queries. we also suggest  that our problem setting will be of interest to theoretical computer science, since it is a rich extension to correlational clustering [ bansal et al., ####  demaine and immorlica, #### ] we have formulated a new class of problems  resource bounded information gathering from the web in the context  of correlational clustering, and have proposed several methods to achieve this goal in the domain of entity resolution. our current approach yields positive results and can be applied for coreference of other object types, e.g. we believe that this problem setting  has the potential to bring together ideas from the areas of active learning, relational learning, decision theory and graph theory, and apply them in a real world domain. in future work we will explore alternative queries, (including input from more than two citations), as well as various new ways of ef ciently selecting candidate queries. we are  interested in investigating more sophisticated querying criteria in the case of web as a mention.
www. we envision a next generation search engine that can provide significantly more powerful models for entity search. with each cluster we also provide a summary description that is representative of the real person associated with that cluster (for instance in this example the summary description may be a list of words such as   computer science, machine learning, professor   ).
domain independent data cleaning via analysis of entity relationship graph. domain independent data cleaning via analysis of entity relationship graph dmitri v. kalashnikov sharad mehrotra university of california irvine in this article we address the problem of reference disambiguation. speci cally we consider a situation where entities in the database are referred to using descriptions (e.g. the key difference between the approach we propose (called reldc) and the traditional techniques is that reldc analyzes not only object features but also inter object relationships to improve the disambiguation quality. our extensive experiments over two real data sets over synthetic datasets show that analysis of relationships signi cantly improves quality of the result.
self tuning in graph based reference disambiguation. in this paper, we address the problem of learning such models directly  from training data. speci cally, we study a way to calibrate a connection   strength measure from training data in the context of reference disambiguation problem. in this paper we provide an answer to that question for one of the graph link analysis measures, called connection strength (cs). given any two nodes u and v in the graph g, the connection strength c(u, v) returns how strongly this material is based upon work supported by the national science foundation u and v are interconnected to each other in g. we study this measure in the context of reference disambiguation problem. the approach in [##], which we employ to test our adaptive solution, views the dataset as a graph of entities that are linked to each other via relationships. the main contribution of this paper is a supervised learning algorithm  that learns the importance of relationships, or cs, among the classi ed entities and makes the approach self tunable to any underlying domain so that the participation of the domain analyst is minimized signi cantly. the rest of this paper is organized as follows. section # de nes the problem of reference disambiguation and the essence of the disambiguation approach we use. our results show that adaptive connection strength model always outperforms  the state of the art randomwalk model.
exploiting relationships for domain independent data cleaning. in this paper, we address the problem of reference disambiguation. speci cally, we consider a situation where  entities in the database are referred to using descriptions  (e.g., a set of instantiated attributes). the key difference between the approach we propose (called reldc)  and the traditional techniques is that reldc analyzes   not only object features but also inter object relationships to improve the disambiguation quality. our extensive experiments over two real datasets and over synthetic datasets show that analysis of relationships signi cantly improves quality of the result. in this paper, we address one such challenge, which we refer to as reference disambiguation. this paper  argues that the quality of disambiguation can be signi cantly improved by exploring additional semantic information. in particular, we observe that references o ccur within a context and de ne relationships connections between entities. in this paper, we propose a domain independent  data cleaning approach for reference disambiguation, referred to as relationship based data cleaning (reldc), which systematically exploits not only features but also  relationships among entities for the purpose of disambiguation. the primary contributions of this paper are  (#)  developing a systematic approach to exploiting both attributes as well as relationships among entities for reference disambiguation (#) establishing that exploiting relationships can signi cantly improve the quality of reference disambiguation by testing the develop ed approach over # real world datasets as well as synthetic datasets. this paper presents the core of the r eldc approach, details of reldc can be found in [##] where we  discuss various implementations, optimizations, computational complexity, sample content and sample graphs for real datasets, and other issues not covered in this paper. the rest of this paper is organized as follows. in section #,  we precisely formulate the problem of reference disambiguation and introduce notation that will help explain  the reldc approach. in this paper, we have shown that analysis of interobject relationships is important for data cleaning and demonstrated one approach that utilizes relationships. as future work, we plan to apply similar techniques to the problem of record linkage. this paper outlines only the core of the reldc approach, for more details the interested reader is referred to [##].
grouping search engine returned citations for person name queries. we present a technique to group search engine returned citations for p erson name queries, such that the search engine   returned citations in each group belong to the same person. to group the returned citations, we use a multi faceted   approach that considers evidence from three facets  (#) attributes, (#) links, and (#) page similarity. based on the  three facets, we construct a relatedness con dence matrix  for pairs of citations. we then merge pairs whose matching   con dence value is above an empirically determined threshold. experimental results from the implementation of our  multi faceted approach are promising. in this paper we introduce a method that is able to group the returned citations from a search engine such as google [#] or yahoo [##] for a person name query, such that each we use citations to refer to the returned results that are related to a speci c query in a search engine. in the output we retain the basic search engine returned citations. further, within each group we maintain the search engine ranking order, and among groups we maintain the relative order of citations as originally presented by the search engine. our method considers three facets  attributes, links, and page similarity. for each facet we generate a con dence matrix. then we construct a  nal con dence matrix for all facets. using a threshold, we apply a grouping algorithm on the  nal con dence matrix for all facets. we present our contribution of providing a solution to the interesting and useful problem of grouping person name queries by person as follows. section # introduces our multi faceted approach to solving the problem by explaining the three facets we use  (attributes, links, and page similarity), showing how to construct a con dence matrix for each facet and how to combine all the con dence matrices into a  nal con dence matrix,  and giving the algorithm we use to group returned citations. section # discusses our experimental results. we designed and implemented a system that can automatically group the returned citations from a search engine p erson name query, such that each group of citations refers to the same person. we used a multi faceted approach that considers three facets  attributes, links, and page similarity. we gave experimental evidence to show that our approach can be successful. in particular we tested ## arbitrary names and found both a low normalized split score (#.###) and a low normalized merge score (#.###). to accomplish this research, we would  rst need to determine how to recognize if a name is popular or not. we would then need to determine how to set thresholds as a function of popularity. we would, however, have to determine new attributes for each kind of proper noun. we would also have to obtain training data and use it to establish the conditional probabilities. we may also need to adjust the threshold values.
web people search via connection analysis. in this paper, we develop a web people search approach that clusters web pages based on their association to different people. our method exploits a variety of semantic information extracted from web pages, such as named entities and hyperlinks, to  disambiguate among namesakes referred to on the web pages. we demonstrate the effectiveness of our approach by testing the  efficacy of the disambiguation algorithms and its impact on person search. with each cluster, we  also provide a summary description that is representative of  the real person associated with that cluster (for instance, in  this example, the summary description may be a list of  words such as  computer science, machine learning, and  professor ). for  example, assume that we are searching for  tom mitchell,  the psychology professor  with his name and keywords   psychology  and  professor. in this paper, we make the following contributions. first, we develop a novel algorithm for disambiguating  among people that have the same name. our algorithm is  based on extracting  significant  entities such as the  names of other persons, organizations, and locations on
adaptive graphical approach to entity resolution. in this pap er we present a graphical approach for entity resolution. in our past work, we have developed a novel graphical methodology for entity resolution [##,## ##]. to overcome this problem, in this paper we present an algorithm that makes the overall approach self adaptive to the data being processed, leading to the improvement of the quality and e ciency of the proposed methodology. the rest of this paper is organized as follows. finally, we conclude in section #. in this paper, we have developed a novel domain independent entity resolution approach and have presented its empirical evaluation on datasets taken from two different domains. in addition, we have developed a  method that minimizes the required domain analyst participation and achieves even higher disambiguation quality by b eing able to self tune the overall approach to the dataset b eing processed.
a testbed for people searching strategies in the www. this paper describes the creation of a testbed to evaluate people searching strategies on the worldwide web. in this paper, we describe the creation of a testbed to evaluate strategies addressing this people searching task on web documents. we provide  (i) a corpus of web pages retrieved using person names as queries to web search engines   (ii) a classification of pages according to the different people (with the same name) they refer to  (iii) manual annotations of relevant information  found in the web pagesdescribing them (e mail, image, profession, phone number, copyright is held by the author owner.
polyphonet  an advanced social network extraction system from the web. we propose a social network extraction system called polyphonet, which employs several advanced techniques to extract  relations of persons, detect groups of persons, and obtain keywords   for a person. several studies have used search engines to extract social networksfromtheweb, butourresearchadvancesthefollowingpoints    first, we reduce the related methods into simple pseudocodes using google so that we can build up integrated systems. second,  we develop several new algorithms for social networking mining   such as those to classify relations into categories, to make extraction scalable, and to obtain and utilize person to word relations. we overview that system. social networks play important roles in our daily lives. our lives are profoundly in uenced mittee (iw#c#). acm # ##### ### # ## ####. by social networks without our knowledge of the implications. social networks enable us to detect such persons with high betweenness. this paper presents advanced algorithms for social network extraction from the web. our contributions are summarized as follows    related studies are summarized and their main algorithms are described in brief pseudocodes. we brie y overview that system. below, we take the jsai cases as examples  a system is developed in japanese language for jsai conferences and in english language for the ubicomp conference. for that reason, we try to keep the algorithms as abstract as possible. we have various evaluations  of algorithms of japanese versions, but we have insuf cient evaluations for the english version. therefore, we show some evaluations in the japanese version if necessary, in order to provide meaningful insights to readers. this paper is organized as follows. we brie y overview polyphonet in section #. we propose super social network mining architecture in section # and conclude this paper. this paper describes a social network mining approach using the web. several studies have addressed similar approaches so far   we organize those methods into small pseudocodes. several algorithms, which classify the relations using google, make the extraction scalable, and obtain a person to word matrix, are novel as far as we know. we implemented every algorithm on polyphonet, which was put into service at jsai conferences over three years and at the ubicomp conference. acquiring knowledgethrough googling is a similar concept to ours [##]. we intend to apply our approach in the future to extract much structural knowledge aside from social networks.
person resolution in person search results  webhawk. first of all, some pages may not contain any person information, referred to as junk pages in this paper, because person names may refer to non person entities, such as products, companies, or places. david lee   coach to investigate how severe the multi referent ambiguity is in reality, we selected the ### most frequent person queries from the log of msn. we issued these queries to msn, and manually  counted for each query the number of different referents that occur in top ### retrieved pages. we see that ##  of person queries retrieve two or more referents, and about ##  retrieve more than ten referents. in this paper we present a person resolution system, called webhawk. msn in this study), webhawk facilitates person search by re organizing the search results in three  steps  first of all, a filter removes junk pages that contain no person information. we see that there are ## junk pages being removed. in the rest of this paper, we first review related work in section  #. section # describes the corpora we used in our study. in sections # to #, we describe each of the four components (i.e., filter, cluster, extractor and namer) in detail, presenting a separate evaluation of each component where appropriate. in section #, we present a pilot user study, where webhawk is compared with another state of the art system. finally, we conclude the paper in section #. we have presented an effective system for person resolution in person search results, called webhawk. we have attempted a new method where we categorized person pages into more elaborate sub categories (e.g. this method however did not improve the performance as we expected in our pilot experiments. we will explore new clustering scenarios in our future work. at present, webhawk focuses only on english person names and we will adapt the system to other languages in the near future.
incorporating prior knowledge with weighted margin support vector machines. in this paper, we propose a simple generalization of svm  weighted margin svm (wmsvms) that  permits the incorporation of prior knowledge. we show   that sequential minimal optimization can be used in training wmsvm. we discuss the issues of incorporating prior   knowledge using this rather general formulation. in this paper, we describe a generalization of svm that allows for incorporating prior knowledge of any form,  as long as it can be used to estimate the conditional inclass probabilities. this paper provides the geometrical motivations for generalized wmsvm formulation, its primary and dual problem, and a modi cation of sequential minimum optimization (smo) training algorithm for wmsvm. we can then incorporate the prior human knowledge through generating the  pseudo  training dataset  from an unlabeled dataset, using the estimation of conditional probability p (y x) over the possible label values   #,+#  as the con dence value. in this paper we use text classi cation as a running example not only because empirical studies [#, ##] suggest svm is well suited for the application and often produces better results, but also because the keyword based prior knowledge is easy to obtain [##] in the text domain. the rest of this paper is structured as follows. we introduce related work in section #. in section #, we present experimental results on a popular text categorization dataset. we conclude in section # with some discussion of potential use of wmsvms. in this paper, we proposed a generalization of the standard svm  weighted margin svm, which can handle the imperfectly labeled dataset. we then introduced a twostep approach to incorporate fuzzy prior knowledge using  the wmsvm. the empirical study of our approach is conducted through text classi cation experiments on standard datasets.
constructing informative prior distributions from domain knowledge in text classi cation. we prop os e   instead combining domain knowledge with training examples in a bayesian framework. we show  on three text categorization data sets that this approach   can rescue what would otherwise be disastrously bad training situations, producing much more effective classi ers. in what follows we apply this approach with logistic regression as our model and text classi cation we begin by reviewing the use of logistic regression in text classi cation, and the bayesian approach in particular (section #), then discuss previous approaches to integrating  domain knowledge in text classi cation (section #). section # presents our bayesian approach, which is simpler and more  exible. section # describes our experimental methods, while section # presents our results. we  nd on three test categorization test collections, using three diverse sources of domain knowledge, that domain speci c priors can yield large effectiveness improvements.
information, prediction, and query by committee. in this article, we consider the problem of learning a binary concept in the absence of noise. we describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. we test our implementation, called an sg network, on three domains and observe significant improvement in generalization. by active learning, we mean any form of learning in which the learning program has some control over the inputs on which it  a preliminary version of this article appears as cohn et al. within the broad definition of active learning, we will restrict our attention to the simple and intuitive form of concept learning via membership queries. we have been studying the problem of learning binary concepts in an errorfree environment.
program.
re examining the potential effectiveness of interactive query expansion. although interactive query expansion has the potential to be an  effective means of improving a search, in this paper we show  that, on average, human searchers are less likely than systems to  make good expansion decisions. we show that  simple instructions on using interactive query expansion do not  necessarily help searchers make good expansion decisions and  discuss difficulties found in making query expansion decisions. in this paper we revisit this claim to investigate more fully the potential effectiveness of iqe. in particular we investigate how good a user s query term selection would have to be to increase retrieval effectiveness over automatic strategies for query expansion. we also compare human assessment of expansion term utility with those assessments made by the system. in section # we discuss the motivation behind our investigation and that of magennis and van rijsbergen. in section # we describe our experimental methodology and data. in section # we investigate the potential effectiveness of iqe and in section # we compare potential strategies for helping users make iqe decisions. in section # we summarise our findings. in this paper we examined the potential effectiveness of interactive query expansion. there are several limitations to this work  for example, we only concentrated on altering the content of the query  future investigations will compare the results obtained here when we use relevance weighting in addition to query expansion. we also do not differentiate between queries although the success of query expansion can vary greatly across queries. we will consider this in future work, our intention here is to investigate the general applicability of query expansion.
text clustering with extended user feedback. we  rst introduce a new probabilistic generative mo del for  text clustering (the speclustering model) and show that   it outperforms the commonly used mixture of multinomials clustering model, even when used in fully autonomous   mo de with no user input. we then describe how to incorp orate four distinct types of user feedback into the clustering algorithm, and provide experimental evidence showing  substantial improvements in text clustering when this user  feedback is incorporated. we as human beings are quite familiar with clustering objects into categories based on features of these objects. there are many other examples  we may informally cluster news stories into categories personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the  rst page. acm sigir  ## seattle, washington usa copyright ###x acm x xxxxx xx x xx xx ...  #.##. such as sp orts, politics, etc., or we may easily recognize in a supermarket what type of products a corridor belongs to. a fourth type of extra information, which we are primarily interested in, is information  about the key surface features for a particular class, or cluster. we are interested in how to best incorporate user input into automated clustering algorithms, and more generally into mixed initiative clustering approaches that allow the user and computer to jointly arrive at coherent clusters that of discovering clusters of interest to the user is somewhat  different from the objective optimized in totally unsupervised clustering algorithms that attempt instead to maximize some statistical property of the clusters (such as data  likelihood, or inter cluster distance). we are speci cally interested in how to incorporate into clustering algorithms the user s emerging understanding about a category # , stimulated by seeing the instances that are clustered together, and by seeing (and editing) summaries of these emerging clusters. the chief contribution of this paper is to introduce a new probabilistic model for clustering that outperforms standard unsup ervised clustering in our experiments, and that also  can accomodate a variety of typ es of user feedback to iteratively re ne the clusters. we present experiments in both  an email clustering domain, and in a second document clustering domain (## newsgroups) showing the performance of this clustering approach. the research we report here is part of our larger research effort to build computer algorithms to automatically infer the key activities, or projects, a user is involved in, given the contents of their workstation (e.g., their emails,  les, directories, calendar entries, personal contacts lists, etc.). in our previous work[#], we have shown that unsupervised clustering of emails can result in useful descriptions of user activities, such as the one shown in figure #. the work we report in this paper is motivated in part by our interest in developing a more mixed initiative approach to inferring  such activity clusters, using both computer analysis of workstation data and user feedback based on examining proposed clusters. contacts, email, task, estimates, zero, reschedule, baseline, rebecca mitchell(##), kaelbling(#), mccallum(#), perrault(#), ray(##), stuart(#), william(#), april(#),   we will describe our probabilistic model and the associated clustering algorithm in the next section. section # then we use the word  cluster  to indicate a set of similar instances grouped together by a clustering algorithm, and the word  category  to indicate a concept in a user s mind which may or may not be re ected by some cluster of instances. in this paper, we focus on the problem of how to cluster #.# spec bound cr+pp cr+wx+hx cr+pp+wx+hx text documents based on the meanings of categories a user  understands or wants. our solution to this problem involves three components. first, we propose a new speclustering model that separates the features of a document that are speci c to a cluster from  other general features that are unrelated to the cluster s semantics. we present an interface that enables a user to browse through cluster results and provide several types of feedback. our experimental results show our unsupervised speclustering algorithm outperforms the commonly used multinomial naive bayes clustering algorithm for both of the text data sets we considered. our approach combines the advantage of the machine s computational power to analyze large data sets, with the  advantages of a human s understanding of categories of interest. acknowledgments we thank sophie wang for useful  discussions and contributions to earlier versions of the clustering email algorithm.
active learning with feedback on both features and instances. we extend the traditional active learning framework to include feedback on features in addition  to labeling instances, and we execute a careful study of the effects of feature selection and human   feedback on features in the setting of text categorization. our experiments on a variety of categorization tasks indicate that there is signi cant potential in improving classi er performance by   feature re weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. our  experiments on human subjects indicate that human feedback on feature relevance can identify a  suf cient proportion of the most relevant features (over ##  in our experiments). we  nd that   on average, labeling a feature takes much less time than labeling a document. we devise an algorithm that interleaves labeling features and documents which signi cantly accelerates standard  active learning in our simulation experiments. in experiments in this paper we study the bene ts and costs of feature feedback via humans on active learning. we try to  nd a marriage between approaches to incorporating user feedback from machine learning and information retrieval and show that active learning should be a twofold process   at the term level and at the document level. we  nd that people have a good intuition for important features in text classi cation tasks, since features are typically words, and the categories to learn may often be approximated by some disjunction or conjunction of a subset of the features. we show that human knowledge on features can indeed increase active learning ef ciency and accelerate training signi cantly in the initial stages of learning. this paper extends our previous work in employing such a two tiered approach to active learning (raghavan et al., ####). we state the active learning problems that we address and present our approach to use feedback on both features and instances to solve the problems in section #. we give the details of the implementations in section #. in section # we describe the data and metrics we will use to evaluate the performance of active learning. we obtain a sense of the extent of the improvement possible via feature feedback by de ning and using a feature oracle. in section # we show that humans can indeed identify useful features. furthermore, we  nd that labeling a feature http   projects.ldc.upenn.edu tdt# annotation label instructions.html . in section #.# we show that the human chosen features signi cantly accelerate learning in experiments that simulate human feedback in an active learning loop. we discuss related work in section # and conclude in section #. teacher assigns label y t to x t c. m t   train classi er( hx i ,y i i i   #... t , m t # ) d. t + + return m t oracle we have demonstrated experimentally that for learning with few labeled examples good (oraclebased) feature selection is extremely useful. we conducted a user study to see how well naive users performed as compared to a feature oracle in the domain of text categorization. our technique weighted the features marked relevant by the users more than the other features. we used our users  outputs in realistically simulated human in the loop experiments and observed a signi cant increase in learning performance with our techniques over plain active learning. in summary, our contributions are  #. we demonstrated that access to a feature importance oracle can improve performance (the f# score) signi cantly over uncertainty sampling, even with as few as # examples labeled. we found that even naive users can provide effective feedback on the most relevant features (about ##  accuracy of the oracle in our experiments). we measured the manual costs of relevance feedback on features versus labeling documents  we found that feature feedback takes about one  fth of the time taken by document labeling on average. we devised a method of simultaneously soliciting class labels and feature feedback that improves classi er performance signi cantly over soliciting class labels alone. in this paper we extended the traditional active learning setting which concerns the issue of minimal feedback and proposed an approach  where the user provides feedback on features as well as documents. we showed that such an approach has good potential in signi cantly decreasing the overall amount of interaction required for training the system. this paper points to three promising inter related questions for further exploration. in our case, the learner asked the teacher labels on word features and documents, both of which required little  effort on the part of the teacher to understand what was being asked of him. our subjects did indeed  nd labeling words without context a little hard, and suggested that context might have helped. we explored one method in this paper and discussed alternatives in section #.
computational learning theory  survey and selected bibliography.
machine learning in automated text categorization. we will discuss in detail issues pertaining to three different problems namely document representation classi er construction classi er evaluation. rule learning methods usually attempt  to select from all the possible covering  rules (i.e., rules that correctly classify  all the training examples) the  best  one acknowledgments this paper owes a lot to the suggestions and constructive criticism of norbert fuhr and david lewis.
text categorization with support vector machines  learning with many relevant f eatures. this paper reports a controlled study with statistical signii cance tests on ve text categorization methods  the support vector machines (svm), a k nearest neighbor (knn) clas siier, a neural network (nnet) approach, the linear least squares fit (llsf) mapping and a naive b a yes (nb) classi er. we focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training set category frequency. our results show that svm, knn and llsf signiicantly outper form nnet and nb when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are suuciently common (over ### instances). in this paper we address the above e v aluation problems by conducting a controlled study on ve w ell known text categorization methods  nnet, svm, nb, knn and llsf.
j a mer soc i n f s c i. this paper is a comparative study of feature selection methods in statistical learning of text categorization. we found ig and chi most eeective in our experiments. indeed we found strong correlations between the df, ig and chi values of a term.
proceedings of ieee international conference on acoustics, speech, and signal processing. this paper outlines the #### research tasks, corpora, evaluation procedures, technical approaches, and results.
unsupervised and supervised clustering for topic tracking                                                                                                                                                                                                                                                                                                                #.
machine learning in automated text categorization. we will discuss in detail issues pertaining to three different problems namely document representation classi er construction classi er evaluation. rule learning methods usually attempt  to select from all the possible covering  rules (i.e., rules that correctly classify  all the training examples) the  best  one acknowledgments this paper owes a lot to the suggestions and constructive criticism of norbert fuhr and david lewis.
exploiting comparable corpora and bilingual dictionaries for cross language text categorization. in this work we present many solutions according to the availability of bilingual resources, and we show that it is possible  to deal with the problem even when no  such resources are accessible. experiments show the effectiveness of our  approach, providing a low cost solution for  the cross language text categorization   task. in the worldwide scenario of the web age, multilinguality is a crucial issue to deal with and to investigate, leading us to reformulate most of the classical natural language processing (nlp)  problems into a multilingual setting. in this paper we present some solutions to deal  with cltc according to the availability of bilingual resources, and we show that it is possible  to deal with the problem even when no such resources are accessible. this allows us to de ne a kernel function (i.e. we  also investigate this problem exploiting synsetaligned multilingual wordnets and standard bilingual dictionaries (e.g. experiments show the effectiveness of our approach, providing a simple and low cost solution for the cross language text categorization  task. section # evaluates our methodologies and section # concludes the paper suggesting some future developments. in this paper we have shown that the problem of cross language text categorization on comparable  corpora is a feasible task. in any case we think that our methodology is low cost and simple, and it can represent a technologically viable solution for multilingual problems. for the future we try to  explore also the use of a word sense disambiguation all words system. we are con dent that even with the actual state of the art wsd performance, we can improve the actual results.
prague czech english dependency treebank #.#. introduction our goal in cross language text classification (cltc) is to use english training data to classify czech documents (although the concepts presented here are applicable to any language pair). cltc is motivated by both the non availability of czech training data (the case, presently, in our dataset) and the possibility of leveraging different topic distributions in different languages to improve overall classification for information retrieval. consider, for example, that english speakers tend to contribute more to some topics than their czech counterparts (e.g., to discuss london more than prague), so that, having only documents in english, we may expect to do poorly at identifying topics like prague. czech speakers, on the other hand, often talk about prague, so that by leveraging czech data, we might expect to improve on detecting the topic prague in english speakers  and prague in english speakers is exactly the sort of thesaurus label which information seekers are most interested in because it is rare. accordingly, while a lack of czech training data presently necessitates cltc, we would have no reason to warrant the method s abandonment if such data were to suddenly become available. our dataset is a collection of manually transcribed, spontaneous , conversational speech in english and czech. as we have training data only in english, we may translate all of the czech data features into english for classification (we refer to this as english sided classification).
cross lingual text categorization. this article deals with the problem of cross lingual text   categorization (cltc), which arises when documents in different languages must be classi ed according to the same classi cation tree. we describe practical and cost effective solutions for automatic cross lingual   text categorization, both in case a su cient number of training examples is available for each new language and in the case that for some  language no training examples are available. it is used in many information providing institutions, either in the form of a hierarchical mono classi cation ( where does this document belong in our topic hierarchy ) or as a multi classi cation, assigning zero or more keywords to the document, with the purpose of enhancing and simplifying retrieval. this article describes the cross lingual classi cation techniques developed in the peking project # and presents the results achieved in classifying the ilo corpus using the lcs classi cation engine. in the following two sections we relate our research to previous research in  cross language information retrieval, describe the ilo corpus and our experimental approach. in section # we establish a baseline for mono lingual classi cation of the ilo corpus, using different classi cation algorithms (winnow  and rocchio). in sections # and # we propose three different solutions for crosslanguage classi cation, implying increasingly smaller (and therefore less costly)  translation tasks. then we describe our main experiments in multi lingual classi cation, and compare the results to the baseline. cross lingual text categorization is actually easier than cross lingual information retrieval, for the same reason that lemmatization and term normalization have much less effect in cltc than in clir  the law of large numbers is with  us. given an abundance of training documents, our statistical classi cation algorithms will function well, even in the absence of term con ation, which is the  cltc equivalent of expansion in clir. we do not have to work hard to ensure that all linguistically related forms or synonyms of a word are con ated  if two equivalent forms of a word occur frequently enough to have an impact on classi cation, they will also do so as independent terms. we have found viable solutions for two extreme cases of cross lingual text categorization, between which all practical cases can be situated. on the one hand we found that poly lingual training, training one single classi er to classify documents in a number of languages, is the simplest approach to cross lingual text categorization, provided that enough training examples are available in the respective languages (tens to hundreds), and the classi cation algorithm used is immune to the evident disjointedness of the resulting class pro le (as is the case for winnow but not for rocchio). although the accuracy is somewhat lower, this pro le based translation provides a very cost effective way to perform cross lingual classi cation  in our experiment an average of ## terms per class had to be translated.
an em based training algorithm for cross language text categorization. in this paper we propose a learning algorithm based on the em scheme which   can be used to train text classi ers in a multilingual environment. in particular, in the proposed approach, we   assume that a prede ned category set and a collection of labeled training data is available for a given language l  # . this technique allows us to extract correct statistical properties of the language l  #  which are not completely available in automatically translated examples, because of the different characteristics of language l  # and of   the approximation of the translation process. our experimental results show that the performance of the proposed   method is very promising when applied on a test document set extracted from newsgroups in english and italian. the cltc task can be stated as follows  suppose we have a good classi er for a set of categories in a language l # and a large amount of unlabeled data in a different language l #   how can we categorize this corpus according to the same categories de ned for language l # without having to manually label any data in l # ? when using the machine learning paradigm, this problem can be reformulated as  how can we train a text classi er for language l # using the examples labeled for language l # ? the approach that we propose is based on two steps   rst the training set available in the language l # is translated into the target language l # using an automatic translation system. this procedure can introduce noise in the translated documents since automatic translation is often approximate, especially if we use simple translation systems. in the next section we  discuss possible approaches to cross language text classi cation and review the existing techniques proposed in the  literature. in this paper we presented a new technique to categorize text documents in a cross language environment. the proi test set recall precision posed approach is based on the idea that we can use a known training set in one language to initialize the em iterations  on an unlabeled set of documents written in a different language. we also show that feature selection is extremely important in this setting. experimental results demonstrated that our approach is highly effective and reaches recall and precision values comparable with the monolingual case, where manually labeled documents are prepared for each language.
the university of amsterdam at clef qa ####. we describe the system that generated our submission for the #### clef question answering dutch monolingual task. our system for this year s task features entirely new  question classi cation, data storage and access, and answer processing components. for our earlier participation in the clef question answering track (#### ####), we have developed a question answering architecture in which answers are generated by different competing strategies. for the #### edition of clef qa, we fo cuse d on converting our text resources to xml in order to make possible a qa as xml retrieval strategy. for ####, we have c onverted all of our data resources (text, annotations, and tables) to  t in an xml database in order to further standardize access. additionally, we have devoted attention to improving known weak parts of our system  question classi cation, type checking, answer clustering, and sc ore estimation. this paper is divided in nine sections. in section #, we give an overview of the c urrent system architecture. in the following four sections, we describe the changes we have made to our system for this year  question classi cation (section #), storing data with multi dimensional markup (section #), and probabilistic answer processing (sections # and #). we present our submitted runs in section # and evaluate them in section #. we conclude in section #. we have described the fourth iteration of our system for the clef question answering dutch mono lingual track (####). this year, our work has focused on converting all data repositories of the system (text, annotation and tables) to xml and allowing them to be accessed via the same interface. additionally, we have modi ed parts of our system which we had suspected of weaknesses  question classi cation and answer processing. at this point, we would like to know if the modi cations of the system have resulted in improvemed performance. however, the #### questions were more di cult than those of last year, and we expect that the workshop will show that the performance of other participants has also dropped. it would be nice to be able to run last year s system on the #### questions but unfortunately, we do not have a copy of that system available. applying the current s ystem to last year s questions is something we can and should do, but the outcome of that experiment may not be completely reliable since those questions have been used for tuning the system. at this moment, we simply do not know if our work has resulted in a better system. one thing we know for certain  there is a lot of room for improvement. luckily, our #### clef qa participation has identi ed key topics for future work  information extraction, query generation, answer type checking, and answer ranking. we hop e that work on these topics will lead to better performance in ####.
. this paper presents the current status of work to extend the javelin qa system with domain semantics for question answering in restricted domains. we discuss how the original architecture was extended, and how the system modules must be adjusted to incorporate knowledge from existing ontologies and information provided by third party annotation tools. to extend the system for use in restricted domains, we first considered the differences between developing an open domain qa system and developing one for a particular domain. symbolic nlp might be possible , but only if we can identify the lexical items in the restricted domain and interpret them properly. under phase ii of arda s aquaint program, we are extending javelin for use with the corpus of documents created by the center for non proliferation studies (cns). the cns corpus contains reports on the weapons capabilities of various geo political entities  for this study we selected a subset of #### documents that are potentially relevant to a particular intelligence scenario (development of bioweapons in egypt).
proceedings of the twelfth text retrieval conference. we believe that achieving high performance in the question answering task requires a combination of multiple strategies designed to capitalize on different characteristics of various resources. the system we deployed for the trec evaluation last year relied exclusively on the world wide web to answer factoid questions (). in many ways, we can utilize huge quantities of data to overcome many thorny problems in natural language processing such as lexical ambiguity and paraphrases . we have identified this class of techniques as the knowledge mining approach to question answering (). in addition to viewing the web as a repository of unstructured documents, we can also take advantage of structured and semistructured sources available on the web using knowledge annotation techniques ( katz, ####  ). these techniques allow our system to view the web as if it were a   virtual database   and use knowledge contained therein to answer user questions.
qualifier in trec ## qa main task. this paper describes a question answering system and its various modules to solve definition, factoid and list questions  defined in the trec## main task. in particular, we tackle the factoid qa task by event based question answering. by analyzing the external  knowledge from pre retrieved trec documents, web documents, wordnet and ontology to discover the qa event  structure, we explore the inherent associations among qa elements and then obtain the answers. we highlight the shared modules, fine grained  named entity recognition, anaphora resolution and canonicalization co reference resolution, among the three subsystems as  well. in trec ## (voorhees ####), we explored the use of external resources like the web and wordnet to extract terms that are highly correlated with the query, and use them to perform linear query expansion (yang  amp chua,####). while the technique has been found to be effective, we found that there is a need to perform structured analysis on the knowledge obtained from the web wordnet to further improve the performance. this year, we model the factoid questions by event based question answering (yang et al. we thus perform event mining to discover and then incorporate the knowledge of event structure systematically for more effective qa. our system, named qualifier (question answering by lexical fabric and external resources), includes modules to perform detailed question analysis, qa event construction, answer justification, fine grained named entity recognition, anaphora resolution, canonicalization co reference, and successive constraint relaxation. we derive detailed question class ontology that corresponds to fine grained named entities. this enables us to extract exact answer from the candidate sentences more accurately. in order to bridge this gap, we use the knowledge of both the web and lexical resources to expand the original query. finally, we structure the query and use it to search for answer candidates through the mg system (witten et al. we will describe the three subsystems one by one in the following sections.
cogex  a logic prover for question answering. this paper introduces the idea of automated  reasoning applied to question answering and  shows the feasibility of integrating a logic  prover into a question answering system. in this paper we introduce one such novel idea, the use  of automated reasoning in qa, and show that it is feasible, effective, and scalable. we have implemented a logic prover, called cogex (from the permutation of  the  rst two syllables of the verb excogitate) which uniformly codi es the question and answer text, as well as world knowledge resources, in order to use its inference  engine to verify and extract any lexical relationships between the question and its candidate answers. our solution is to integrate the prover into the qa system and rely on reasoning methods only to augment other previously implemented answer extraction techniques.
in question answering, two heads are better than one. motivated by the success of ensemble methods   in machine learning and other areas of natural language processing, we developed a multistrategy and multi source approach to question   answering which is based on combining the results from different answering agents searching   for answers in multiple corpora. we present our multi level answer  resolution algorithm that combines results from  the answering agents at the question, passage,  and or answer levels. experiments evaluating   the effectiveness of our answer resolution algorithm show a ##.#  relative improvement over  our baseline system in the number of questions  correctly answered, and a ##.#  improvement  according to the average precision metric. in our own earlier work, we developed a specialized mechanism called virtual annotation for handling de nition questions (e.g.,  who was galileo? we have shown that better performance is achieved by applying virtual annotation and our general purpose qa strategy in parallel. in this  paper, we investigate the impact of adopting such a multistrategy and multi source approach to qa in a more general fashion. our approach to question answering is additionally  motivated by the success of ensemble methods in machine learning, where multiple classi ers are employed and their results are combined to produce the  nal output of the ensemble (for an overview, see (dietterich, ####)). in our question answering system, piquant, we utilize in parallel multiple answering agents  that adopt different processing strategies and consult different knowledge sources in identifying answers to given  questions, and we employ resolution mechanisms to combine the results produced by the individual answering agents. we call our approach multi strategy since we combine the results from a number of independent agents implementing different answer  nding strategies. we also call it multi source since the different agents can search  for answers in multiple knowledge sources. in this paper, we focus on two answering agents that adopt fundamentally different strategies  one agent uses predominantly knowledge based mechanisms, whereas the other agent is based on statistical methods. our multi level resolution algorithm enables combination of results from  each answering agent at the question, passage, and or answer levels. our experiments show that in most cases  our multi level resolution algorithm outperforms its components, supporting a tightly coupled design for multiagent qa systems. experimental results show signi cant performance improvement over our single strategy,  single source baselines, with the best performing multilevel resolution algorithm achieving a ##.#  relative improvement in the number of correct answers and a ##.#   improvement in average precision, on a previously unseen test set. search search analysis qgoals answering agent answering agent corpus corpus resolution answering agent answering agent in this paper, we introduced a multi strategy and multisource approach to question answering that enables combination of answering agents adopting different strategies  and consulting multiple knowledge sources. in particular, we focused on two answering agents, one adopting a knowledge based approach and one using statistical methods. we discussed our answer resolution component which employs a multi level combination algorithm that allows for resolution at the question, passage, and answer levels. our experiments showed that our best performing algorithms achieved a  ##.#  relative improvement in the number of correct answers and a ##.#  improvement in average precision on a previously unseen test set.
answer selection and confidence estimation. we describe bbn s question answering work at trec ####. we focus on two issues  answer selection and confidence estimation. we found that some simple constraints on the candidate answers can improve a pure ir based technique for answer selection. we also found that a few simple features derived from the question answer pairs can be used for effective confidence estimation. our results also confirmed earlier findings that the worldwide web is a very useful resource for answering trec style factoid questions. for answer selection, we used a hmm based ir system ( miller et al, #### ) to first select documents that are likely to contain answers to a question and then rank candidate answers based on the answer contexts using the same ir system. then we used a few constraints to re rank the candidates. instead, we computed the probability based on a few features that concern q and a.
clef #### working notes. in this paper we investigated the use of wikipedia, the open domain encyclopedia, for the question answering task. we focused on some different aspects of the problem, such as the validation of the answers as returned by our question answering system and on the use of wikipedia   categories   in order to determine a set of patterns that should fit with the expected answer. we performed our experiments using the spanish version of wikipedia, with the set of questions of the last clef spanish monolingual exercise. we investigated the use of wikipedia in some slightly different aspects of the question answering task  answer validation and generation of answer patterns. in this case, the class is   fruit   , and we want to find a  ing all subtypes but the   country   one) and   defini  tion   .
type checking in open domain question answering. we compare two kinds  of strategies for answer type checking for open domain qa. we focus on one particular kind of  ltering  answer type checking, that is, checking whether a given answer candidate belongs to the expected semantic type (or set of types). on top of the usually coarsegrained expected answer types used for extracting answer candidates (such as person, location, date or organization), it is often possible (and necessary) to identify more precisely whether we are looking for, e.g., an actor, a capital, a year, or an amsterdam, the netherlands. how can we operationalize answer type checking? we discuss two strategies, both using freely available on line resources. the aim of this paper is to study the potential impact of answer type checking on the performance of open domain qa systems. we break this general issue up into more manageable issues, each of which we aim to address in this paper  (#) does type checking (generally) improve the performance of open domain qa systems? both questions have to be answered while keeping in mind that the external knowledge sources  that we employ are usually domain speci c. can our results be transferred from one knowledge source and domain to others? we describe algorithms for knowledge intensive and redundancybased answer typechecking (in sections# and #). to be able toassess knowledge intensive answer type checking, we need access to fairly  rich knowledge sources. for this reason, we carried out an experimental evaluation and comparison of the two methods on location questions. we report on our experiments in section # and conclude in section #. we start out by discussing related work, in section #. we presented two strategies for answer type checking,  ltering and re ranking, which we implemented using knowledge intensive methods for the former, and a redundancy based approach for the latter. our experimental  ndings clearly show the merits of answer type checking in general, but there is a mixed message about the two approaches. in current research we combined knowledge intensive and redundancy based approaches, where an implementation of one of them showed  rst promising results. our evaluation is very speci c for the large class of questions about geography that we considered   this is an ideal domain for knowledge intensive approaches. nevertheless, we believe that our that we expect to be able to apply substantial parts of our general strategy in other domains.
chapter  . we study three distinct approaches to solving this  problem  one approach uses algorithms that rely on rich knowledge bases and  sophisticated syntactic semantic processing  one approach uses patterns that  are learned in an unsupervised manner from the web, using computational  biology inspired alignment algorithms  and one approach uses statistical  noisy channel algorithms similar to those used in machine translation. we  assess the strengths and weaknesses of these three approaches and show how  they can be combined using a maximum entropy based framework. in this paper we focus on the second stage. we situate this work in the context of the trec question answering evaluation competitions, organized annually since #### by nist (voorhees, ####  ####  ####  ####), and use both the trec question and answer collections, the trec text corpus of some # million newspaper and similar articles, and the trec scoring method of mean reciprocal rank (mrr), in order to make our results comparable to other research. the trec guidelines (http   trec.nist.gov pubs.html) specify, for instance, that given the question what river in the us is known as the big muddy? , strings such as  mississippi ,  the mississippi ,  the mississippi river , and  mississippi river  should be judged as exact answers, while strings such as  #,### miles  mississippi ,  mississip , and  known as the big muddy, the mississippi is the longest river in the us  should be considered inexact. for this reason, we are constantly designing experiments that will enable us to understand better the strengths and weaknesses of the components we are using and to prioritize our work, in order to increase the overall performance of our system. to diagnose the impact of the answer selection component, we did the following    we used the ### trec #### questions that were paired by human assessors with correct, exact answers in the trec collection. for each question, we selected all sentences that were judged as containing a correct, exact answer to it. #   we presented the questions and just these answer sentences to the best answer selection module we had available at that time  in other words, we created perfect experimental conditions, consistent to those that one would achieve if one had perfect document, paragraph, and sentence retrieval components. to our surprise, we found that our answer selection module was capable of selecting the correct, exact answer in only ##.#  of the cases. that is, even when we gave our system only sentences that contained correct, exact answers, it failed to identify more than ##  of them! two other answer selection modules, which we were developing at the same time, produced even worse results  ##.#  and ##.#  correct. somewhat more encouraging, we determined that an oracle that could select the best answer produced by any of the three answer selection modules would have produced ##.#  correct, exact answers. the results of this experiment suggested two clear ways for improving the performance of our overall qa system    increase the performance of any (or all) answer selection module(s). in this chapter, we show that the maximum entropy (me) framework can be used to address both of these problems. by using a relatively small corpus of question answer pairs annotated by humans with correctness  judgments as training data, and by tuning a relatively small number of loglinear features on the training corpus, we show that we can substantially increase the performance of each of our individual answer selection modules. this suggests that our answer selection modules have complementary strengths and that the me framework enables one to learn and exploit well the individualities of each system. because of this, to substantiate the claims made in this chapter, we carried out all our evaluations in the context of an end to end qa system, textmap, in which we varied only the answer selection component(s). each of the answer selection modules described in this paper pinpoints the correct answers and in the resulting ### sentences and assigns them a score. for the contrastive analysis of the answer selection modules we present in this chapter, we chose to use in all of our experiments the ### factoid questions made available by nist as part of the trec #### qa evaluation. in all our experiments, we run our end to end qa system against documents available on either the web or the trec collection. we pinpoint exact answers in web  or trec retrieved sentences using different answer selection or combinations of answer selection modules. to measure the performance of our answer selection modules in the context of the end to end qa system, we created by hand an exhaustive set of correct and exact answer patterns. one can still have correct, exact answers that are not covered by the patterns we created  or one can return answers that are correct and exact but unsupported. we took, however, great care in creating the answer patterns. qualitative evaluations of the correctness of the results reported in our experiments suggest that our methodology is highly reliable. in this chapter, we present three different approaches to answer selection. we assess the performance of each individual system in terms of   number of correct, exact answers ranked in the top position    number of correct, exact answers ranked in the top # positions    mrr score # based on the top five answers. we show that maximum entropy working with a relative small number of features has a significant impact on the performance of each system (section #). we also show that the same me based approach can be used to combine the outputs of the three systems. when we do so, the performance of the end to end qa system increases further (section #). in this paper, we show how three rather different modules provide somewhat complementary results, and in the paragraphs above, we suggest ## chapter   some reasons for the complementarity. nonetheless, we remain optimistic that by investigating alternative approaches, we will be able to determine which types of modules work best for which types of problem variations (including parameters such as domain, amount of training data, ease of acquisition of knowledge, types of question asked, complexity of answers required, etc.
comparing statistical and content based. we  present a novel approach to answer validation based on the intuition that the amount  of implicit knowledge which connects an answer to a question can be estimated by  exploiting the redundancy of web information. this paper presents a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be estimated by exploiting the redundancy of web information. our hypothesis is that a web search of documents in which the question and the answer co occur can provide all the information needed to accomplish the answer validation task. we have presented approaches to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be estimated by exploiting  the redundancy of web information. first, a generate and test module based on the validationalgorithm presented in this paper will be integratedin the architecture of the diogene qa system under development at irst. we also consider the possibility combine the two approaches in more effective way.
